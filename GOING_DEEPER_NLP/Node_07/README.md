# 7. Seq2seq와 Attention

**언어 모델이 발전해 온 과정에 대해 개략적으로 공부하고, NLP의 큰 흐름 중 하나인 Sequence to Sequence(Seq2seq)에 대해 공부합니다. 이를 발전시키기 위한 기법이자 지금은 없어선 안 될 중요한 메커니즘인 Attention에 대해서도 살펴볼 거고요. 부디 즐거운 시간이 되시길 바랍니다!**

---

|-|목차|⏲ 240분|
|:---:|---|:---:|
|7-1| 들어가며 | 5분|
|7-2| 우리가 만드는 언어 모델 | 30분|
|7-3| Sequence to Sequence 문제 | 40분|
|7-4| Sequence to Sequence 구현 | 40분|
|7-5| Attention! (1) Bahdanau Attention | 60분|
|7-6| Attention! (2) Luong Attention | 30분|
|7-7| 트랜스포머로 가기 전 징검다리? | 30분|
|7-8| 마무리 | 5분|

---

## 학습 목표

- 언어 모델이 발전해 온 과정을 개략적으로 파악한다.
- 기존 RNN 기법이 번역에서 보인 한계를 파악하고, 이를 개선한 Seq2seq를 이해한다.
- Seq2seq를 발전시킨 Attention에 대해 알아본다.

## 학습 내용

- 우리가 만드는 언어 모델
- Sequence to Sequence 문제
- Sequence to Sequence 구현
- Attention! (1) Bahdanau Attention
- Attention! (2) Luong Attention
- 미래의 기법들
- 마무리

---
