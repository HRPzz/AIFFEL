# 1. 텍스트 데이터 다루기

**텍스트 처리성능에 큰 영향을 주는 다양한 텍스트 데이터 전처리 기법을 소개한다. Word나 형태소 레벨의 tokenizer 및 Subword 레벨 tokenizing 기법(BPE, sentencepiece)을 소개하고 사용법을 익힌다.**

---

|-|목차|⏲ 190분|
|:---:|---|:---:|
|1-1| 들어가며 | 30분|
|1-2| 전처리:자연어의 노이즈 제거 | 20분|
|1-3| 분산표현:바나나와 사과의 관계를 어떻게 표현할까? | 20분|
|1-4| 토큰화:그녀는? 그녀+는? | 30분|
|1-5| 토큰화:다른 방법들 | 30분|
|1-6| 코튼에게 의미를 부여하기 | 30분|
|1-7| 마무리 | 30분|

---

### 학습 목표

- 분산 표현에 대한 직관적 이해를 얻는다.
- 문장 데이터를 정제하는 방법을 배운다.
- 토큰화의 여러 가지 기법들을 배운다.
- 단어 Embedding을 구축하는 방법에 대해 가볍게 맛본다.

---

- 언어
  - 자연어(Natural Language)
    - 문맥의존 문법(Context-sensitive Grammar)
    - 단순히 프로그래밍으로 만든 파서로는 자연어 처리를 할 수 없다.
    - 문맥에 따라 해석이 달라진다. => 언어 문법, 언어의 의미까지 이해해야 함
    - 머신 러닝 기법 - 워드 벡터(word vector) 활용
  - 인공어(Aritificial Language)
    - 프로그래밍 언어(Programming Language)
      - 문맥자유 문법(Context-free Grammar)
      - 모호성 없이 파싱트리를 만들 수 있다.
- 토큰화(Tokenization)
  - 단어의 의미 단위로 쪼개는 작업
  - 자연어 처리 모델 성능에 결정적 영향을 미침
  - 토크나이저가 문장을 쪼개는 방식에 따라 같은 문장도 다른 워드 벡터 리스트가 됨
  - cf. Subword level 처리 기법(Wordpiece Model)
- 텍스트 데이터 전처리
  - 노이즈 제거
    - 일상어는 예외적으로 변형(띄어쓰기, 맞춤법, 약어 등)될 여지가 무궁무진함 => 모델 입장에서는 노이즈가 됨
    - 소설책, 신문 기사같은 비교적 노이즈가 적은 데이터를 사용하는 것이 일반적
    - 한국어 노이즈 유형
      - 불완전한 문장으로 구성된 대화
        - 한 문장을 여러 번에 나눠서 전송
        - 여러 문장을 한 번에 전송
      - 문장의 길이가 너무 길거나 짧은 경우
        - 아주 짧은 문장: 의미 없음 or 리액션일 경우
        - 아주 긴 문장: 대화와 관계없는 문장
      - 채팅 데이터에서 문장 시간 간격이 너무 긴 경우
        - 대화가 아니라 서로 자기 말만 하는 상태
        - 연속된 대화로 보기 어려움
      - 바람직하지 않은 문장의 사용
        - 욕설, 오타 비율이 높은 문장은 자연어 모델 학습에 사용하지 말 것
    - 영어 대표적인 3가지 노이즈 유형
      - 문장부호
        - 문장부호 양쪽에 공백 추가 -> 이후에 공백 기반 토큰화를 하기 위함
        - 모든 규칙에 대해 변환을 정의해 줄 수 없음 => 불가피한 손실로 취급하고 넘어감
        - sentence = sentence.replace(p, " " + p " ")
      - 대소문자
        - 모든 문자를 소문자/대문자로 바꿈
        - sentence.lower()
        - sentence.upper()
      - 특수문자
        - 무한한 특수문자를 모두 정의하여 제거할 수 없음 => 사용할 알파벳, 기호를 정의해서 나머지 모두 제거
        - import re  # 정규표현식 패키지
        - sentence = re.sub("([^a-zA-Z.,?!])", " ", sentence)

```python
def cleaning_text(text, punc, regex):
    # 노이즈 유형 (1) 문장부호 공백추가
    for p in punc:
        text = text.replace(p, " " + p + " ")

    # 노이즈 유형 (2), (3) 소문자화 및 특수문자 제거
    text = re.sub(regex, " ", text).lower()

    return text
```

- 단어의 희소 표현 vs 분산 표현
  - 임베딩 레이어(Embedding Layer)를통해 단어를 벡터로 표현하겠다는 뜻
  - 희소 표현(Sparse representation)
    - 벡터의 각 차원마다 단어의 특정 의미 속성을 대응시킴 => 속성값을 임의로 지정
    - 속성의 종류가 늘어날수록 워드 벡터의 차원이 늘어남 => 대부분의 차원이 0.0 으로 채워짐
    - 단점
      - 불필요한 메모리와 연산량이 낭비됨
      - 희소 표현의 워드 벡터끼리 단어들 간 의미적 유사도(코사인 유사도(Cosine Similarity))를 계산할 수 없다.
  - 분산 표현(distributed representation)
    - 각 단어가 몇 차원의 속성을 가질지 정의
    - e.g. embedding_layer = tf.keras.layers.Embedding(input_dim=100, output_dim=256)  # 100개의 단어를 256차원 속성으로 표현
    - 컴퓨터 입장에서는 단어 사전이 됨
- 토큰화
  - 한 문장에서 단어의 수를 결정함
  - 토큰(Token): 문장에서 쪼개진 각 단어
  - 토큰화(Tokenization) 기법: 문장을 쪼갠 기준
  - 토큰화 방법
    - 공백 기반 토큰화
      - tokens = corpus.split()
      - days 와 day 가 구분되어 따로 저장되는 문제는 불가피한 손실로 취급하고 넘어감
    - 형태소 기반 토큰화
      - 형태소: 뜻을 가진 가장 작은 말의 단위
      - e.g. 오늘도 공부만 한다 => 오늘,도,공부,만,한다
      - 한국어 형태소 분석기
        - [[형태소 분석기 속도 비교]](https://iostream.tistory.com/144)
        - KoNLPy 라이브러리
        - mecab: 속도 측면에서 가장 뛰어난 분석기
        - KOMORAN + (macab, 꼬꼬마): 시간보다 정확도(맞춤법에 강건함)가 뛰어난 분석기
    - 문제점: OOV(Out-Of-Vocabulary)
      - 자주 등장한 상위 N개 단어만 사용, 나머지는 <UNK> 특수 토큰(Unknown Token)으로 치환
      - 새로 등장한(본 적 없는) 단어에 대해 약한 모습
      - 해결 시도: Wordpiece Model
  - 토큰화 다른 방법
    - WPM(Wordpiece Model)
      - 한 단어를 여러 개의 Subword 의 집합으로 보는 방법
      - BPE 를 변형해서 제안한 알고리즘
        - BPE 와 다른점
          - 공백 복원을 위해 단어 시작 부분에 언더바 _추가 => 문장 복원할 때 모든 토큰을 합친 후, 언더바_ 를 공백으로 치환하면 끝남!
          - 빈도수 기반이 아닌 가능도(Likelihood) 증가시키는 방향으로 문자 쌍 합침
        - BPE(Byte Pair Encoding) 알고리즘
          - 1994년 데이터 압축을 위해 생김
          - 가장 많이 등장하는 바이트 쌍(Byte Pair)를 새로운 단어로 치환하여 압축하는 작업을 반복
          - 2015년 토큰화에 적용
          - OOV 문제 완전 해결!
          - 단어 개수 줄어듦 => 메모리 절약 (Embedding 레이어는 단어 개수*Embedding 차원 수의 weight 을 생성하기 때문)
      - 조사, 어미 활용이 많고 복잡한 한국어의 경우 WPM이 좋은 대안이 될 수 있다.
      - 어떤 언어든 무관하게 적용 가능한 language-neutral 하고 general 한 기법
      - 공개되어 있지 않고 구글의 SentencePiece 라이브러리를 통해 고성능 BPE 사용 가능
    - soynlp
      - 한국어 자연어 처리를 위한 라이브러리
      - 토크나이저, 단어 추출, 품사 판별, 전처리 기능 제공
      - 형태소 기반 KoNLPy 단점을 해결하기 위해 soynlp 사용
      - 단어의 경계를 비지도학습(통계적 방법)을 통해 결정 => 미등록 단어도 토큰화 가능
- 토큰에 의미를 부여하는 알고리즘
  - Word2Vec
    - 동시에 등장하는 단어끼리 연관성이 있다.
    - 2가지 방식 CBOW, Skip-gram 중에서 후자가 성능이 더 좋다.
    - 은닉층 1개 => 딥러닝 모델X, Shallow Neural Network
    - 단점: 자주 등장하지 않는 단어는 최악의 경우 단 한 번의 연산만을 거쳐 랜덤하게 초기화된 값과 크게 다르지 않은 상태로 알고리즘이 종료될 수 있음
  - FastText
    - Word2Vec 단점을 해결하기 위해 BPE 와 비슷한 아이디어 적용
    - 한 단어를 n-gram 의 집합이라고 보고 단어를 쪼개어 각 n-gram 에 할당된 Embedding 의 평균값 사용
  - ELMo
    - Word2Vec, FastText 와 다르게 동음이의어를 처리할 수 있다.
    - Contextualized Word Embedding
      - 문맥(context) 활용 => 단어의 의미 벡터를 구하기 위해서 단어와 단어가 놓인 주변 단어 배치의 맥락이 함께 고려돼야 함
      - ELMo, BERT 등에 많이 사용되는 획기적인 아이디어
      - 벡터 = 입력 토큰 word vector + 순방향 LSTM 의 hidden state vector + 역방향 LSTM 의 hidden state vector
- 정리
  - 문장 입력 -> 토큰화 -> 임베딩(Embedding)을 통해 분산 표현으로 만듦
  - 한국어 토큰화
    - 형태소 기반 KoNLPy 사용
    - WordPiece Model 인 SentencePiece 사용
  - 토큰 간 관계성 주입
    - Word2Vec, FastText
    - 문장 문맥까지 고려: ELMo
