{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. modern NLP의 흐름에 올라타보자\n",
    "\n",
    "**Transformer를 바탕으로 한 최근 NLP의 모델에 대해서 알아보겠습니다**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13-1. 들어가며"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13-2. Transfer Learning과 Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13-3. ELMO(Embedding from Language Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13-4. GPT(Generative Pre-Training Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class TFAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, nx, n_ctx, config, scale=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n",
    "        # [switch nx => n_state from Block to Attention to keep identical to TF implem]\n",
    "        assert n_state % config.n_head == 0\n",
    "        self.n_ctx = n_ctx\n",
    "        self.n_head = config.n_head\n",
    "        self.split_size = n_state\n",
    "        self.scale = scale\n",
    "        self.output_attentions = config.output_attentions\n",
    "\n",
    "        self.c_attn = TFConv1D(n_state * 3, nx, initializer_range=config.initializer_range, name=\"c_attn\")\n",
    "        self.c_proj = TFConv1D(n_state, nx, initializer_range=config.initializer_range, name=\"c_proj\")\n",
    "        self.attn_dropout = tf.keras.layers.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = tf.keras.layers.Dropout(config.resid_pdrop)\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def causal_attention_mask(nd, ns, dtype):\n",
    "        \"\"\"\n",
    "        1-2) masked attention에서 설명한 masking 부분\n",
    "        \"\"\"\n",
    "        i = tf.range(nd)[:, None]\n",
    "        j = tf.range(ns)\n",
    "        m = i >= j - ns + nd\n",
    "        return tf.cast(m, dtype)\n",
    "\n",
    "    def _attn(self, q, k, v, attention_mask, head_mask, output_attentions, training=False):\n",
    "                \"\"\"\n",
    "                1-2) attention 계산\n",
    "        q, k, v 의 shape : [batch, heads, sequence, features]\n",
    "                \"\"\"\n",
    "\n",
    "        w = tf.matmul(q, k, transpose_b=True)\n",
    "        if self.scale:\n",
    "            dk = tf.cast(shape_list(k)[-1], tf.float32)  # scale attention_scores\n",
    "            w = w / tf.math.sqrt(dk)\n",
    "\n",
    "        # w shape : [batch, heads, dst_sequence, src_sequence]\n",
    "        _, _, nd, ns = shape_list(w)\n",
    "        b = self.causal_attention_mask(nd, ns, dtype=w.dtype)\n",
    "        b = tf.reshape(b, [1, 1, nd, ns])\n",
    "        w = w * b - 1e4 * (1 - b)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # attention mask 적용\n",
    "            w = w + attention_mask\n",
    "\n",
    "        w = tf.nn.softmax(w, axis=-1)\n",
    "        w = self.attn_dropout(w, training=training)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            w = w * head_mask\n",
    "\n",
    "        outputs = [tf.matmul(w, v)]\n",
    "        if output_attentions:\n",
    "            outputs.append(w)\n",
    "        return outputs\n",
    "\n",
    "    def merge_heads(self, x):\n",
    "        x = tf.transpose(x, [0, 2, 1, 3])\n",
    "        x_shape = shape_list(x)\n",
    "        new_x_shape = x_shape[:-2] + [x_shape[-2] * x_shape[-1]]\n",
    "        return tf.reshape(x, new_x_shape)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        x_shape = shape_list(x)\n",
    "        new_x_shape = x_shape[:-1] + [self.n_head, x_shape[-1] // self.n_head]\n",
    "        x = tf.reshape(x, new_x_shape)\n",
    "        return tf.transpose(x, (0, 2, 1, 3))  # (batch, head, seq_length, head_features)\n",
    "\n",
    "    def call(self, x, layer_past, attention_mask, head_mask, use_cache, output_attentions, training=False):\n",
    "        x = self.c_attn(x)\n",
    "        query, key, value = tf.split(x, 3, axis=2)\n",
    "        query = self.split_heads(query)\n",
    "        key = self.split_heads(key)\n",
    "        value = self.split_heads(value)\n",
    "        if layer_past is not None:\n",
    "            past_key, past_value = tf.unstack(layer_past, axis=0)\n",
    "            key = tf.concat([past_key, key], axis=-2)\n",
    "            value = tf.concat([past_value, value], axis=-2)\n",
    "\n",
    "        # keras serialization을 위한 코드\n",
    "        if use_cache:\n",
    "            present = tf.stack([key, value], axis=0)\n",
    "        else:\n",
    "            present = (None,)\n",
    "\n",
    "        attn_outputs = self._attn(query, key, value, attention_mask, head_mask, output_attentions, training=training)\n",
    "        a = attn_outputs[0]\n",
    "\n",
    "        a = self.merge_heads(a)\n",
    "        a = self.c_proj(a)\n",
    "        a = self.resid_dropout(a, training=training)\n",
    "\n",
    "        outputs = [a, present] + attn_outputs[1:]\n",
    "        return outputs  # a, present, (attentions)\n",
    "\n",
    "\n",
    "class TFMLP(tf.keras.layers.Layer):\n",
    "\"\"\"\n",
    "Transformer의 Decoder Block에서 Feed Foward를 구현해 둔 부분\n",
    "\"\"\"\n",
    "    def __init__(self, n_state, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        nx = config.n_embd\n",
    "        self.c_fc = TFConv1D(n_state, nx, initializer_range=config.initializer_range, name=\"c_fc\")\n",
    "        self.c_proj = TFConv1D(nx, n_state, initializer_range=config.initializer_range, name=\"c_proj\")\n",
    "        self.act = get_tf_activation(\"gelu\")\n",
    "        self.dropout = tf.keras.layers.Dropout(config.resid_pdrop)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        h = self.act(self.c_fc(x)) # conv1d로 flatten 후 activation 적용\n",
    "        h2 = self.c_proj(h)\n",
    "        h2 = self.dropout(h2, training=training)\n",
    "        return h2\n",
    "\n",
    "\n",
    "class TFBlock(tf.keras.layers.Layer):\n",
    "\"\"\"\n",
    "Transformer의 Decoder Block을 구현해 둔 부분\n",
    "\"\"\"\n",
    "    def __init__(self, n_ctx, config, scale=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        nx = config.n_embd\n",
    "        inner_dim = config.n_inner if config.n_inner is not None else 4 * nx\n",
    "        self.ln_1 = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name=\"ln_1\")\n",
    "        self.attn = TFAttention(nx, n_ctx, config, scale, name=\"attn\")\n",
    "        self.ln_2 = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name=\"ln_2\")\n",
    "        self.mlp = TFMLP(inner_dim, config, name=\"mlp\")\n",
    "\n",
    "    def call(self, x, layer_past, attention_mask, head_mask, use_cache, output_attentions, training=False):\n",
    "        a = self.ln_1(x)\n",
    "        output_attn = self.attn(\n",
    "            a, layer_past, attention_mask, head_mask, use_cache, output_attentions, training=training\n",
    "        )\n",
    "        a = output_attn[0]  # output_attn: a, present, (attentions)\n",
    "        x = x + a\n",
    "\n",
    "        m = self.ln_2(x)\n",
    "        m = self.mlp(m, training=training)\n",
    "        x = x + m\n",
    "\n",
    "        outputs = [x] + output_attn[1:]\n",
    "        return outputs  # x, present, (attentions)\n",
    "\n",
    "\n",
    "@keras_serializable\n",
    "class TFGPT2MainLayer(tf.keras.layers.Layer):\n",
    "\"\"\"\n",
    "모델의 전체 구조\n",
    "\"\"\"\n",
    "    config_class = GPT2Config\n",
    "\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super().__init__(*inputs, **kwargs)\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.output_hidden_states = config.output_hidden_states\n",
    "        self.use_cache = config.use_cache\n",
    "        self.return_dict = config.use_return_dict\n",
    "\n",
    "        self.num_hidden_layers = config.n_layer\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        self.wte = TFSharedEmbeddings(\n",
    "            config.vocab_size, config.hidden_size, initializer_range=config.initializer_range, name=\"wte\"\n",
    "        )\n",
    "        self.wpe = tf.keras.layers.Embedding(\n",
    "            config.n_positions,\n",
    "            config.n_embd,\n",
    "            embeddings_initializer=get_initializer(config.initializer_range),\n",
    "            name=\"wpe\",\n",
    "        )\n",
    "        self.drop = tf.keras.layers.Dropout(config.embd_pdrop)\n",
    "        self.h = [TFBlock(config.n_ctx, config, scale=True, name=\"h_._{}\".format(i)) for i in range(config.n_layer)]\n",
    "        self.ln_f = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name=\"ln_f\")\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.wte\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.wte.weight = value\n",
    "        self.wte.vocab_size = self.wte.weight.shape[0]\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\"\n",
    "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        inputs,\n",
    "        past=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        training=False,\n",
    "    ):\n",
    "        if isinstance(inputs, (tuple, list)):\n",
    "            input_ids = inputs[0]\n",
    "            past = inputs[1] if len(inputs) > 1 else past\n",
    "            attention_mask = inputs[2] if len(inputs) > 2 else attention_mask\n",
    "            token_type_ids = inputs[3] if len(inputs) > 3 else token_type_ids\n",
    "            position_ids = inputs[4] if len(inputs) > 4 else position_ids\n",
    "            head_mask = inputs[5] if len(inputs) > 5 else head_mask\n",
    "            inputs_embeds = inputs[6] if len(inputs) > 6 else inputs_embeds\n",
    "            use_cache = inputs[7] if len(inputs) > 7 else use_cache\n",
    "            output_attentions = inputs[8] if len(inputs) > 8 else output_attentions\n",
    "            output_hidden_states = inputs[9] if len(inputs) > 9 else output_hidden_states\n",
    "            return_dict = inputs[10] if len(inputs) > 10 else return_dict\n",
    "            assert len(inputs) <= 11, \"Too many inputs.\"\n",
    "        elif isinstance(inputs, (dict, BatchEncoding)):\n",
    "            input_ids = inputs.get(\"input_ids\")\n",
    "            past = inputs.get(\"past\", past)\n",
    "            attention_mask = inputs.get(\"attention_mask\", attention_mask)\n",
    "            token_type_ids = inputs.get(\"token_type_ids\", token_type_ids)\n",
    "            position_ids = inputs.get(\"position_ids\", position_ids)\n",
    "            head_mask = inputs.get(\"head_mask\", head_mask)\n",
    "            inputs_embeds = inputs.get(\"inputs_embeds\", inputs_embeds)\n",
    "            use_cache = inputs.get(\"use_cache\", use_cache)\n",
    "            output_attentions = inputs.get(\"output_attentions\", output_attentions)\n",
    "            output_hidden_states = inputs.get(\"output_hidden_states\", output_hidden_states)\n",
    "            return_dict = inputs.get(\"return_dict\", return_dict)\n",
    "            assert len(inputs) <= 11, \"Too many inputs.\"\n",
    "        else:\n",
    "            input_ids = inputs\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n",
    "        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.output_hidden_states\n",
    "        use_cache = use_cache if use_cache is not None else self.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.return_dict\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = shape_list(input_ids)\n",
    "            input_ids = tf.reshape(input_ids, [-1, input_shape[-1]])\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = shape_list(inputs_embeds)[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        if past is None:\n",
    "            past_length = 0\n",
    "            past = [None] * len(self.h)\n",
    "        else:\n",
    "            past_length = shape_list(past[0][0])[-2]\n",
    "        if position_ids is None:\n",
    "            position_ids = tf.range(past_length, input_shape[-1] + past_length, dtype=tf.int32)[tf.newaxis, :]\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # 3D attention mask 만들기\n",
    "            # Sizes : [batch_size, 1, 1, to_seq_length]\n",
    "            # 3D attention mask를 [batch_size, num_heads, from_seq_length, to_seq_length]로 브로드캐스팅\n",
    "\n",
    "            attention_mask = attention_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "            # attention_mask가 1.0이면 포함, 0.0이면 미포함 하여 attention 계산\n",
    "            attention_mask = tf.cast(attention_mask, tf.float32)\n",
    "            attention_mask = (1.0 - attention_mask) * -10000.0\n",
    "        else:\n",
    "            attention_mask = None\n",
    "\n",
    "        # head_mask가 1.0이면, head를 유지\n",
    "        # attention_probs : shape bsz x n_heads x N x N\n",
    "        # input head_mask : shape [num_heads] 또는 [num_hidden_layers x num_heads]\n",
    "        # head_mask : shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        if head_mask is not None:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            head_mask = [None] * self.num_hidden_layers\n",
    "            # head_mask = tf.constant([0] * self.num_hidden_layers)\n",
    "\n",
    "        position_ids = tf.reshape(position_ids, [-1, shape_list(position_ids)[-1]])\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.wte(input_ids, mode=\"embedding\")\n",
    "        position_embeds = self.wpe(position_ids)\n",
    "        if token_type_ids is not None:\n",
    "            token_type_ids = tf.reshape(token_type_ids, [-1, shape_list(token_type_ids)[-1]])\n",
    "            token_type_embeds = self.wte(token_type_ids, mode=\"embedding\")\n",
    "        else:\n",
    "            token_type_embeds = 0\n",
    "        hidden_states = inputs_embeds + position_embeds + token_type_embeds\n",
    "        hidden_states = self.drop(hidden_states, training=training)\n",
    "\n",
    "        output_shape = input_shape + [shape_list(hidden_states)[-1]]\n",
    "\n",
    "        presents = () if use_cache else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        for i, (block, layer_past) in enumerate(zip(self.h, past)):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (tf.reshape(hidden_states, output_shape),)\n",
    "\n",
    "            outputs = block(\n",
    "                hidden_states,\n",
    "                layer_past,\n",
    "                attention_mask,\n",
    "                head_mask[i],\n",
    "                use_cache,\n",
    "                output_attentions,\n",
    "                training=training,\n",
    "            )\n",
    "\n",
    "            hidden_states, present = outputs[:2]\n",
    "            if use_cache:\n",
    "                presents = presents + (present,)\n",
    "\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (outputs[2],)\n",
    "\n",
    "        hidden_states = self.ln_f(hidden_states)\n",
    "\n",
    "        hidden_states = tf.reshape(hidden_states, output_shape)\n",
    "        # Add last hidden state\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            attention_output_shape = input_shape[:-1] + [-1] + shape_list(all_attentions[0])[-2:]\n",
    "            all_attentions = tuple(tf.reshape(t, attention_output_shape) for t in all_attentions)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, presents, all_hidden_states, all_attentions] if v is not None)\n",
    "\n",
    "        return TFBaseModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=presents,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_attentions,\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13-5. BERT(Bidirectional Encoder Representations from Transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class TFBertPreTrainingLoss:\n",
    "    \"\"\"\n",
    "    BERT의 경우 Pretraining으로 NSP + MLM 두 가지를 함께 학습하게 됨. 그것을 위한 loss \n",
    "        -100으로 label(logit)이 되어있는 경우 loss 계산 시 제외\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_loss(self, labels, logits):\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n",
    "        )\n",
    "\n",
    "        masked_lm_active_loss = tf.not_equal(tf.reshape(labels[\"labels\"], (-1,)), -100)\n",
    "        masked_lm_reduced_logits = tf.boolean_mask(\n",
    "            tf.reshape(logits[0], (-1, shape_list(logits[0])[2])),\n",
    "            masked_lm_active_loss,\n",
    "        )\n",
    "        masked_lm_labels = tf.boolean_mask(tf.reshape(labels[\"labels\"], (-1,)), masked_lm_active_loss)\n",
    "        next_sentence_active_loss = tf.not_equal(tf.reshape(labels[\"next_sentence_label\"], (-1,)), -100)\n",
    "        next_sentence_reduced_logits = tf.boolean_mask(tf.reshape(logits[1], (-1, 2)), next_sentence_active_loss)\n",
    "        next_sentence_label = tf.boolean_mask(\n",
    "            tf.reshape(labels[\"next_sentence_label\"], (-1,)), mask=next_sentence_active_loss\n",
    "        )\n",
    "        masked_lm_loss = loss_fn(masked_lm_labels, masked_lm_reduced_logits)\n",
    "        next_sentence_loss = loss_fn(next_sentence_label, next_sentence_reduced_logits)\n",
    "        masked_lm_loss = tf.reshape(masked_lm_loss, (-1, shape_list(next_sentence_loss)[0]))\n",
    "        masked_lm_loss = tf.reduce_mean(masked_lm_loss, 0)\n",
    "\n",
    "        return masked_lm_loss + next_sentence_loss\n",
    "\n",
    "\n",
    "class TFBertEmbeddings(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "        1-1)에 해당하는 부분으로 3가지 embedding을 만들고 그 embedding을 모두 합산하여 layer normalize와 dropout을 적용\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.initializer_range = config.initializer_range\n",
    "        self.position_embeddings = tf.keras.layers.Embedding(\n",
    "            config.max_position_embeddings,\n",
    "            config.hidden_size,\n",
    "            embeddings_initializer=get_initializer(self.initializer_range),\n",
    "            name=\"position_embeddings\",\n",
    "        )\n",
    "        self.token_type_embeddings = tf.keras.layers.Embedding(\n",
    "            config.type_vocab_size,\n",
    "            config.hidden_size,\n",
    "            embeddings_initializer=get_initializer(self.initializer_range),\n",
    "            name=\"token_type_embeddings\",\n",
    "        )\n",
    "\n",
    "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n",
    "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"shared word embedding layer \"\"\"\n",
    "        with tf.name_scope(\"word_embeddings\"):\n",
    "            # Create and initialize weights. The random normal initializer was chosen\n",
    "            # arbitrarily, and works well.\n",
    "            self.word_embeddings = self.add_weight(\n",
    "                \"weight\",\n",
    "                shape=[self.vocab_size, self.hidden_size],\n",
    "                initializer=get_initializer(self.initializer_range),\n",
    "            )\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        position_ids=None,\n",
    "        token_type_ids=None,\n",
    "        inputs_embeds=None,\n",
    "        mode=\"embedding\",\n",
    "        training=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        input의 token embeddings\n",
    "        Args:\n",
    "            inputs: int64 tensors (shape [batch_size, length]) 3개를 담은 리스트: (input_ids, position_ids, token_type_ids)\n",
    "            mode: \"embedding\" | \"linear\"\n",
    "        Returns:\n",
    "            outputs: mode == \"embedding\"; output embedding tensor(float32, shape [batch_size, length, embedding_size])\n",
    "                                         mode == \"linear\", output linear tensor(float32, shape [batch_size, length, vocab_size])\n",
    "        Raises:\n",
    "            ValueError: if mode is not valid.\n",
    "                \"\"\"\n",
    "\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(input_ids, position_ids, token_type_ids, inputs_embeds, training=training)\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(input_ids)\n",
    "        else:\n",
    "            raise ValueError(\"mode {} is not valid.\".format(mode))\n",
    "\n",
    "    def _embedding(self, input_ids, position_ids, token_type_ids, inputs_embeds, training=False):\n",
    "        \"\"\"input tensor에 기반하여 임베딩 적용\"\"\"\n",
    "        assert not (input_ids is None and inputs_embeds is None)\n",
    "\n",
    "        if input_ids is not None:\n",
    "            input_shape = shape_list(input_ids)\n",
    "        else:\n",
    "            input_shape = shape_list(inputs_embeds)[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = tf.range(seq_length, dtype=tf.int32)[tf.newaxis, :]\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = tf.fill(input_shape, 0)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = tf.gather(self.word_embeddings, input_ids)\n",
    "\n",
    "        position_embeddings = tf.cast(self.position_embeddings(position_ids), inputs_embeds.dtype)\n",
    "        token_type_embeddings = tf.cast(self.token_type_embeddings(token_type_ids), inputs_embeds.dtype)\n",
    "        embeddings = inputs_embeds + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings, training=training)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def _linear(self, inputs):\n",
    "        \"\"\"\n",
    "               linear layer를 통해서 input의 logit을 계산\n",
    "        Args:\n",
    "            inputs: float32 tensor (shape [batch_size, length, hidden_size])\n",
    "        Returns:\n",
    "            float32 tensor (shape [batch_size, length, vocab_size])\n",
    "        \"\"\"\n",
    "        batch_size = shape_list(inputs)[0]\n",
    "        length = shape_list(inputs)[1]\n",
    "        x = tf.reshape(inputs, [-1, self.hidden_size])\n",
    "        logits = tf.matmul(x, self.word_embeddings, transpose_b=True)\n",
    "\n",
    "        return tf.reshape(logits, [batch_size, length, self.vocab_size])\n",
    "\n",
    "\n",
    "class TFBertSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        if config.hidden_size % config.num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        assert config.hidden_size % config.num_attention_heads == 0\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        self.query = tf.keras.layers.Dense(\n",
    "            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n",
    "        )\n",
    "        self.key = tf.keras.layers.Dense(\n",
    "            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key\"\n",
    "        )\n",
    "        self.value = tf.keras.layers.Dense(\n",
    "            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"value\"\n",
    "        )\n",
    "        self.dropout = tf.keras.layers.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_attention_heads, self.attention_head_size))\n",
    "\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, hidden_states, attention_mask, head_mask, output_attentions, training=False):\n",
    "        batch_size = shape_list(hidden_states)[0]\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\n",
    "\n",
    "        # \"query\"와 \"key\"의 dot product : raw attention scores\n",
    "        attention_scores = tf.matmul(\n",
    "            query_layer, key_layer, transpose_b=True\n",
    "        )  # (batch size, num_heads, seq_len_q, seq_len_k)\n",
    "        dk = tf.cast(shape_list(key_layer)[-1], attention_scores.dtype)  # scale attention_scores\n",
    "        attention_scores = attention_scores / tf.math.sqrt(dk)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = tf.nn.softmax(attention_scores, axis=-1)\n",
    "\n",
    "        attention_probs = self.dropout(attention_probs, training=training)\n",
    "\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = tf.matmul(attention_probs, value_layer)\n",
    "        context_layer = tf.transpose(context_layer, perm=[0, 2, 1, 3])\n",
    "        context_layer = tf.reshape(\n",
    "            context_layer, (batch_size, -1, self.all_head_size)\n",
    "        )  # (batch_size, seq_len_q, all_head_size)\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class TFBertSelfOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n",
    "        )\n",
    "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n",
    "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def call(self, hidden_states, input_tensor, training=False):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states, training=training)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class TFBertAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.self_attention = TFBertSelfAttention(config, name=\"self\")\n",
    "        self.dense_output = TFBertSelfOutput(config, name=\"output\")\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def call(self, input_tensor, attention_mask, head_mask, output_attentions, training=False):\n",
    "        self_outputs = self.self_attention(\n",
    "            input_tensor, attention_mask, head_mask, output_attentions, training=training\n",
    "        )\n",
    "        attention_output = self.dense_output(self_outputs[0], input_tensor, training=training)\n",
    "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class TFBertIntermediate(tf.keras.layers.Layer):\n",
    "\"\"\"\n",
    "Transformer Block에서의 feedforward\n",
    "\"\"\"\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n",
    "        )\n",
    "\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.intermediate_act_fn = get_tf_activation(config.hidden_act)\n",
    "        else:\n",
    "            self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "    def call(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class TFBertOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n",
    "        )\n",
    "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n",
    "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def call(self, hidden_states, input_tensor, training=False):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states, training=training)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class TFBertLayer(tf.keras.layers.Layer):\n",
    "\"\"\"\n",
    "Transformer Encoder Block과 동일한 구조 : Attention,Feedforward,dropout,layer nomalization\n",
    "\"\"\"\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.attention = TFBertAttention(config, name=\"attention\")\n",
    "        self.intermediate = TFBertIntermediate(config, name=\"intermediate\")\n",
    "        self.bert_output = TFBertOutput(config, name=\"output\")\n",
    "\n",
    "    def call(self, hidden_states, attention_mask, head_mask, output_attentions, training=False):\n",
    "        attention_outputs = self.attention(\n",
    "            hidden_states, attention_mask, head_mask, output_attentions, training=training\n",
    "        )\n",
    "        attention_output = attention_outputs[0]\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.bert_output(intermediate_output, attention_output, training=training)\n",
    "        outputs = (layer_output,) + attention_outputs[1:]  # add attentions if we output them\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class TFBertEncoder(tf.keras.layers.Layer):\n",
    "\"\"\"\n",
    "Transformer Encoder Block(코드 상에서 TFBertLayer)를 n_layer만큼 여러개 쌓은 구조\n",
    "\"\"\"\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.layer = [TFBertLayer(config, name=\"layer_._{}\".format(i)) for i in range(config.num_hidden_layers)]\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask,\n",
    "        head_mask,\n",
    "        output_attentions,\n",
    "        output_hidden_states,\n",
    "        return_dict,\n",
    "        training=False,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_outputs = layer_module(\n",
    "                hidden_states, attention_mask, head_mask[i], output_attentions, training=training\n",
    "            )\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "        # Add last layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, all_hidden_states, all_attentions] if v is not None)\n",
    "\n",
    "        return TFBaseModelOutput(\n",
    "            last_hidden_state=hidden_states, hidden_states=all_hidden_states, attentions=all_attentions\n",
    "        )\n",
    "\n",
    "\n",
    "class TFBertPooler(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            config.hidden_size,\n",
    "            kernel_initializer=get_initializer(config.initializer_range),\n",
    "            activation=\"tanh\",\n",
    "            name=\"dense\",\n",
    "        )\n",
    "\n",
    "    def call(self, hidden_states):\n",
    "        # 첫 번째 토큰의 hidden state를 얻기 위해 pool\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "\n",
    "        return pooled_output\n",
    "\n",
    "\n",
    "class TFBertPredictionHeadTransform(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n",
    "        )\n",
    "\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.transform_act_fn = get_tf_activation(config.hidden_act)\n",
    "        else:\n",
    "            self.transform_act_fn = config.hidden_act\n",
    "\n",
    "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n",
    "\n",
    "    def call(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class TFBertLMPredictionHead(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, input_embeddings, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.transform = TFBertPredictionHeadTransform(config, name=\"transform\")\n",
    "\n",
    "        # input embeddings과 동일한 weight를 가지고 있지만 각각의 token에 대하여 output만 바이어스를 가지고 있음\n",
    "        self.input_embeddings = input_embeddings\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bias = self.add_weight(shape=(self.vocab_size,), initializer=\"zeros\", trainable=True, name=\"bias\")\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, hidden_states):\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "        hidden_states = self.input_embeddings(hidden_states, mode=\"linear\")\n",
    "        hidden_states = hidden_states + self.bias\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class TFBertMLMHead(tf.keras.layers.Layer):\n",
    "\"\"\"\n",
    "2-1)Masked LM을 위한 class\n",
    "\"\"\"\n",
    "    def __init__(self, config, input_embeddings, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.predictions = TFBertLMPredictionHead(config, input_embeddings, name=\"predictions\")\n",
    "\n",
    "    def call(self, sequence_output):\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "\n",
    "        return prediction_scores\n",
    "\n",
    "\n",
    "class TFBertNSPHead(tf.keras.layers.Layer):\n",
    "\"\"\"\n",
    "2-2)NSP를 위한 class\n",
    "\"\"\"\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.seq_relationship = tf.keras.layers.Dense(\n",
    "            2, kernel_initializer=get_initializer(config.initializer_range), name=\"seq_relationship\"\n",
    "        )\n",
    "\n",
    "    def call(self, pooled_output):\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "\n",
    "        return seq_relationship_score\n",
    "\n",
    "\n",
    "@keras_serializable\n",
    "class TFBertMainLayer(tf.keras.layers.Layer):\n",
    "\"\"\"\n",
    "모델의 전체 구조\n",
    "\"\"\"\n",
    "    config_class = BertConfig\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_hidden_layers = config.num_hidden_layers\n",
    "        self.initializer_range = config.initializer_range\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.output_hidden_states = config.output_hidden_states\n",
    "        self.return_dict = config.use_return_dict\n",
    "        self.embeddings = TFBertEmbeddings(config, name=\"embeddings\")\n",
    "        self.encoder = TFBertEncoder(config, name=\"encoder\")\n",
    "        self.pooler = TFBertPooler(config, name=\"pooler\")\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "        self.embeddings.vocab_size = value.shape[0]\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\"\n",
    "        heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        inputs,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        training=False,\n",
    "    ):\n",
    "        if isinstance(inputs, (tuple, list)):\n",
    "            input_ids = inputs[0]\n",
    "            attention_mask = inputs[1] if len(inputs) > 1 else attention_mask\n",
    "            token_type_ids = inputs[2] if len(inputs) > 2 else token_type_ids\n",
    "            position_ids = inputs[3] if len(inputs) > 3 else position_ids\n",
    "            head_mask = inputs[4] if len(inputs) > 4 else head_mask\n",
    "            inputs_embeds = inputs[5] if len(inputs) > 5 else inputs_embeds\n",
    "            output_attentions = inputs[6] if len(inputs) > 6 else output_attentions\n",
    "            output_hidden_states = inputs[7] if len(inputs) > 7 else output_hidden_states\n",
    "            return_dict = inputs[8] if len(inputs) > 8 else return_dict\n",
    "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
    "        elif isinstance(inputs, (dict, BatchEncoding)):\n",
    "            input_ids = inputs.get(\"input_ids\")\n",
    "            attention_mask = inputs.get(\"attention_mask\", attention_mask)\n",
    "            token_type_ids = inputs.get(\"token_type_ids\", token_type_ids)\n",
    "            position_ids = inputs.get(\"position_ids\", position_ids)\n",
    "            head_mask = inputs.get(\"head_mask\", head_mask)\n",
    "            inputs_embeds = inputs.get(\"inputs_embeds\", inputs_embeds)\n",
    "            output_attentions = inputs.get(\"output_attentions\", output_attentions)\n",
    "            output_hidden_states = inputs.get(\"output_hidden_states\", output_hidden_states)\n",
    "            return_dict = inputs.get(\"return_dict\", return_dict)\n",
    "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
    "        else:\n",
    "            input_ids = inputs\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n",
    "        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.output_hidden_states\n",
    "        return_dict = return_dict if return_dict is not None else self.return_dict\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = shape_list(input_ids)\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = shape_list(inputs_embeds)[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = tf.fill(input_shape, 1)\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = tf.fill(input_shape, 0)\n",
    "\n",
    "        embedding_output = self.embeddings(input_ids, position_ids, token_type_ids, inputs_embeds, training=training)\n",
    "\n",
    "                # 3D attention mask 만들기\n",
    "        # Sizes : [batch_size, 1, 1, to_seq_length]\n",
    "        # 3D attention mask를 [batch_size, num_heads, from_seq_length, to_seq_length]로 브로드캐스팅\n",
    "\n",
    "        extended_attention_mask = attention_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "                # attention_mask가 1.0이면 포함, 0.0이면 미포함 하여 attention 계산\n",
    "        extended_attention_mask = tf.cast(extended_attention_mask, embedding_output.dtype)\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        # head_mask가 1.0이면, head를 유지\n",
    "        # attention_probs : shape bsz x n_heads x N x N\n",
    "        # input head_mask : shape [num_heads] 또는 [num_hidden_layers x num_heads]\n",
    "        # head_mask : shape [num_hidden_layers x batch x num_heads x seq_length x seq_length\n",
    "        if head_mask is not None:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            head_mask = [None] * self.num_hidden_layers\n",
    "            # head_mask = tf.constant([0] * self.num_hidden_layers)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            extended_attention_mask,\n",
    "            head_mask,\n",
    "            output_attentions,\n",
    "            output_hidden_states,\n",
    "            return_dict,\n",
    "            training=training,\n",
    "        )\n",
    "\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "\n",
    "        if not return_dict:\n",
    "            return (\n",
    "                sequence_output,\n",
    "                pooled_output,\n",
    "            ) + encoder_outputs[1:]\n",
    "\n",
    "        return TFBaseModelOutputWithPooling(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13-6. Transformer-XL(Transformer Extra Long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class TFPositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, demb, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.inv_freq = 1 / (10000 ** (tf.range(0, demb, 2.0) / demb))\n",
    "\n",
    "    def call(self, pos_seq, bsz=None):\n",
    "        sinusoid_inp = tf.einsum(\"i,j->ij\", pos_seq, self.inv_freq)\n",
    "        pos_emb = tf.concat([tf.sin(sinusoid_inp), tf.cos(sinusoid_inp)], -1)\n",
    "\n",
    "        if bsz is not None:\n",
    "            return tf.tile(pos_emb[:, None, :], [1, bsz, 1])\n",
    "        else:\n",
    "            return pos_emb[:, None, :]\n",
    "\n",
    "\n",
    "class TFPositionwiseFF(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_inner, dropout, pre_lnorm=False, layer_norm_epsilon=1e-5, init_std=0.02, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_inner = d_inner\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.layer_1 = tf.keras.layers.Dense(\n",
    "            d_inner, kernel_initializer=get_initializer(init_std), activation=tf.nn.relu, name=\"CoreNet_._0\"\n",
    "        )\n",
    "        self.drop_1 = tf.keras.layers.Dropout(dropout)\n",
    "        self.layer_2 = tf.keras.layers.Dense(d_model, kernel_initializer=get_initializer(init_std), name=\"CoreNet_._3\")\n",
    "        self.drop_2 = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name=\"layer_norm\")\n",
    "\n",
    "        self.pre_lnorm = pre_lnorm\n",
    "\n",
    "    def call(self, inp, training=False):\n",
    "        if self.pre_lnorm:\n",
    "            # layer normalization + positionwise feed-forward\n",
    "            core_out = self.layer_norm(inp)\n",
    "            core_out = self.layer_1(core_out)\n",
    "            core_out = self.drop_1(core_out, training=training)\n",
    "            core_out = self.layer_2(core_out)\n",
    "            core_out = self.drop_2(core_out, training=training)\n",
    "\n",
    "            # residual connection\n",
    "            output = core_out + inp\n",
    "        else:\n",
    "            # positionwise feed-forward\n",
    "            core_out = self.layer_1(inp)\n",
    "            core_out = self.drop_1(core_out, training=training)\n",
    "            core_out = self.layer_2(core_out)\n",
    "            core_out = self.drop_2(core_out, training=training)\n",
    "\n",
    "            # residual connection + layer normalization\n",
    "            output = self.layer_norm(inp + core_out)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class TFRelPartialLearnableMultiHeadAttn(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_head,\n",
    "        d_model,\n",
    "        d_head,\n",
    "        dropout,\n",
    "        dropatt=0.0,\n",
    "        pre_lnorm=False,\n",
    "        r_r_bias=None,\n",
    "        r_w_bias=None,\n",
    "        layer_norm_epsilon=1e-5,\n",
    "        init_std=0.02,\n",
    "        output_attentions=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.d_head = d_head\n",
    "        self.dropout = dropout\n",
    "        self.output_attentions = output_attentions\n",
    "\n",
    "        self.qkv_net = tf.keras.layers.Dense(\n",
    "            3 * n_head * d_head, kernel_initializer=get_initializer(init_std), use_bias=False, name=\"qkv_net\"\n",
    "        )\n",
    "\n",
    "        self.drop = tf.keras.layers.Dropout(dropout)\n",
    "        self.dropatt = tf.keras.layers.Dropout(dropatt)\n",
    "        self.o_net = tf.keras.layers.Dense(\n",
    "            d_model, kernel_initializer=get_initializer(init_std), use_bias=False, name=\"o_net\"\n",
    "        )\n",
    "\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name=\"layer_norm\")\n",
    "\n",
    "        self.scale = 1 / (d_head ** 0.5)\n",
    "\n",
    "        self.pre_lnorm = pre_lnorm\n",
    "\n",
    "        if r_r_bias is not None and r_w_bias is not None:  # Biases are shared\n",
    "            self.r_r_bias = r_r_bias\n",
    "            self.r_w_bias = r_w_bias\n",
    "        else:\n",
    "            self.r_r_bias = None\n",
    "            self.r_w_bias = None\n",
    "\n",
    "        self.r_net = tf.keras.layers.Dense(\n",
    "            self.n_head * self.d_head, kernel_initializer=get_initializer(init_std), use_bias=False, name=\"r_net\"\n",
    "        )\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.r_r_bias is None or self.r_w_bias is None:  # Biases are not shared\n",
    "            self.r_r_bias = self.add_weight(\n",
    "                shape=(self.n_head, self.d_head), initializer=\"zeros\", trainable=True, name=\"r_r_bias\"\n",
    "            )\n",
    "            self.r_w_bias = self.add_weight(\n",
    "                shape=(self.n_head, self.d_head), initializer=\"zeros\", trainable=True, name=\"r_w_bias\"\n",
    "            )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def _rel_shift(self, x):\n",
    "        \"\"\"\n",
    "        relative attention을 수행하기 위한 masking\n",
    "        \"\"\"\n",
    "\n",
    "        x_size = shape_list(x)\n",
    "\n",
    "        x = tf.pad(x, [[0, 0], [1, 0], [0, 0], [0, 0]])\n",
    "        x = tf.reshape(x, [x_size[1] + 1, x_size[0], x_size[2], x_size[3]])\n",
    "        x = tf.slice(x, [1, 0, 0, 0], [-1, -1, -1, -1])\n",
    "        x = tf.reshape(x, x_size)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def call(self, w, r, attn_mask, mems, head_mask, output_attentions, training=False):\n",
    "        \"\"\"\n",
    "                w는 embedding, r은 postional embedding\n",
    "                \"\"\"\n",
    "\n",
    "                qlen, rlen, bsz = shape_list(w)[0], shape_list(r)[0], shape_list(w)[1]\n",
    "\n",
    "        if mems is not None:\n",
    "            cat = tf.concat([mems, w], 0)\n",
    "            if self.pre_lnorm:\n",
    "                w_heads = self.qkv_net(self.layer_norm(cat))\n",
    "            else:\n",
    "                w_heads = self.qkv_net(cat)\n",
    "            r_head_k = self.r_net(r)\n",
    "\n",
    "            w_head_q, w_head_k, w_head_v = tf.split(w_heads, 3, axis=-1)\n",
    "            w_head_q = w_head_q[-qlen:]\n",
    "        else:\n",
    "            if self.pre_lnorm:\n",
    "                w_heads = self.qkv_net(self.layer_norm(w))\n",
    "            else:\n",
    "                w_heads = self.qkv_net(w)\n",
    "            r_head_k = self.r_net(r)\n",
    "\n",
    "            w_head_q, w_head_k, w_head_v = tf.split(w_heads, 3, axis=-1)\n",
    "\n",
    "        klen = shape_list(w_head_k)[0]\n",
    "\n",
    "        w_head_q = tf.reshape(w_head_q, (qlen, bsz, self.n_head, self.d_head))  # qlen x bsz x n_head x d_head\n",
    "        w_head_k = tf.reshape(w_head_k, (klen, bsz, self.n_head, self.d_head))  # qlen x bsz x n_head x d_head\n",
    "        w_head_v = tf.reshape(w_head_v, (klen, bsz, self.n_head, self.d_head))  # qlen x bsz x n_head x d_head\n",
    "\n",
    "        r_head_k = tf.reshape(r_head_k, (rlen, self.n_head, self.d_head))  # qlen x n_head x d_head\n",
    "\n",
    "        # compute attention score\n",
    "        rw_head_q = w_head_q + self.r_w_bias  # qlen x bsz x n_head x d_head\n",
    "        AC = tf.einsum(\"ibnd,jbnd->ijbn\", rw_head_q, w_head_k)  # qlen x klen x bsz x n_head\n",
    "\n",
    "        rr_head_q = w_head_q + self.r_r_bias\n",
    "        BD = tf.einsum(\"ibnd,jnd->ijbn\", rr_head_q, r_head_k)  # qlen x klen x bsz x n_head\n",
    "        BD = self._rel_shift(BD)\n",
    "\n",
    "        # [qlen x klen x bsz x n_head]\n",
    "        attn_score = AC + BD\n",
    "        attn_score = attn_score * self.scale\n",
    "\n",
    "        # compute attention probability\n",
    "        if attn_mask is not None:\n",
    "            attn_mask_t = attn_mask[:, :, None, None]\n",
    "            attn_score = attn_score * (1 - attn_mask_t) - 1e30 * attn_mask_t\n",
    "\n",
    "        # [qlen x klen x bsz x n_head]\n",
    "        attn_prob = tf.nn.softmax(attn_score, axis=1)\n",
    "        attn_prob = self.dropatt(attn_prob, training=training)\n",
    "\n",
    "        if head_mask is not None:\n",
    "            attn_prob = attn_prob * head_mask\n",
    "\n",
    "        # compute attention vector\n",
    "        attn_vec = tf.einsum(\"ijbn,jbnd->ibnd\", attn_prob, w_head_v)\n",
    "\n",
    "        # [qlen x bsz x n_head x d_head]\n",
    "        attn_vec_sizes = shape_list(attn_vec)\n",
    "        attn_vec = tf.reshape(attn_vec, (attn_vec_sizes[0], attn_vec_sizes[1], self.n_head * self.d_head))\n",
    "\n",
    "        # linear projection\n",
    "        attn_out = self.o_net(attn_vec)\n",
    "        attn_out = self.drop(attn_out, training=training)\n",
    "\n",
    "        if self.pre_lnorm:\n",
    "            # residual connection\n",
    "            outputs = [w + attn_out]\n",
    "        else:\n",
    "            # residual connection + layer normalization\n",
    "            outputs = [self.layer_norm(w + attn_out)]\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs.append(attn_prob)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class TFRelPartialLearnableDecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_head,\n",
    "        d_model,\n",
    "        d_head,\n",
    "        d_inner,\n",
    "        dropout,\n",
    "        dropatt=0.0,\n",
    "        pre_lnorm=False,\n",
    "        r_w_bias=None,\n",
    "        r_r_bias=None,\n",
    "        layer_norm_epsilon=1e-5,\n",
    "        init_std=0.02,\n",
    "        output_attentions=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.dec_attn = TFRelPartialLearnableMultiHeadAttn(\n",
    "            n_head,\n",
    "            d_model,\n",
    "            d_head,\n",
    "            dropout,\n",
    "            dropatt=dropatt,\n",
    "            pre_lnorm=pre_lnorm,\n",
    "            r_w_bias=r_w_bias,\n",
    "            r_r_bias=r_r_bias,\n",
    "            init_std=init_std,\n",
    "            layer_norm_epsilon=layer_norm_epsilon,\n",
    "            output_attentions=output_attentions,\n",
    "            name=\"dec_attn\",\n",
    "        )\n",
    "        self.pos_ff = TFPositionwiseFF(\n",
    "            d_model,\n",
    "            d_inner,\n",
    "            dropout,\n",
    "            pre_lnorm=pre_lnorm,\n",
    "            init_std=init_std,\n",
    "            layer_norm_epsilon=layer_norm_epsilon,\n",
    "            name=\"pos_ff\",\n",
    "        )\n",
    "\n",
    "    def call(self, dec_inp, r, dec_attn_mask, mems, head_mask, output_attentions, training=False):\n",
    "        attn_outputs = self.dec_attn(dec_inp, r, dec_attn_mask, mems, head_mask, output_attentions, training=training)\n",
    "        ff_output = self.pos_ff(attn_outputs[0], training=training)\n",
    "\n",
    "        outputs = [ff_output] + attn_outputs[1:]\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class TFAdaptiveEmbedding(tf.keras.layers.Layer):\n",
    "\"\"\"\n",
    "n개의 token을 한 번에 임베딩하는 것이 아니라 먼저 cutoff를 통해 n개의 tokens을 나누고 mask_i = (inp_flat >= l_idx) & (inp_flat < r_idx) 을 만족하는 토큰들만을 임베딩하는 방식\n",
    "\"\"\"\n",
    "\n",
    "    def __init__(self, n_token, d_embed, d_proj, cutoffs, div_val=1, init_std=0.02, sample_softmax=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.n_token = n_token\n",
    "        self.d_embed = d_embed\n",
    "        self.init_std = init_std\n",
    "\n",
    "        self.cutoffs = cutoffs + [n_token]\n",
    "        self.div_val = div_val\n",
    "        self.d_proj = d_proj\n",
    "\n",
    "        self.emb_scale = d_proj ** 0.5\n",
    "\n",
    "        self.cutoff_ends = [0] + self.cutoffs\n",
    "\n",
    "        self.emb_layers = []\n",
    "        self.emb_projs = []\n",
    "        if div_val == 1:\n",
    "            raise NotImplementedError  # Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint\n",
    "        else:\n",
    "            for i in range(len(self.cutoffs)):\n",
    "                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n",
    "                d_emb_i = d_embed // (div_val ** i)\n",
    "                self.emb_layers.append(\n",
    "                    tf.keras.layers.Embedding(\n",
    "                        r_idx - l_idx,\n",
    "                        d_emb_i,\n",
    "                        embeddings_initializer=get_initializer(init_std),\n",
    "                        name=\"emb_layers_._{}\".format(i),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        for i in range(len(self.cutoffs)):\n",
    "            d_emb_i = self.d_embed // (self.div_val ** i)\n",
    "            self.emb_projs.append(\n",
    "                self.add_weight(\n",
    "                    shape=(d_emb_i, self.d_proj),\n",
    "                    initializer=get_initializer(self.init_std),\n",
    "                    trainable=True,\n",
    "                    name=\"emb_projs_._{}\".format(i),\n",
    "                )\n",
    "            )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inp):\n",
    "        if self.div_val == 1:\n",
    "            raise NotImplementedError  # Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint\n",
    "        else:\n",
    "            inp_flat = tf.reshape(inp, (-1,))\n",
    "            emb_flat = tf.zeros([shape_list(inp_flat)[0], self.d_proj])\n",
    "            for i in range(len(self.cutoffs)):\n",
    "                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n",
    "\n",
    "                mask_i = (inp_flat >= l_idx) & (inp_flat < r_idx)\n",
    "\n",
    "                inp_i = tf.boolean_mask(inp_flat, mask_i) - l_idx\n",
    "                emb_i = self.emb_layers[i](inp_i)\n",
    "                emb_i = tf.einsum(\"id,de->ie\", emb_i, self.emb_projs[i])\n",
    "\n",
    "                mask_idx = tf.cast(tf.where(mask_i), dtype=tf.int64)\n",
    "                emb_flat += tf.scatter_nd(mask_idx, emb_i, tf.cast(shape_list(emb_flat), dtype=tf.int64))\n",
    "\n",
    "            embed_shape = shape_list(inp) + [self.d_proj]\n",
    "            embed = tf.reshape(emb_flat, embed_shape)\n",
    "\n",
    "        embed *= self.emb_scale\n",
    "\n",
    "        return embed\n",
    "\n",
    "\n",
    "@keras_serializable\n",
    "class TFTransfoXLMainLayer(tf.keras.layers.Layer):\n",
    "    config_class = TransfoXLConfig\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_hidden_states = config.output_hidden_states\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.return_dict = config.use_return_dict\n",
    "\n",
    "        self.n_token = config.vocab_size\n",
    "\n",
    "        self.d_embed = config.d_embed\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "        self.untie_r = config.untie_r\n",
    "\n",
    "        self.word_emb = TFAdaptiveEmbedding(\n",
    "            config.vocab_size,\n",
    "            config.d_embed,\n",
    "            config.d_model,\n",
    "            config.cutoffs,\n",
    "            div_val=config.div_val,\n",
    "            init_std=config.init_std,\n",
    "            name=\"word_emb\",\n",
    "        )\n",
    "\n",
    "        self.drop = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "        self.n_layer = config.n_layer\n",
    "        self.mem_len = config.mem_len\n",
    "        self.attn_type = config.attn_type\n",
    "\n",
    "        self.layers = []\n",
    "        if config.attn_type == 0:  # the default attention\n",
    "            for i in range(config.n_layer):\n",
    "                self.layers.append(\n",
    "                    TFRelPartialLearnableDecoderLayer(\n",
    "                        config.n_head,\n",
    "                        config.d_model,\n",
    "                        config.d_head,\n",
    "                        config.d_inner,\n",
    "                        config.dropout,\n",
    "                        dropatt=config.dropatt,\n",
    "                        pre_lnorm=config.pre_lnorm,\n",
    "                        r_w_bias=None if self.untie_r else self.r_w_bias,\n",
    "                        r_r_bias=None if self.untie_r else self.r_r_bias,\n",
    "                        layer_norm_epsilon=config.layer_norm_epsilon,\n",
    "                        init_std=config.init_std,\n",
    "                        output_attentions=self.output_attentions,\n",
    "                        name=\"layers_._{}\".format(i),\n",
    "                    )\n",
    "                )\n",
    "        else:  # learnable embeddings and absolute embeddings\n",
    "            raise NotImplementedError  # Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint\n",
    "\n",
    "        self.same_length = config.same_length\n",
    "        self.clamp_len = config.clamp_len\n",
    "\n",
    "        if self.attn_type == 0:  # default attention\n",
    "            self.pos_emb = TFPositionalEmbedding(self.d_model, name=\"pos_emb\")\n",
    "        else:  # learnable embeddings and absolute embeddings\n",
    "            raise NotImplementedError  # Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if not self.untie_r:\n",
    "            self.r_w_bias = self.add_weight(\n",
    "                shape=(self.n_head, self.d_head), initializer=\"zeros\", trainable=True, name=\"r_w_bias\"\n",
    "            )\n",
    "            self.r_r_bias = self.add_weight(\n",
    "                shape=(self.n_head, self.d_head), initializer=\"zeros\", trainable=True, name=\"r_r_bias\"\n",
    "            )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.word_emb\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _resize_token_embeddings(self, new_num_tokens):\n",
    "        return self.word_emb\n",
    "\n",
    "    def backward_compatible(self):\n",
    "        self.sample_softmax = -1\n",
    "\n",
    "    def reset_memory_length(self, mem_len):\n",
    "        self.mem_len = mem_len\n",
    "\n",
    "    def _prune_heads(self, heads):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def init_mems(self, bsz):\n",
    "        if self.mem_len > 0:\n",
    "            mems = []\n",
    "            for i in range(self.n_layer):\n",
    "                empty = tf.zeros([self.mem_len, bsz, self.d_model])\n",
    "                mems.append(empty)\n",
    "\n",
    "            return mems\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _update_mems(self, hids, mems, mlen, qlen):\n",
    "        \"\"\"\n",
    "        한 칸씩 슬라이딩하며, memory에 새로운 segment를 추가합니다. 이때, tf.stop_gradient를 통해 이전부터 보았던 segment는 gradient가 더 이상 흐르지 않도록 tf.stop_gradient를 사용\n",
    "        \"\"\"\n",
    "        if mems is None:\n",
    "            return None\n",
    "\n",
    "        # mems is not None\n",
    "        assert len(hids) == len(mems), \"len(hids) != len(mems)\"\n",
    "\n",
    "        # `mlen + qlen` steps\n",
    "        new_mems = []\n",
    "        end_idx = mlen + max(0, qlen)\n",
    "        beg_idx = max(0, end_idx - self.mem_len)\n",
    "        for i in range(len(hids)):\n",
    "\n",
    "            cat = tf.concat([mems[i], hids[i]], axis=0)\n",
    "            tf.stop_gradient(cat)\n",
    "            new_mems.append(cat[beg_idx:end_idx])\n",
    "\n",
    "        return new_mems\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        inputs,\n",
    "        mems=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        training=False,\n",
    "    ):\n",
    "        if isinstance(inputs, (tuple, list)):\n",
    "            input_ids = inputs[0]\n",
    "            mems = inputs[1] if len(inputs) > 1 else mems\n",
    "            head_mask = inputs[2] if len(inputs) > 2 else head_mask\n",
    "            inputs_embeds = inputs[3] if len(inputs) > 3 else inputs_embeds\n",
    "            output_attentions = inputs[4] if len(inputs) > 4 else output_attentions\n",
    "            output_hidden_states = inputs[5] if len(inputs) > 5 else output_hidden_states\n",
    "            return_dict = inputs[6] if len(inputs) > 6 else return_dict\n",
    "            assert len(inputs) <= 7, \"Too many inputs.\"\n",
    "        elif isinstance(inputs, (dict, BatchEncoding)):\n",
    "            input_ids = inputs.get(\"input_ids\")\n",
    "            mems = inputs.get(\"mems\", mems)\n",
    "            head_mask = inputs.get(\"head_mask\", head_mask)\n",
    "            inputs_embeds = inputs.get(\"inputs_embeds\", inputs_embeds)\n",
    "            output_attentions = inputs.get(\"output_attentions\", output_attentions)\n",
    "            output_hidden_states = inputs.get(\"output_hidden_states\", output_hidden_states)\n",
    "            return_dict = inputs.get(\"return_dict\", return_dict)\n",
    "            assert len(inputs) <= 7, \"Too many inputs.\"\n",
    "        else:\n",
    "            input_ids = inputs\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n",
    "        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.output_hidden_states\n",
    "        return_dict = return_dict if return_dict is not None else self.return_dict\n",
    "\n",
    "        # the original code for Transformer-XL used shapes [len, bsz] but we want a unified interface in the library\n",
    "        # so we transpose here from shape [bsz, len] to shape [len, bsz]\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_ids = tf.transpose(input_ids, perm=(1, 0))\n",
    "            qlen, bsz = shape_list(input_ids)\n",
    "        elif inputs_embeds is not None:\n",
    "            inputs_embeds = tf.transpose(inputs_embeds, perm=(1, 0, 2))\n",
    "            qlen, bsz = shape_list(inputs_embeds)[:2]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        if mems is None:\n",
    "            mems = self.init_mems(bsz)\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer)\n",
    "        # and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head]\n",
    "        if head_mask is not None:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            head_mask = [None] * self.n_layer\n",
    "\n",
    "        if inputs_embeds is not None:\n",
    "            word_emb = inputs_embeds\n",
    "        else:\n",
    "            word_emb = self.word_emb(input_ids)\n",
    "\n",
    "        mlen = shape_list(mems[0])[0] if mems is not None else 0\n",
    "        klen = mlen + qlen\n",
    "\n",
    "        attn_mask = tf.ones([qlen, qlen])\n",
    "        mask_u = tf.linalg.band_part(attn_mask, 0, -1)\n",
    "        mask_dia = tf.linalg.band_part(attn_mask, 0, 0)\n",
    "        attn_mask_pad = tf.zeros([qlen, mlen])\n",
    "        dec_attn_mask = tf.concat([attn_mask_pad, mask_u - mask_dia], 1)\n",
    "        if self.same_length:\n",
    "            mask_l = tf.linalg.band_part(attn_mask, -1, 0)\n",
    "            dec_attn_mask = tf.concat([dec_attn_mask[:, :qlen] + mask_l - mask_dia, dec_attn_mask[:, qlen:]], 1)\n",
    "        # ::: PyTorch masking code for reference :::\n",
    "        # if self.same_length:\n",
    "        #     all_ones = word_emb.new_ones((qlen, klen), dtype=torch.uint8)\n",
    "        #     mask_len = klen - self.mem_len\n",
    "        #     if mask_len > 0:\n",
    "        #         mask_shift_len = qlen - mask_len\n",
    "        #     else:\n",
    "        #         mask_shift_len = qlen\n",
    "        #     dec_attn_mask = (torch.triu(all_ones, 1+mlen)\n",
    "        #             + torch.tril(all_ones, -mask_shift_len))[:, :, None] # -1\n",
    "        # else:\n",
    "        #     dec_attn_mask = torch.triu(\n",
    "        #         word_emb.new_ones((qlen, klen), dtype=torch.uint8), diagonal=1+mlen)[:,:,None]\n",
    "\n",
    "        hids = []\n",
    "        attentions = [] if output_attentions else None\n",
    "        if self.attn_type == 0:  # default\n",
    "            pos_seq = tf.range(klen - 1, -1, -1.0)\n",
    "            if self.clamp_len > 0:\n",
    "                pos_seq = tf.minimum(pos_seq, self.clamp_len)\n",
    "            pos_emb = self.pos_emb(pos_seq)\n",
    "\n",
    "            core_out = self.drop(word_emb, training=training)\n",
    "            pos_emb = self.drop(pos_emb, training=training)\n",
    "\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                hids.append(core_out)\n",
    "                mems_i = None if mems is None else mems[i]\n",
    "                layer_outputs = layer(\n",
    "                    core_out,\n",
    "                    pos_emb,\n",
    "                    dec_attn_mask,\n",
    "                    mems_i,\n",
    "                    head_mask[i],\n",
    "                    output_attentions,\n",
    "                    training=training,\n",
    "                )\n",
    "                core_out = layer_outputs[0]\n",
    "                if output_attentions:\n",
    "                    attentions.append(layer_outputs[1])\n",
    "        else:  # learnable embeddings and absolute embeddings\n",
    "            raise NotImplementedError\n",
    "\n",
    "        core_out = self.drop(core_out, training=training)\n",
    "\n",
    "        new_mems = self._update_mems(hids, mems, mlen, qlen)\n",
    "\n",
    "        # [bsz, len, hidden_dim]\n",
    "        core_out = tf.transpose(core_out, perm=(1, 0, 2))\n",
    "\n",
    "        if output_hidden_states:\n",
    "            # last layer를 추가하고 다시 library standard shape [bsz, len, hidden_dim]으로 transpose\n",
    "            hids.append(core_out)\n",
    "            hids = tuple(tf.transpose(t, perm=(1, 0, 2)) for t in hids)\n",
    "        else:\n",
    "            hids = None\n",
    "        if output_attentions:\n",
    "            # Transpose to library standard shape [bsz, n_heads, query_seq_len, key_seq_len]\n",
    "            attentions = tuple(tf.transpose(t, perm=(2, 3, 0, 1)) for t in attentions)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [core_out, new_mems, hids, attentions] if v is not None)\n",
    "\n",
    "        return TFTransfoXLModelOutput(\n",
    "            last_hidden_state=core_out,\n",
    "            mems=new_mems,\n",
    "            hidden_states=hids,\n",
    "            attentions=attentions,\n",
    "        )\n",
    "\n",
    "class TFAdaptiveSoftmaxMask(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_embed, d_proj, cutoffs, div_val=1, keep_order=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_embed = d_embed\n",
    "        self.d_proj = d_proj\n",
    "\n",
    "        self.cutoffs = cutoffs + [vocab_size]\n",
    "        self.cutoff_ends = [0] + self.cutoffs\n",
    "        self.div_val = div_val\n",
    "\n",
    "        self.shortlist_size = self.cutoffs[0]\n",
    "        self.n_clusters = len(self.cutoffs) - 1\n",
    "        self.head_size = self.shortlist_size + self.n_clusters\n",
    "        self.keep_order = keep_order\n",
    "\n",
    "        self.out_layers = []\n",
    "        self.out_projs = []\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.n_clusters > 0:\n",
    "            self.cluster_weight = self.add_weight(\n",
    "                shape=(self.n_clusters, self.d_embed), initializer=\"zeros\", trainable=True, name=\"cluster_weight\"\n",
    "            )\n",
    "            self.cluster_bias = self.add_weight(\n",
    "                shape=(self.n_clusters,), initializer=\"zeros\", trainable=True, name=\"cluster_bias\"\n",
    "            )\n",
    "\n",
    "        if self.div_val == 1:\n",
    "            for i in range(len(self.cutoffs)):\n",
    "                if self.d_proj != self.d_embed:\n",
    "                    weight = self.add_weight(\n",
    "                        shape=(self.d_embed, self.d_proj),\n",
    "                        initializer=\"zeros\",\n",
    "                        trainable=True,\n",
    "                        name=\"out_projs_._{}\".format(i),\n",
    "                    )\n",
    "                    self.out_projs.append(weight)\n",
    "                else:\n",
    "                    self.out_projs.append(None)\n",
    "                weight = self.add_weight(\n",
    "                    shape=(\n",
    "                        self.vocab_size,\n",
    "                        self.d_embed,\n",
    "                    ),\n",
    "                    initializer=\"zeros\",\n",
    "                    trainable=True,\n",
    "                    name=\"out_layers_._{}_._weight\".format(i),\n",
    "                )\n",
    "                bias = self.add_weight(\n",
    "                    shape=(self.vocab_size,),\n",
    "                    initializer=\"zeros\",\n",
    "                    trainable=True,\n",
    "                    name=\"out_layers_._{}_._bias\".format(i),\n",
    "                )\n",
    "                self.out_layers.append((weight, bias))\n",
    "        else:\n",
    "            for i in range(len(self.cutoffs)):\n",
    "                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n",
    "                d_emb_i = self.d_embed // (self.div_val ** i)\n",
    "\n",
    "                weight = self.add_weight(\n",
    "                    shape=(d_emb_i, self.d_proj), initializer=\"zeros\", trainable=True, name=\"out_projs_._{}\".format(i)\n",
    "                )\n",
    "                self.out_projs.append(weight)\n",
    "                weight = self.add_weight(\n",
    "                    shape=(\n",
    "                        r_idx - l_idx,\n",
    "                        d_emb_i,\n",
    "                    ),\n",
    "                    initializer=\"zeros\",\n",
    "                    trainable=True,\n",
    "                    name=\"out_layers_._{}_._weight\".format(i),\n",
    "                )\n",
    "                bias = self.add_weight(\n",
    "                    shape=(r_idx - l_idx,),\n",
    "                    initializer=\"zeros\",\n",
    "                    trainable=True,\n",
    "                    name=\"out_layers_._{}_._bias\".format(i),\n",
    "                )\n",
    "                self.out_layers.append((weight, bias))\n",
    "        super().build(input_shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def _logit(x, W, b, proj=None):\n",
    "        y = x\n",
    "        if proj is not None:\n",
    "            y = tf.einsum(\"ibd,ed->ibe\", y, proj)\n",
    "        return tf.einsum(\"ibd,nd->ibn\", y, W) + b\n",
    "\n",
    "    @staticmethod\n",
    "    def _gather_logprob(logprob, target):\n",
    "        lp_size = shape_list(logprob)\n",
    "        r = tf.range(lp_size[0])\n",
    "        idx = tf.stack([r, target], 1)\n",
    "        return tf.gather_nd(logprob, idx)\n",
    "\n",
    "    def call(self, hidden, target, return_mean=True, training=False):\n",
    "        head_logprob = 0\n",
    "        if self.n_clusters == 0:\n",
    "            output = self._logit(hidden, self.out_layers[0][0], self.out_layers[0][1], self.out_projs[0])\n",
    "            if target is not None:\n",
    "                loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target, logits=output)\n",
    "            out = tf.nn.log_softmax(output, axis=-1)\n",
    "        else:\n",
    "            hidden_sizes = shape_list(hidden)\n",
    "            out = []\n",
    "            loss = tf.zeros(hidden_sizes[:2], dtype=tf.float32)\n",
    "            for i in range(len(self.cutoffs)):\n",
    "                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n",
    "                if target is not None:\n",
    "                    mask = (target >= l_idx) & (target < r_idx)\n",
    "                    mask_idx = tf.where(mask)\n",
    "                    cur_target = tf.boolean_mask(target, mask) - l_idx\n",
    "\n",
    "                if self.div_val == 1:\n",
    "                    cur_W = self.out_layers[0][0][l_idx:r_idx]\n",
    "                    cur_b = self.out_layers[0][1][l_idx:r_idx]\n",
    "                else:\n",
    "                    cur_W = self.out_layers[i][0]\n",
    "                    cur_b = self.out_layers[i][1]\n",
    "\n",
    "                if i == 0:\n",
    "                    cur_W = tf.concat([cur_W, self.cluster_weight], 0)\n",
    "                    cur_b = tf.concat([cur_b, self.cluster_bias], 0)\n",
    "\n",
    "                    head_logit = self._logit(hidden, cur_W, cur_b, self.out_projs[0])\n",
    "                    head_logprob = tf.nn.log_softmax(head_logit)\n",
    "                    out.append(head_logprob[..., : self.cutoffs[0]])\n",
    "                    if target is not None:\n",
    "                        cur_head_logprob = tf.boolean_mask(head_logprob, mask)\n",
    "                        cur_logprob = self._gather_logprob(cur_head_logprob, cur_target)\n",
    "                else:\n",
    "                    tail_logit = self._logit(hidden, cur_W, cur_b, self.out_projs[i])\n",
    "                    tail_logprob = tf.nn.log_softmax(tail_logit)\n",
    "                    cluster_prob_idx = self.cutoffs[0] + i - 1  # No probability for the head cluster\n",
    "                    logprob_i = head_logprob[..., cluster_prob_idx, None] + tail_logprob\n",
    "                    out.append(logprob_i)\n",
    "                    if target is not None:\n",
    "                        cur_head_logprob = tf.boolean_mask(head_logprob, mask)\n",
    "                        cur_tail_logprob = tf.boolean_mask(tail_logprob, mask)\n",
    "                        cur_logprob = self._gather_logprob(cur_tail_logprob, cur_target)\n",
    "                        cur_logprob += cur_head_logprob[:, self.cutoff_ends[1] + i - 1]\n",
    "                if target is not None:\n",
    "                    loss += tf.scatter_nd(mask_idx, -cur_logprob, tf.cast(shape_list(loss), dtype=tf.int64))\n",
    "            out = tf.concat(out, axis=-1)\n",
    "\n",
    "        if target is not None:\n",
    "            if return_mean:\n",
    "                loss = tf.reduce_mean(loss)\n",
    "            # `self.add_loss()`를 통해 training시의 loss 추가\n",
    "            self.add_loss(loss)\n",
    "\n",
    "            # Log the loss as a metric\n",
    "            self.add_metric(loss, name=self.name, aggregation=\"mean\" if return_mean else \"\")\n",
    "\n",
    "        return out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13-7. XLNet, BART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class TFXLNetRelativeAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        if config.d_model % config.n_head != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (config.d_model, config.n_head)\n",
    "            )\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "        self.d_model = config.d_model\n",
    "        self.scale = 1 / (config.d_head ** 0.5)\n",
    "        self.initializer_range = config.initializer_range\n",
    "        self.output_attentions = config.output_attentions\n",
    "\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layer_norm\")\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        initializer = get_initializer(self.initializer_range)\n",
    "        self.q = self.add_weight(\n",
    "            shape=(self.d_model, self.n_head, self.d_head), initializer=initializer, trainable=True, name=\"q\"\n",
    "        )\n",
    "        self.k = self.add_weight(\n",
    "            shape=(self.d_model, self.n_head, self.d_head), initializer=initializer, trainable=True, name=\"k\"\n",
    "        )\n",
    "        self.v = self.add_weight(\n",
    "            shape=(self.d_model, self.n_head, self.d_head), initializer=initializer, trainable=True, name=\"v\"\n",
    "        )\n",
    "        self.o = self.add_weight(\n",
    "            shape=(self.d_model, self.n_head, self.d_head), initializer=initializer, trainable=True, name=\"o\"\n",
    "        )\n",
    "        self.r = self.add_weight(\n",
    "            shape=(self.d_model, self.n_head, self.d_head), initializer=initializer, trainable=True, name=\"r\"\n",
    "        )\n",
    "        self.r_r_bias = self.add_weight(\n",
    "            shape=(self.n_head, self.d_head), initializer=\"zeros\", trainable=True, name=\"r_r_bias\"\n",
    "        )\n",
    "        self.r_s_bias = self.add_weight(\n",
    "            shape=(self.n_head, self.d_head), initializer=\"zeros\", trainable=True, name=\"r_s_bias\"\n",
    "        )\n",
    "        self.r_w_bias = self.add_weight(\n",
    "            shape=(self.n_head, self.d_head), initializer=\"zeros\", trainable=True, name=\"r_w_bias\"\n",
    "        )\n",
    "        self.seg_embed = self.add_weight(\n",
    "            shape=(2, self.n_head, self.d_head), initializer=initializer, trainable=True, name=\"seg_embed\"\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def rel_shift(self, x, klen=-1):\n",
    "        \"\"\"perform relative shift to form the relative attention score.\"\"\"\n",
    "        x_size = shape_list(x)\n",
    "\n",
    "        x = tf.reshape(x, (x_size[1], x_size[0], x_size[2], x_size[3]))\n",
    "        x = x[1:, ...]\n",
    "        x = tf.reshape(x, (x_size[0], x_size[1] - 1, x_size[2], x_size[3]))\n",
    "        x = x[:, 0:klen, :, :]\n",
    "        # x = torch.index_select(x, 1, torch.arange(klen, device=x.device, dtype=torch.long))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def rel_attn_core(\n",
    "        self, q_head, k_head_h, v_head_h, k_head_r, seg_mat, attn_mask, head_mask, output_attentions, training=False\n",
    "    ):\n",
    "        \"\"\"Core relative positional attention operations.\"\"\"\n",
    "        # content based attention score\n",
    "        ac = tf.einsum(\"ibnd,jbnd->ijbn\", q_head + self.r_w_bias, k_head_h)\n",
    "\n",
    "        # position based attention score\n",
    "        bd = tf.einsum(\"ibnd,jbnd->ijbn\", q_head + self.r_r_bias, k_head_r)\n",
    "        bd = self.rel_shift(bd, klen=shape_list(ac)[1])\n",
    "\n",
    "        # segment based attention score\n",
    "        if seg_mat is None:\n",
    "            ef = 0\n",
    "        else:\n",
    "            ef = tf.einsum(\"ibnd,snd->ibns\", q_head + self.r_s_bias, self.seg_embed)\n",
    "            ef = tf.einsum(\"ijbs,ibns->ijbn\", seg_mat, ef)\n",
    "\n",
    "        # merge attention scores and perform masking\n",
    "        attn_score = (ac + bd + ef) * self.scale\n",
    "        if attn_mask is not None:\n",
    "            # attn_score = attn_score * (1 - attn_mask) - 1e30 * attn_mask\n",
    "            if attn_mask.dtype == tf.float16:\n",
    "                attn_score = attn_score - 65500 * attn_mask\n",
    "            else:\n",
    "                attn_score = attn_score - 1e30 * attn_mask\n",
    "\n",
    "        # attention probability\n",
    "        attn_prob = tf.nn.softmax(attn_score, axis=1)\n",
    "\n",
    "        attn_prob = self.dropout(attn_prob, training=training)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attn_prob = attn_prob * head_mask\n",
    "\n",
    "        # attention output\n",
    "        attn_vec = tf.einsum(\"ijbn,jbnd->ibnd\", attn_prob, v_head_h)\n",
    "\n",
    "        if output_attentions:\n",
    "            return attn_vec, attn_prob\n",
    "\n",
    "        return attn_vec\n",
    "\n",
    "    def post_attention(self, h, attn_vec, residual=True, training=False):\n",
    "        \"\"\"Post-attention processing.\"\"\"\n",
    "        # post-attention projection (back to `d_model`)\n",
    "        attn_out = tf.einsum(\"ibnd,hnd->ibh\", attn_vec, self.o)\n",
    "\n",
    "        attn_out = self.dropout(attn_out, training=training)\n",
    "\n",
    "        if residual:\n",
    "            attn_out = attn_out + h\n",
    "        output = self.layer_norm(attn_out)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        h,\n",
    "        g,\n",
    "        attn_mask_h,\n",
    "        attn_mask_g,\n",
    "        r,\n",
    "        seg_mat,\n",
    "        mems,\n",
    "        target_mapping,\n",
    "        head_mask,\n",
    "        output_attentions,\n",
    "        training=False,\n",
    "    ):\n",
    "        if g is not None:\n",
    "            # Two-stream attention with relative positional encoding.\n",
    "            # content based attention score\n",
    "            if mems is not None and len(shape_list(mems)) > 1:\n",
    "                cat = tf.concat([mems, h], axis=0)\n",
    "            else:\n",
    "                cat = h\n",
    "\n",
    "            # content-based key head\n",
    "            k_head_h = tf.einsum(\"ibh,hnd->ibnd\", cat, self.k)\n",
    "\n",
    "            # content-based value head\n",
    "            v_head_h = tf.einsum(\"ibh,hnd->ibnd\", cat, self.v)\n",
    "\n",
    "            # position-based key head\n",
    "            k_head_r = tf.einsum(\"ibh,hnd->ibnd\", r, self.r)\n",
    "\n",
    "            # h-stream\n",
    "            # content-stream query head\n",
    "            q_head_h = tf.einsum(\"ibh,hnd->ibnd\", h, self.q)\n",
    "\n",
    "            # core attention ops\n",
    "            attn_vec_h = self.rel_attn_core(\n",
    "                q_head_h,\n",
    "                k_head_h,\n",
    "                v_head_h,\n",
    "                k_head_r,\n",
    "                seg_mat,\n",
    "                attn_mask_h,\n",
    "                head_mask,\n",
    "                output_attentions,\n",
    "                training=training,\n",
    "            )\n",
    "\n",
    "            if output_attentions:\n",
    "                attn_vec_h, attn_prob_h = attn_vec_h\n",
    "\n",
    "            # post processing\n",
    "            output_h = self.post_attention(h, attn_vec_h, training=training)\n",
    "\n",
    "            # g-stream\n",
    "            # query-stream query head\n",
    "            q_head_g = tf.einsum(\"ibh,hnd->ibnd\", g, self.q)\n",
    "\n",
    "            # core attention ops\n",
    "            if target_mapping is not None:\n",
    "                q_head_g = tf.einsum(\"mbnd,mlb->lbnd\", q_head_g, target_mapping)\n",
    "                attn_vec_g = self.rel_attn_core(\n",
    "                    q_head_g,\n",
    "                    k_head_h,\n",
    "                    v_head_h,\n",
    "                    k_head_r,\n",
    "                    seg_mat,\n",
    "                    attn_mask_g,\n",
    "                    head_mask,\n",
    "                    output_attentions,\n",
    "                    training=training,\n",
    "                )\n",
    "\n",
    "                if output_attentions:\n",
    "                    attn_vec_g, attn_prob_g = attn_vec_g\n",
    "\n",
    "                attn_vec_g = tf.einsum(\"lbnd,mlb->mbnd\", attn_vec_g, target_mapping)\n",
    "            else:\n",
    "                attn_vec_g = self.rel_attn_core(\n",
    "                    q_head_g,\n",
    "                    k_head_h,\n",
    "                    v_head_h,\n",
    "                    k_head_r,\n",
    "                    seg_mat,\n",
    "                    attn_mask_g,\n",
    "                    head_mask,\n",
    "                    output_attentions,\n",
    "                    training=training,\n",
    "                )\n",
    "\n",
    "                if output_attentions:\n",
    "                    attn_vec_g, attn_prob_g = attn_vec_g\n",
    "\n",
    "            # post processing\n",
    "            output_g = self.post_attention(g, attn_vec_g, training=training)\n",
    "\n",
    "            if output_attentions:\n",
    "                attn_prob = attn_prob_h, attn_prob_g\n",
    "\n",
    "        else:\n",
    "            # Multi-head attention with relative positional encoding\n",
    "            if mems is not None and len(shape_list(mems)) > 1:\n",
    "                cat = tf.concat([mems, h], axis=0)\n",
    "            else:\n",
    "                cat = h\n",
    "\n",
    "            # content heads\n",
    "            q_head_h = tf.einsum(\"ibh,hnd->ibnd\", h, self.q)\n",
    "            k_head_h = tf.einsum(\"ibh,hnd->ibnd\", cat, self.k)\n",
    "            v_head_h = tf.einsum(\"ibh,hnd->ibnd\", cat, self.v)\n",
    "\n",
    "            # positional heads\n",
    "            k_head_r = tf.einsum(\"ibh,hnd->ibnd\", r, self.r)\n",
    "\n",
    "            # core attention ops\n",
    "            attn_vec = self.rel_attn_core(\n",
    "                q_head_h,\n",
    "                k_head_h,\n",
    "                v_head_h,\n",
    "                k_head_r,\n",
    "                seg_mat,\n",
    "                attn_mask_h,\n",
    "                head_mask,\n",
    "                output_attentions,\n",
    "                training=training,\n",
    "            )\n",
    "\n",
    "            if output_attentions:\n",
    "                attn_vec, attn_prob = attn_vec\n",
    "\n",
    "            # post processing\n",
    "            output_h = self.post_attention(h, attn_vec, training=training)\n",
    "            output_g = None\n",
    "\n",
    "        outputs = (output_h, output_g)\n",
    "        if output_attentions:\n",
    "            outputs = outputs + (attn_prob,)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class TFXLNetFeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layer_norm\")\n",
    "        self.layer_1 = tf.keras.layers.Dense(\n",
    "            config.d_inner, kernel_initializer=get_initializer(config.initializer_range), name=\"layer_1\"\n",
    "        )\n",
    "        self.layer_2 = tf.keras.layers.Dense(\n",
    "            config.d_model, kernel_initializer=get_initializer(config.initializer_range), name=\"layer_2\"\n",
    "        )\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "        if isinstance(config.ff_activation, str):\n",
    "            self.activation_function = get_tf_activation(config.ff_activation)\n",
    "        else:\n",
    "            self.activation_function = config.ff_activation\n",
    "\n",
    "    def call(self, inp, training=False):\n",
    "        output = inp\n",
    "        output = self.layer_1(output)\n",
    "        output = self.activation_function(output)\n",
    "        output = self.dropout(output, training=training)\n",
    "        output = self.layer_2(output)\n",
    "        output = self.dropout(output, training=training)\n",
    "        output = self.layer_norm(output + inp)\n",
    "        return output\n",
    "\n",
    "\n",
    "class TFXLNetLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.rel_attn = TFXLNetRelativeAttention(config, name=\"rel_attn\")\n",
    "        self.ff = TFXLNetFeedForward(config, name=\"ff\")\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        output_h,\n",
    "        output_g,\n",
    "        non_tgt_mask,\n",
    "        attn_mask,\n",
    "        pos_emb,\n",
    "        seg_mat,\n",
    "        mems,\n",
    "        target_mapping,\n",
    "        head_mask,\n",
    "        output_attentions,\n",
    "        training=False,\n",
    "    ):\n",
    "        outputs = self.rel_attn(\n",
    "            output_h,\n",
    "            output_g,\n",
    "            non_tgt_mask,\n",
    "            attn_mask,\n",
    "            pos_emb,\n",
    "            seg_mat,\n",
    "            mems,\n",
    "            target_mapping,\n",
    "            head_mask,\n",
    "            output_attentions,\n",
    "            training=training,\n",
    "        )\n",
    "        output_h, output_g = outputs[:2]\n",
    "\n",
    "        if output_g is not None:\n",
    "            output_g = self.ff(output_g, training=training)\n",
    "        output_h = self.ff(output_h, training=training)\n",
    "\n",
    "        outputs = (output_h, output_g) + outputs[2:]  # Add again attentions if there are there\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class TFXLNetLMHead(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, input_embeddings, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        # The output weights are the same as the input embeddings, but there is\n",
    "        # an output-only bias for each token.\n",
    "        self.input_embeddings = input_embeddings\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bias = self.add_weight(shape=(self.vocab_size,), initializer=\"zeros\", trainable=True, name=\"bias\")\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, hidden_states):\n",
    "        hidden_states = self.input_embeddings(hidden_states, mode=\"linear\")\n",
    "        hidden_states = hidden_states + self.bias\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "@keras_serializable\n",
    "class TFXLNetMainLayer(tf.keras.layers.Layer):\n",
    "    config_class = XLNetConfig\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.config = config\n",
    "        self.output_hidden_states = config.output_hidden_states\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.return_dict = config.return_dict\n",
    "\n",
    "        self.mem_len = config.mem_len\n",
    "        self.reuse_len = config.reuse_len\n",
    "        self.d_model = config.d_model\n",
    "        self.same_length = config.same_length\n",
    "        self.attn_type = config.attn_type\n",
    "        self.bi_data = config.bi_data\n",
    "        self.clamp_len = config.clamp_len\n",
    "        self.n_layer = config.n_layer\n",
    "        self.use_bfloat16 = config.use_bfloat16\n",
    "        self.initializer_range = config.initializer_range\n",
    "\n",
    "        self.word_embedding = TFSharedEmbeddings(\n",
    "            config.vocab_size, config.d_model, initializer_range=config.initializer_range, name=\"word_embedding\"\n",
    "        )\n",
    "        self.layer = [TFXLNetLayer(config, name=\"layer_._{}\".format(i)) for i in range(config.n_layer)]\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "        self.use_mems_eval = config.use_mems_eval\n",
    "        self.use_mems_train = config.use_mems_train\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.word_embedding\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.word_embedding.weight = value\n",
    "        self.word_embedding.vocab_size = value.shape[0]\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        initializer = get_initializer(self.initializer_range)\n",
    "        self.mask_emb = self.add_weight(\n",
    "            shape=(1, 1, self.d_model), initializer=initializer, trainable=True, name=\"mask_emb\"\n",
    "        )\n",
    "\n",
    "    def _resize_token_embeddings(self, new_num_tokens):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def create_mask(self, qlen, mlen, dtype=tf.float32):\n",
    "        \"\"\"\n",
    "        Creates causal attention mask. Float mask where 1.0 indicates masked, 0.0 indicates not-masked.\n",
    "\n",
    "        Args:\n",
    "            qlen: TODO Lysandre didn't fill\n",
    "            mlen: TODO Lysandre didn't fill\n",
    "\n",
    "        ::\n",
    "\n",
    "                  same_length=False:      same_length=True:\n",
    "                   <  qlen >        <  qlen >\n",
    "               ^ [0 0 0 0 0 1 1 1 1]     [0 0 0 0 0 1 1 1 1]\n",
    "                 [0 0 0 0 0 0 1 1 1]     [1 0 0 0 0 0 1 1 1]\n",
    "            qlen [0 0 0 0 0 0 0 1 1]     [1 1 0 0 0 0 0 1 1]\n",
    "                 [0 0 0 0 0 0 0 0 1]     [1 1 1 0 0 0 0 0 1]\n",
    "               v [0 0 0 0 0 0 0 0 0]     [1 1 1 1 0 0 0 0 0]\n",
    "\n",
    "        \"\"\"\n",
    "        attn_mask = tf.ones([qlen, qlen], dtype=dtype)\n",
    "        mask_u = tf.matrix_band_part(attn_mask, 0, -1)\n",
    "        mask_dia = tf.matrix_band_part(attn_mask, 0, 0)\n",
    "        attn_mask_pad = tf.zeros([qlen, mlen], dtype=dtype)\n",
    "        ret = tf.concat([attn_mask_pad, mask_u - mask_dia], 1)\n",
    "        if self.same_length:\n",
    "            mask_l = tf.matrix_band_part(attn_mask, -1, 0)\n",
    "            ret = tf.concat([ret[:, :qlen] + mask_l - mask_dia, ret[:, qlen:]], 1)\n",
    "        return ret\n",
    "\n",
    "    def cache_mem(self, curr_out, prev_mem):\n",
    "        # cache hidden states into memory.\n",
    "        if self.reuse_len is not None and self.reuse_len > 0:\n",
    "            curr_out = curr_out[: self.reuse_len]\n",
    "\n",
    "        if self.mem_len is None or self.mem_len == 0:\n",
    "            # If :obj:`use_mems` is active but no `mem_len` is defined, the model behaves like GPT-2 at inference time\n",
    "            # and returns all of the past and current hidden states.\n",
    "            cutoff = 0\n",
    "        else:\n",
    "            # If :obj:`use_mems` is active and `mem_len` is defined, the model returns the last `mem_len` hidden\n",
    "            # states. This is the preferred setting for training and long-form generation.\n",
    "            cutoff = -self.mem_len\n",
    "        if prev_mem is None:\n",
    "            # if :obj:`use_mems` is active and `mem_len` is defined, the model\n",
    "            new_mem = curr_out[cutoff:]\n",
    "        else:\n",
    "            new_mem = tf.concat([prev_mem, curr_out], 0)[cutoff:]\n",
    "\n",
    "        return tf.stop_gradient(new_mem)\n",
    "\n",
    "    @staticmethod\n",
    "    def positional_embedding(pos_seq, inv_freq, bsz=None):\n",
    "        sinusoid_inp = tf.einsum(\"i,d->id\", pos_seq, inv_freq)\n",
    "        pos_emb = tf.concat([tf.sin(sinusoid_inp), tf.cos(sinusoid_inp)], axis=-1)\n",
    "        pos_emb = pos_emb[:, None, :]\n",
    "\n",
    "        if bsz is not None:\n",
    "            pos_emb = tf.tile(pos_emb, [1, bsz, 1])\n",
    "\n",
    "        return pos_emb\n",
    "\n",
    "    def relative_positional_encoding(self, qlen, klen, bsz=None, dtype=None):\n",
    "        \"\"\"create relative positional encoding.\"\"\"\n",
    "        freq_seq = tf.range(0, self.d_model, 2.0)\n",
    "        if dtype is not None and dtype != tf.float32:\n",
    "            freq_seq = tf.cast(freq_seq, dtype=dtype)\n",
    "        inv_freq = 1 / (10000 ** (freq_seq / self.d_model))\n",
    "\n",
    "        if self.attn_type == \"bi\":\n",
    "            # beg, end = klen - 1, -qlen\n",
    "            beg, end = klen, -qlen\n",
    "        elif self.attn_type == \"uni\":\n",
    "            # beg, end = klen - 1, -1\n",
    "            beg, end = klen, -1\n",
    "        else:\n",
    "            raise ValueError(\"Unknown `attn_type` {}.\".format(self.attn_type))\n",
    "\n",
    "        if self.bi_data:\n",
    "            fwd_pos_seq = tf.range(beg, end, -1.0)\n",
    "            bwd_pos_seq = tf.range(-beg, -end, 1.0)\n",
    "\n",
    "            if dtype is not None and dtype != tf.float32:\n",
    "                fwd_pos_seq = tf.cast(fwd_pos_seq, dtype=dtype)\n",
    "                bwd_pos_seq = tf.cast(bwd_pos_seq, dtype=dtype)\n",
    "\n",
    "            if self.clamp_len > 0:\n",
    "                fwd_pos_seq = tf.clip_by_value(fwd_pos_seq, -self.clamp_len, self.clamp_len)\n",
    "                bwd_pos_seq = tf.clip_by_value(bwd_pos_seq, -self.clamp_len, self.clamp_len)\n",
    "\n",
    "            if bsz is not None:\n",
    "                assert bsz % 2 == 0, f\"With bi_data, the batch size {bsz} should be divisible by 2\"\n",
    "                fwd_pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq, bsz // 2)\n",
    "                bwd_pos_emb = self.positional_embedding(bwd_pos_seq, inv_freq, bsz // 2)\n",
    "            else:\n",
    "                fwd_pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq)\n",
    "                bwd_pos_emb = self.positional_embedding(bwd_pos_seq, inv_freq)\n",
    "\n",
    "            pos_emb = tf.concat([fwd_pos_emb, bwd_pos_emb], axis=1)\n",
    "        else:\n",
    "            fwd_pos_seq = tf.range(beg, end, -1.0)\n",
    "            if dtype is not None and dtype != tf.float32:\n",
    "                fwd_pos_seq = tf.cast(fwd_pos_seq, dtype=dtype)\n",
    "            if self.clamp_len > 0:\n",
    "                fwd_pos_seq = tf.clip_by_value(fwd_pos_seq, -self.clamp_len, self.clamp_len)\n",
    "            pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq, bsz)\n",
    "\n",
    "        return pos_emb\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        mems=None,\n",
    "        perm_mask=None,\n",
    "        target_mapping=None,\n",
    "        token_type_ids=None,\n",
    "        input_mask=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        use_mems=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        training=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        inputs = input_processing(\n",
    "            func=self.call,\n",
    "            config=self.config,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            mems=mems,\n",
    "            perm_mask=perm_mask,\n",
    "            target_mapping=target_mapping,\n",
    "            token_type_ids=token_type_ids,\n",
    "            input_mask=input_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_mems=use_mems,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            training=training,\n",
    "            kwargs_call=kwargs,\n",
    "        )\n",
    "\n",
    "        if training and inputs[\"use_mems\"] is None:\n",
    "            inputs[\"use_mems\"] = self.use_mems_train\n",
    "        else:\n",
    "            inputs[\"use_mems\"] = self.use_mems_eval\n",
    "\n",
    "        # the original code for XLNet uses shapes [len, bsz] with the batch dimension at the end\n",
    "        # but we want a unified interface in the library with the batch size on the first dimension\n",
    "        # so we move here the first dimension (batch) to the end\n",
    "\n",
    "        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif inputs[\"input_ids\"] is not None:\n",
    "            inputs[\"input_ids\"] = tf.transpose(inputs[\"input_ids\"], perm=(1, 0))\n",
    "            qlen, bsz = shape_list(inputs[\"input_ids\"])[:2]\n",
    "        elif inputs[\"inputs_embeds\"] is not None:\n",
    "            inputs[\"inputs_embeds\"] = tf.transpose(inputs[\"inputs_embeds\"], perm=(1, 0, 2))\n",
    "            qlen, bsz = shape_list(inputs[\"inputs_embeds\"])[:2]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        inputs[\"token_type_ids\"] = (\n",
    "            tf.transpose(inputs[\"token_type_ids\"], perm=(1, 0)) if inputs[\"token_type_ids\"] is not None else None\n",
    "        )\n",
    "        inputs[\"input_mask\"] = (\n",
    "            tf.transpose(inputs[\"input_mask\"], perm=(1, 0)) if inputs[\"input_mask\"] is not None else None\n",
    "        )\n",
    "        inputs[\"attention_mask\"] = (\n",
    "            tf.transpose(inputs[\"attention_mask\"], perm=(1, 0)) if inputs[\"attention_mask\"] is not None else None\n",
    "        )\n",
    "        inputs[\"perm_mask\"] = (\n",
    "            tf.transpose(inputs[\"perm_mask\"], perm=(1, 2, 0)) if inputs[\"perm_mask\"] is not None else None\n",
    "        )\n",
    "        inputs[\"target_mapping\"] = (\n",
    "            tf.transpose(inputs[\"target_mapping\"], perm=(1, 2, 0)) if inputs[\"target_mapping\"] is not None else None\n",
    "        )\n",
    "\n",
    "        mlen = shape_list(inputs[\"mems\"][0])[0] if inputs[\"mems\"] is not None and inputs[\"mems\"][0] is not None else 0\n",
    "        klen = mlen + qlen\n",
    "\n",
    "        dtype_float = tf.bfloat16 if self.use_bfloat16 else tf.float32\n",
    "\n",
    "        # Attention mask\n",
    "        # causal attention mask\n",
    "        if self.attn_type == \"uni\":\n",
    "            attn_mask = self.create_mask(qlen, mlen)\n",
    "            attn_mask = attn_mask[:, :, None, None]\n",
    "        elif self.attn_type == \"bi\":\n",
    "            attn_mask = None\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported attention type: {}\".format(self.attn_type))\n",
    "\n",
    "        # data mask: input mask & perm mask\n",
    "        assert inputs[\"input_mask\"] is None or inputs[\"attention_mask\"] is None, (\n",
    "            \"You can only use one of input_mask (uses 1 for padding) \"\n",
    "            \"or attention_mask (uses 0 for padding, added for compatibility with BERT). Please choose one.\"\n",
    "        )\n",
    "        if inputs[\"input_mask\"] is None and inputs[\"attention_mask\"] is not None:\n",
    "            inputs[\"input_mask\"] = 1.0 - tf.cast(inputs[\"attention_mask\"], dtype=dtype_float)\n",
    "        if inputs[\"input_mask\"] is not None and inputs[\"perm_mask\"] is not None:\n",
    "            data_mask = inputs[\"input_mask\"][None] + inputs[\"perm_mask\"]\n",
    "        elif inputs[\"input_mask\"] is not None and inputs[\"perm_mask\"] is None:\n",
    "            data_mask = inputs[\"input_mask\"][None]\n",
    "        elif inputs[\"input_mask\"] is None and inputs[\"perm_mask\"] is not None:\n",
    "            data_mask = inputs[\"perm_mask\"]\n",
    "        else:\n",
    "            data_mask = None\n",
    "\n",
    "        if data_mask is not None:\n",
    "            # all mems can be attended to\n",
    "            if mlen > 0:\n",
    "                mems_mask = tf.zeros([shape_list(data_mask)[0], mlen, bsz], dtype=dtype_float)\n",
    "                data_mask = tf.concat([mems_mask, data_mask], axis=1)\n",
    "            if attn_mask is None:\n",
    "                attn_mask = data_mask[:, :, :, None]\n",
    "            else:\n",
    "                attn_mask += data_mask[:, :, :, None]\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = tf.cast(attn_mask > 0, dtype=dtype_float)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            non_tgt_mask = -tf.eye(qlen, dtype=dtype_float)\n",
    "            if mlen > 0:\n",
    "                non_tgt_mask = tf.concat([tf.zeros([qlen, mlen], dtype=dtype_float), non_tgt_mask], axis=-1)\n",
    "            non_tgt_mask = tf.cast((attn_mask + non_tgt_mask[:, :, None, None]) > 0, dtype=dtype_float)\n",
    "        else:\n",
    "            non_tgt_mask = None\n",
    "\n",
    "        # Word embeddings and prepare h & g hidden states\n",
    "        if inputs[\"inputs_embeds\"] is not None:\n",
    "            word_emb_k = inputs[\"inputs_embeds\"]\n",
    "        else:\n",
    "            word_emb_k = self.word_embedding(inputs[\"input_ids\"])\n",
    "        output_h = self.dropout(word_emb_k, training=inputs[\"training\"])\n",
    "        if inputs[\"target_mapping\"] is not None:\n",
    "            word_emb_q = tf.tile(self.mask_emb, [shape_list(inputs[\"target_mapping\"])[0], bsz, 1])\n",
    "            # else:  # We removed the inp_q input which was same as target mapping\n",
    "            #     inp_q_ext = inp_q[:, :, None]\n",
    "            #     word_emb_q = inp_q_ext * self.mask_emb + (1 - inp_q_ext) * word_emb_k\n",
    "            output_g = self.dropout(word_emb_q, training=inputs[\"training\"])\n",
    "        else:\n",
    "            output_g = None\n",
    "\n",
    "        # Segment embedding\n",
    "        if inputs[\"token_type_ids\"] is not None:\n",
    "            # Convert `token_type_ids` to one-hot `seg_mat`\n",
    "            if mlen > 0:\n",
    "                mem_pad = tf.zeros([mlen, bsz], dtype=tf.int32)\n",
    "                cat_ids = tf.concat([mem_pad, inputs[\"token_type_ids\"]], 0)\n",
    "            else:\n",
    "                cat_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "            # `1` indicates not in the same segment [qlen x klen x bsz]\n",
    "            seg_mat = tf.cast(tf.logical_not(tf.equal(inputs[\"token_type_ids\"][:, None], cat_ids[None, :])), tf.int32)\n",
    "            seg_mat = tf.one_hot(seg_mat, 2, dtype=dtype_float)\n",
    "        else:\n",
    "            seg_mat = None\n",
    "\n",
    "        # Positional encoding\n",
    "        pos_emb = self.relative_positional_encoding(qlen, klen, bsz=bsz, dtype=dtype_float)\n",
    "        pos_emb = self.dropout(pos_emb, training=inputs[\"training\"])\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer)\n",
    "        # and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head]\n",
    "        if inputs[\"head_mask\"] is not None:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            inputs[\"head_mask\"] = [None] * self.n_layer\n",
    "\n",
    "        new_mems = ()\n",
    "        if inputs[\"mems\"] is None:\n",
    "            inputs[\"mems\"] = [None] * len(self.layer)\n",
    "\n",
    "        attentions = [] if inputs[\"output_attentions\"] else None\n",
    "        hidden_states = [] if inputs[\"output_hidden_states\"] else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            # cache new mems\n",
    "            if inputs[\"use_mems\"]:\n",
    "                new_mems = new_mems + (self.cache_mem(output_h, inputs[\"mems\"][i]),)\n",
    "            if inputs[\"output_hidden_states\"]:\n",
    "                hidden_states.append((output_h, output_g) if output_g is not None else output_h)\n",
    "\n",
    "            outputs = layer_module(\n",
    "                output_h,\n",
    "                output_g,\n",
    "                non_tgt_mask,\n",
    "                attn_mask,\n",
    "                pos_emb,\n",
    "                seg_mat,\n",
    "                inputs[\"mems\"][i],\n",
    "                inputs[\"target_mapping\"],\n",
    "                inputs[\"head_mask\"][i],\n",
    "                inputs[\"output_attentions\"],\n",
    "                training=inputs[\"training\"],\n",
    "            )\n",
    "            output_h, output_g = outputs[:2]\n",
    "            if inputs[\"output_attentions\"]:\n",
    "                attentions.append(outputs[2])\n",
    "\n",
    "        # Add last hidden state\n",
    "        if inputs[\"output_hidden_states\"]:\n",
    "            hidden_states.append((output_h, output_g) if output_g is not None else output_h)\n",
    "\n",
    "        output = self.dropout(output_g if output_g is not None else output_h, training=inputs[\"training\"])\n",
    "\n",
    "        # Prepare outputs, we transpose back here to shape [bsz, len, hidden_dim] (cf. beginning of forward() method)\n",
    "        output = tf.transpose(output, perm=(1, 0, 2))\n",
    "\n",
    "        if not inputs[\"use_mems\"]:\n",
    "            new_mems = None\n",
    "        if inputs[\"output_hidden_states\"]:\n",
    "            if output_g is not None:\n",
    "                hidden_states = tuple(tf.transpose(h, perm=(1, 0, 2)) for hs in hidden_states for h in hs)\n",
    "            else:\n",
    "                hidden_states = tuple(tf.transpose(hs, perm=(1, 0, 2)) for hs in hidden_states)\n",
    "        if inputs[\"output_attentions\"]:\n",
    "            attentions = tuple(tf.transpose(t, perm=(2, 3, 0, 1)) for t in attentions)\n",
    "\n",
    "        if not inputs[\"return_dict\"]:\n",
    "            return tuple(v for v in [output, new_mems, hidden_states, attentions] if v is not None)\n",
    "\n",
    "        return TFXLNetModelOutput(\n",
    "            last_hidden_state=output, mems=new_mems, hidden_states=hidden_states, attentions=attentions\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class TFBartLearnedPositionalEmbedding(TFSharedEmbeddings):\n",
    "    \"\"\"\n",
    "    This module learns positional embeddings up to a fixed maximum size. Padding ids are ignored by either offsetting\n",
    "    based on padding_idx or by setting padding_idx to None and ensuring that the appropriate position ids are passed to\n",
    "    the forward function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, offset, **kwargs):\n",
    "        # Bart is set up so that if padding_idx is specified then offset the embedding ids by 2\n",
    "        # and adjust num_embeddings appropriately. Other models dont have this hack\n",
    "        self.offset = offset\n",
    "        assert padding_idx is not None, \"padding_idx cannot be None\"\n",
    "        num_embeddings += offset\n",
    "        super().__init__(num_embeddings, embedding_dim, **kwargs)\n",
    "\n",
    "    def call(self, input_shape: tf.TensorShape, past_key_values_length: int = 0):\n",
    "        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n",
    "        bsz, seq_len = input_shape[:2]\n",
    "\n",
    "        positions = tf.range(\n",
    "            past_key_values_length, seq_len + past_key_values_length, delta=1, dtype=tf.int32, name=\"range\"\n",
    "        )\n",
    "        return super().call(positions + self.offset)  # super object is not callable for some reason\n",
    "\n",
    "\n",
    "class TFBartSinusoidalPositionalEmbedding(tf.keras.layers.Embedding):\n",
    "    \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n",
    "\n",
    "    def __init__(self, num_positions: int, embedding_dim: int, **kwargs):\n",
    "\n",
    "        if embedding_dim % 2 != 0:\n",
    "            raise NotImplementedError(f\"odd embedding_dim {embedding_dim} not supported\")\n",
    "        super().__init__(\n",
    "            num_positions,\n",
    "            embedding_dim,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        \"\"\"\n",
    "        Build shared token embedding layer Shared weights logic adapted from\n",
    "        https://github.com/tensorflow/models/blob/a009f4fb9d2fc4949e32192a944688925ef78659/official/transformer/v2/embedding_layer.py#L24\n",
    "        \"\"\"\n",
    "        super().build(input_shape)  # Instantiates self.weight so it can be loaded\n",
    "        weight: np.ndarray = self._init_weight(self.input_dim, self.output_dim)\n",
    "        self.set_weights([weight])  # overwrite self.weight to correct value\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_weight(n_pos: int, dim: int):\n",
    "        \"\"\"\n",
    "        Identical to the XLM create_sinusoidal_embeddings except features are not interleaved. The cos features are in\n",
    "        the 2nd half of the vector. [dim // 2:]\n",
    "        \"\"\"\n",
    "        position_enc = np.array(\n",
    "            [[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)]\n",
    "        )\n",
    "        # index 0 is all zero\n",
    "        position_enc[:, 0 : dim // 2] = np.sin(position_enc[:, 0::2])\n",
    "        position_enc[:, dim // 2 :] = np.cos(position_enc[:, 1::2])\n",
    "        # convert to tensor\n",
    "        table = tf.convert_to_tensor(position_enc, dtype=tf.float32)\n",
    "        tf.stop_gradient(table)\n",
    "        return table\n",
    "\n",
    "    def call(self, input_shape: tf.TensorShape, past_key_values_length: int = 0):\n",
    "        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n",
    "        bsz, seq_len = input_shape[:2]\n",
    "\n",
    "        positions = tf.range(\n",
    "            past_key_values_length, seq_len + past_key_values_length, delta=1, dtype=tf.int32, name=\"range\"\n",
    "        )\n",
    "        return super().call(positions)\n",
    "\n",
    "\n",
    "class TFBartAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"Multi-headed attention from \"Attention Is All You Need\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.0,\n",
    "        is_decoder: bool = False,\n",
    "        bias: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        self.scaling = self.head_dim ** -0.5\n",
    "        self.is_decoder = is_decoder\n",
    "\n",
    "        self.k_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"k_proj\")\n",
    "        self.q_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"q_proj\")\n",
    "        self.v_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"v_proj\")\n",
    "        self.out_proj = tf.keras.layers.Dense(embed_dim, use_bias=bias, name=\"out_proj\")\n",
    "\n",
    "    def _shape(self, tensor: tf.Tensor, seq_len: int, bsz: int):\n",
    "        return tf.transpose(tf.reshape(tensor, (bsz, seq_len, self.num_heads, self.head_dim)), (0, 2, 1, 3))\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        hidden_states: tf.Tensor,\n",
    "        key_value_states: Optional[tf.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[Tuple[tf.Tensor]]] = None,\n",
    "        attention_mask: Optional[tf.Tensor] = None,\n",
    "        training=False,\n",
    "    ) -> Tuple[tf.Tensor, Optional[tf.Tensor]]:\n",
    "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
    "\n",
    "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
    "        # for the decoder\n",
    "        is_cross_attention = key_value_states is not None\n",
    "        bsz, tgt_len, embed_dim = shape_list(hidden_states)\n",
    "\n",
    "        # get query proj\n",
    "        query_states = self.q_proj(hidden_states) * self.scaling\n",
    "        # get key, value proj\n",
    "        if is_cross_attention and past_key_value is not None:\n",
    "            # reuse k,v, cross_attentions\n",
    "            key_states = past_key_value[0]\n",
    "            value_states = past_key_value[1]\n",
    "        elif is_cross_attention:\n",
    "            # cross_attentions\n",
    "            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n",
    "        elif past_key_value is not None:\n",
    "            # reuse k, v, self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "            key_states = tf.concat([past_key_value[0], key_states], axis=2)\n",
    "            value_states = tf.concat([past_key_value[1], value_states], axis=2)\n",
    "        else:\n",
    "            # self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            # if cross_attention save Tuple(tf.Tensor, tf.Tensor) of all cross attention key/value_states.\n",
    "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
    "            # key/value_states (first \"if\" case)\n",
    "            # if uni-directional self-attention (decoder) save Tuple(tf.Tensor, tf.Tensor) of\n",
    "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
    "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
    "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
    "            past_key_value = (key_states, value_states)\n",
    "\n",
    "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
    "        query_states = tf.reshape(self._shape(query_states, tgt_len, bsz), proj_shape)\n",
    "        key_states = tf.reshape(key_states, proj_shape)\n",
    "        value_states = tf.reshape(value_states, proj_shape)\n",
    "\n",
    "        src_len = shape_list(key_states)[1]\n",
    "        attn_weights = tf.matmul(query_states, key_states, transpose_b=True)\n",
    "\n",
    "        tf.debugging.assert_equal(\n",
    "            shape_list(attn_weights),\n",
    "            [bsz * self.num_heads, tgt_len, src_len],\n",
    "            message=f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {shape_list(attn_weights)}\",\n",
    "        )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            tf.debugging.assert_equal(\n",
    "                shape_list(attention_mask),\n",
    "                [bsz, 1, tgt_len, src_len],\n",
    "                message=f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {shape_list(attention_mask)}\",\n",
    "            )\n",
    "            attn_weights = tf.reshape(attn_weights, (bsz, self.num_heads, tgt_len, src_len)) + attention_mask\n",
    "            attn_weights = tf.reshape(attn_weights, (bsz * self.num_heads, tgt_len, src_len))\n",
    "\n",
    "        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n",
    "\n",
    "        attn_probs = self.dropout(attn_weights, training=training)\n",
    "\n",
    "        attn_output = tf.matmul(attn_probs, value_states)\n",
    "\n",
    "        tf.debugging.assert_equal(\n",
    "            shape_list(attn_output),\n",
    "            [bsz * self.num_heads, tgt_len, self.head_dim],\n",
    "            message=f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is {shape_list(attn_output)}\",\n",
    "        )\n",
    "\n",
    "        attn_output = tf.transpose(\n",
    "            tf.reshape(attn_output, (bsz, self.num_heads, tgt_len, self.head_dim)), (0, 2, 1, 3)\n",
    "        )\n",
    "        attn_output = tf.reshape(attn_output, (bsz, tgt_len, embed_dim))\n",
    "\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        attn_weights: tf.Tensor = tf.reshape(attn_weights, (bsz, self.num_heads, tgt_len, src_len))\n",
    "\n",
    "        return attn_output, attn_weights, past_key_value\n",
    "\n",
    "\n",
    "class TFBartEncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, config: BartConfig, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = config.d_model\n",
    "        self.self_attn = TFBartAttention(\n",
    "            self.embed_dim, config.encoder_attention_heads, dropout=config.attention_dropout, name=\"self_attn\"\n",
    "        )\n",
    "        self.normalize_before = config.normalize_before\n",
    "        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "        self.activation_fn = ACT2FN[config.activation_function]\n",
    "        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n",
    "        self.fc1 = tf.keras.layers.Dense(config.encoder_ffn_dim, name=\"fc1\")\n",
    "        self.fc2 = tf.keras.layers.Dense(self.embed_dim, name=\"fc2\")\n",
    "        self.final_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"final_layer_norm\")\n",
    "\n",
    "    def call(self, hidden_states: tf.Tensor, attention_mask: tf.Tensor, training=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (:obj:`tf.Tensor`): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
    "            attention_mask (:obj:`tf.Tensor`): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "        \"\"\"\n",
    "        residual = hidden_states\n",
    "        if self.normalize_before:\n",
    "            hidden_states = self.self_attn_layer_norm(hidden_states)\n",
    "        hidden_states, self_attn_weights, _ = self.self_attn(\n",
    "            hidden_states=hidden_states, attention_mask=attention_mask\n",
    "        )\n",
    "        tf.debugging.assert_equal(\n",
    "            shape_list(hidden_states),\n",
    "            shape_list(residual),\n",
    "            message=f\"Self attn modified the shape of query {shape_list(residual)} to {shape_list(hidden_states)}\",\n",
    "        )\n",
    "        hidden_states = self.dropout(hidden_states, training=training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        if not self.normalize_before:\n",
    "            hidden_states = self.self_attn_layer_norm(hidden_states)\n",
    "\n",
    "        residual = hidden_states\n",
    "        if self.normalize_before:\n",
    "            hidden_states = self.final_layer_norm(hidden_states)\n",
    "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
    "        hidden_states = self.activation_dropout(hidden_states, training=training)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states, training=training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        if not self.normalize_before:\n",
    "            hidden_states = self.final_layer_norm(hidden_states)\n",
    "\n",
    "        return hidden_states, self_attn_weights\n",
    "\n",
    "\n",
    "class TFBartDecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, config: BartConfig, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = config.d_model\n",
    "        self.self_attn = TFBartAttention(\n",
    "            embed_dim=self.embed_dim,\n",
    "            num_heads=config.decoder_attention_heads,\n",
    "            dropout=config.attention_dropout,\n",
    "            name=\"self_attn\",\n",
    "            is_decoder=True,\n",
    "        )\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "        self.activation_fn = ACT2FN[config.activation_function]\n",
    "        self.activation_dropout = tf.keras.layers.Dropout(config.activation_dropout)\n",
    "        self.normalize_before = config.normalize_before\n",
    "\n",
    "        self.self_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"self_attn_layer_norm\")\n",
    "        self.encoder_attn = TFBartAttention(\n",
    "            self.embed_dim,\n",
    "            config.decoder_attention_heads,\n",
    "            dropout=config.attention_dropout,\n",
    "            name=\"encoder_attn\",\n",
    "            is_decoder=True,\n",
    "        )\n",
    "        self.encoder_attn_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"encoder_attn_layer_norm\")\n",
    "        self.fc1 = tf.keras.layers.Dense(config.decoder_ffn_dim, name=\"fc1\")\n",
    "        self.fc2 = tf.keras.layers.Dense(self.embed_dim, name=\"fc2\")\n",
    "        self.final_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"final_layer_norm\")\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask: Optional[tf.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[tf.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[tf.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[tf.Tensor]] = None,\n",
    "        training=False,\n",
    "    ) -> Tuple[tf.Tensor, tf.Tensor, Tuple[Tuple[tf.Tensor]]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (:obj:`tf.Tensor`): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
    "            attention_mask (:obj:`tf.Tensor`): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            encoder_hidden_states (:obj:`tf.Tensor`): cross attention input to the layer of shape `(seq_len, batch, embed_dim)`\n",
    "            encoder_attention_mask (:obj:`tf.Tensor`): encoder attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            past_key_value (:obj:`Tuple(tf.Tensor)`): cached past key and value projection states\n",
    "        \"\"\"\n",
    "        residual = hidden_states\n",
    "        if self.normalize_before:\n",
    "            hidden_states = self.self_attn_layer_norm(hidden_states)\n",
    "\n",
    "        # Self Attention\n",
    "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
    "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
    "        # add present self-attn cache to positions 1,2 of present_key_value tuple\n",
    "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            past_key_value=self_attn_past_key_value,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        hidden_states = self.dropout(hidden_states, training=training)\n",
    "        hidden_states = residual + hidden_states\n",
    "        if not self.normalize_before:\n",
    "            hidden_states = self.self_attn_layer_norm(hidden_states)\n",
    "\n",
    "        # Cross-Attention Block\n",
    "        cross_attn_present_key_value = None\n",
    "        if encoder_hidden_states is not None:\n",
    "            residual = hidden_states\n",
    "            if self.normalize_before:\n",
    "                hidden_states = self.encoder_attn_layer_norm(hidden_states)\n",
    "\n",
    "            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n",
    "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
    "            hidden_states, _, cross_attn_present_key_value = self.encoder_attn(\n",
    "                hidden_states=hidden_states,\n",
    "                key_value_states=encoder_hidden_states,\n",
    "                attention_mask=encoder_attention_mask,\n",
    "                past_key_value=cross_attn_past_key_value,\n",
    "            )\n",
    "            hidden_states = self.dropout(hidden_states, training=training)\n",
    "            hidden_states = residual + hidden_states\n",
    "            if not self.normalize_before:\n",
    "                hidden_states = self.encoder_attn_layer_norm(hidden_states)\n",
    "\n",
    "            # add cross-attn to positions 3,4 of present_key_value tuple\n",
    "            present_key_value = present_key_value + cross_attn_present_key_value\n",
    "\n",
    "        # Fully Connected\n",
    "        residual = hidden_states\n",
    "        if self.normalize_before:\n",
    "            hidden_states = self.final_layer_norm(hidden_states)\n",
    "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
    "        hidden_states = self.activation_dropout(hidden_states, training=training)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states, training=training)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        if not self.normalize_before:\n",
    "            hidden_states = self.final_layer_norm(hidden_states)\n",
    "\n",
    "        return (\n",
    "            hidden_states,\n",
    "            self_attn_weights,\n",
    "            present_key_value,\n",
    "        )\n",
    "@keras_serializable\n",
    "class TFBartEncoder(tf.keras.layers.Layer):\n",
    "    config_class = BartConfig\n",
    "    \"\"\"\n",
    "    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n",
    "    :class:`TFBartEncoderLayer`.\n",
    "\n",
    "    Args:\n",
    "        config: BartConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: BartConfig, embed_tokens: Optional[TFSharedEmbeddings] = None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.config = config\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "        self.layerdrop = config.encoder_layerdrop\n",
    "        self.embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.max_source_positions = config.max_position_embeddings\n",
    "\n",
    "        self.embed_tokens = embed_tokens\n",
    "        if config.static_position_embeddings:\n",
    "            self.embed_positions = TFBartSinusoidalPositionalEmbedding(\n",
    "                config.max_position_embeddings,\n",
    "                config.d_model,\n",
    "                name=\"embed_positions\",\n",
    "            )\n",
    "        else:\n",
    "            self.embed_positions = TFBartLearnedPositionalEmbedding(\n",
    "                config.max_position_embeddings,\n",
    "                config.d_model,\n",
    "                self.padding_idx,\n",
    "                config.extra_pos_embeddings,\n",
    "                name=\"embed_positions\",\n",
    "            )\n",
    "        self.layers = [TFBartEncoderLayer(config, name=f\"layers.{i}\") for i in range(config.encoder_layers)]\n",
    "        self.layernorm_embedding = (\n",
    "            tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"layernorm_embedding\")\n",
    "            if config.normalize_embedding\n",
    "            else tf.keras.layers.Layer()\n",
    "        )\n",
    "        self.layer_norm = (\n",
    "            tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"layer_norm\")\n",
    "            if config.add_final_layer_norm\n",
    "            else None\n",
    "        )\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        inputs_embeds=None,\n",
    "        attention_mask=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        training=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids (:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length)`):\n",
    "                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
    "                provide it.\n",
    "\n",
    "                Indices can be obtained using :class:`~transformers.BartTokenizer`. See\n",
    "                :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__`\n",
    "                for details.\n",
    "\n",
    "                `What are input IDs? <../glossary.html#input-ids>`__\n",
    "            attention_mask (:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n",
    "\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "\n",
    "                `What are attention masks? <../glossary.html#attention-mask>`__\n",
    "            inputs_embeds (:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
    "                Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded\n",
    "                representation. This is useful if you want more control over how to convert :obj:`input_ids` indices\n",
    "                into associated vectors than the model's internal embedding lookup matrix.\n",
    "            output_attentions (:obj:`bool`, `optional`):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
    "                returned tensors for more detail.\n",
    "            output_hidden_states (:obj:`bool`, `optional`):\n",
    "                Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors\n",
    "                for more detail.\n",
    "            return_dict (:obj:`bool`, `optional`):\n",
    "                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
    "        \"\"\"\n",
    "        inputs = input_processing(\n",
    "            func=self.call,\n",
    "            config=self.config,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            training=training,\n",
    "            kwargs_call=kwargs,\n",
    "        )\n",
    "\n",
    "        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif inputs[\"input_ids\"] is not None:\n",
    "            input_shape = shape_list(inputs[\"input_ids\"])\n",
    "        elif inputs[\"inputs_embeds\"] is not None:\n",
    "            input_shape = shape_list(inputs[\"inputs_embeds\"])[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        if inputs[\"inputs_embeds\"] is None:\n",
    "            inputs_embeds = self.embed_tokens(inputs[\"input_ids\"])\n",
    "        else:\n",
    "            inputs_embeds = inputs[\"inputs_embeds\"]\n",
    "\n",
    "        inputs_embeds = inputs_embeds * self.embed_scale\n",
    "\n",
    "        embed_pos = self.embed_positions(input_shape)\n",
    "        hidden_states = inputs_embeds + embed_pos\n",
    "        hidden_states = self.layernorm_embedding(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states, training=inputs[\"training\"])\n",
    "\n",
    "        # check attention mask and invert\n",
    "        if inputs[\"attention_mask\"] is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            attention_mask = _expand_mask(inputs[\"attention_mask\"])\n",
    "        else:\n",
    "            attention_mask = None\n",
    "\n",
    "        encoder_states = () if inputs[\"output_hidden_states\"] else None\n",
    "        all_attentions = () if inputs[\"output_attentions\"] else None\n",
    "\n",
    "        # encoder layers\n",
    "        for encoder_layer in self.layers:\n",
    "\n",
    "            if inputs[\"output_hidden_states\"]:\n",
    "                encoder_states = encoder_states + (hidden_states,)\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            dropout_probability = random.uniform(0, 1)\n",
    "            if inputs[\"training\"] and (dropout_probability < self.layerdrop):  # skip the layer\n",
    "                continue\n",
    "\n",
    "            hidden_states, attn = encoder_layer(hidden_states, attention_mask)\n",
    "\n",
    "            if inputs[\"output_attentions\"]:\n",
    "                all_attentions += (attn,)\n",
    "        if self.layer_norm:\n",
    "            hidden_states = self.layer_norm(hidden_states)\n",
    "        if inputs[\"output_hidden_states\"]:\n",
    "            encoder_states = encoder_states + (hidden_states,)\n",
    "\n",
    "        if not inputs[\"return_dict\"]:\n",
    "            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n",
    "        return TFBaseModelOutput(\n",
    "            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n",
    "        )\n",
    "\n",
    "\n",
    "@keras_serializable\n",
    "class TFBartDecoder(tf.keras.layers.Layer):\n",
    "    config_class = BartConfig\n",
    "    \"\"\"\n",
    "    Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a :class:`TFBartDecoderLayer`\n",
    "\n",
    "    Args:\n",
    "        config: BartConfig\n",
    "        embed_tokens: output embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: BartConfig, embed_tokens: Optional[TFSharedEmbeddings] = None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.config = config\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.embed_tokens = embed_tokens\n",
    "        self.embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n",
    "        self.layerdrop = config.decoder_layerdrop\n",
    "        if config.static_position_embeddings:\n",
    "            self.embed_positions = TFBartSinusoidalPositionalEmbedding(\n",
    "                config.max_position_embeddings,\n",
    "                config.d_model,\n",
    "                name=\"embed_positions\",\n",
    "            )\n",
    "        else:\n",
    "            self.embed_positions = TFBartLearnedPositionalEmbedding(\n",
    "                config.max_position_embeddings,\n",
    "                config.d_model,\n",
    "                self.padding_idx,\n",
    "                config.extra_pos_embeddings,\n",
    "                name=\"embed_positions\",\n",
    "            )\n",
    "        self.layers = [TFBartDecoderLayer(config, name=f\"layers.{i}\") for i in range(config.decoder_layers)]\n",
    "        self.layernorm_embedding = (\n",
    "            tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"layernorm_embedding\")\n",
    "            if config.normalize_embedding\n",
    "            else tf.keras.layers.Layer()\n",
    "        )\n",
    "        self.layer_norm = (\n",
    "            tf.keras.layers.LayerNormalization(epsilon=1e-5, name=\"layer_norm\")\n",
    "            if config.add_final_layer_norm\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "        self.do_blenderbot_90_layernorm = config.do_blenderbot_90_layernorm\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        inputs_embeds=None,\n",
    "        attention_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        training=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            input_ids (:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length)`):\n",
    "                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
    "                provide it.\n",
    "\n",
    "                Indices can be obtained using :class:`~transformers.BartTokenizer`. See\n",
    "                :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__`\n",
    "                for details.\n",
    "\n",
    "                `What are input IDs? <../glossary.html#input-ids>`__\n",
    "            attention_mask (:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n",
    "\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "\n",
    "                `What are attention masks? <../glossary.html#attention-mask>`__\n",
    "            encoder_hidden_states (:obj:`tf.Tensor` of shape :obj:`(batch_size, encoder_sequence_length, hidden_size)`, `optional`):\n",
    "                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n",
    "                of the decoder.\n",
    "            encoder_attention_mask (:obj:`tf.Tensor` of shape :obj:`(batch_size, encoder_sequence_length)`, `optional`):\n",
    "                Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values\n",
    "                selected in ``[0, 1]``:\n",
    "\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "\n",
    "                `What are attention masks? <../glossary.html#attention-mask>`__\n",
    "            past_key_values (:obj:`Tuple[Tuple[tf.Tensor]]` of length :obj:`config.n_layers` with each tuple having 2 tuples each of which has 2 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
    "                Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up\n",
    "                decoding.\n",
    "\n",
    "                If :obj:`past_key_values` are used, the user can optionally input only the last\n",
    "                :obj:`decoder_input_ids` (those that don't have their past key value states given to this model) of\n",
    "                shape :obj:`(batch_size, 1)` instead of all :obj:`decoder_input_ids`` of shape :obj:`(batch_size,\n",
    "                sequence_length)`.\n",
    "            inputs_embeds (:obj:`tf.Tensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
    "                Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded\n",
    "                representation. This is useful if you want more control over how to convert :obj:`input_ids` indices\n",
    "                into associated vectors than the model's internal embedding lookup matrix.\n",
    "            output_attentions (:obj:`bool`, `optional`):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
    "                returned tensors for more detail.\n",
    "            output_hidden_states (:obj:`bool`, `optional`):\n",
    "                Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors\n",
    "                for more detail.\n",
    "            return_dict (:obj:`bool`, `optional`):\n",
    "                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
    "        \"\"\"\n",
    "        inputs = input_processing(\n",
    "            func=self.call,\n",
    "            config=self.config,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            training=training,\n",
    "            kwargs_call=kwargs,\n",
    "        )\n",
    "\n",
    "        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\n",
    "            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n",
    "        elif inputs[\"input_ids\"] is not None:\n",
    "            input_shape = shape_list(inputs[\"input_ids\"])\n",
    "        elif inputs[\"inputs_embeds\"] is not None:\n",
    "            input_shape = shape_list(inputs[\"inputs_embeds\"])[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n",
    "\n",
    "        past_key_values_length = (\n",
    "            inputs[\"past_key_values\"][0][0].shape[2] if inputs[\"past_key_values\"] is not None else 0\n",
    "        )\n",
    "\n",
    "        # embed positions\n",
    "        positions = self.embed_positions(input_shape, past_key_values_length)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(inputs[\"input_ids\"])\n",
    "        else:\n",
    "            inputs_embeds = inputs[\"inputs_embeds\"]\n",
    "\n",
    "        hidden_states = inputs_embeds * self.embed_scale\n",
    "\n",
    "        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "        combined_attention_mask = None\n",
    "        if input_shape[-1] > 1:\n",
    "            combined_attention_mask = _make_causal_mask(input_shape, past_key_values_length=past_key_values_length)\n",
    "\n",
    "        if inputs[\"attention_mask\"] is None and inputs[\"input_ids\"] is not None and input_shape[-1] > 1:\n",
    "            attention_mask = tf.cast(\n",
    "                tf.math.not_equal(inputs[\"input_ids\"], self.config.pad_token_id), inputs[\"input_ids\"].dtype\n",
    "            )\n",
    "        else:\n",
    "            attention_mask = tf.ones(input_shape, dtype=tf.int32)\n",
    "\n",
    "        if attention_mask is not None and combined_attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            combined_attention_mask = combined_attention_mask + _expand_mask(\n",
    "                attention_mask, past_key_values_length=past_key_values_length\n",
    "            )\n",
    "\n",
    "        encoder_hidden_states = inputs[\"encoder_hidden_states\"]\n",
    "        if encoder_hidden_states is not None and inputs[\"encoder_attention_mask\"] is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            encoder_attention_mask = _expand_mask(inputs[\"encoder_attention_mask\"], tgt_len=input_shape[-1])\n",
    "\n",
    "        if self.do_blenderbot_90_layernorm:\n",
    "            hidden_states = self.layernorm_embedding(hidden_states) + positions\n",
    "        else:\n",
    "            hidden_states = self.layernorm_embedding(hidden_states + positions)\n",
    "        hidden_states = self.dropout(hidden_states, training=inputs[\"training\"])\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = ()\n",
    "        all_self_attns = ()\n",
    "        present_key_values = ()\n",
    "        for idx, decoder_layer in enumerate(self.layers):\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            if inputs[\"output_hidden_states\"]:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "            dropout_probability = random.uniform(0, 1)\n",
    "\n",
    "            if inputs[\"training\"] and (dropout_probability < self.layerdrop):\n",
    "                continue\n",
    "\n",
    "            past_key_value = inputs[\"past_key_values\"][idx] if inputs[\"past_key_values\"] is not None else None\n",
    "\n",
    "            hidden_states, layer_self_attn, present_key_value = decoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask=combined_attention_mask,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                encoder_attention_mask=encoder_attention_mask,\n",
    "                past_key_value=past_key_value,\n",
    "            )\n",
    "\n",
    "            if inputs[\"use_cache\"]:\n",
    "                present_key_values += (present_key_value,)\n",
    "\n",
    "            if inputs[\"output_attentions\"]:\n",
    "                all_self_attns += (layer_self_attn,)\n",
    "\n",
    "        if self.layer_norm is not None:  # same as if config.add_final_layer_norm\n",
    "            hidden_states = self.layer_norm(hidden_states)\n",
    "\n",
    "        # Convert to standard output format: (seq_len, BS, model_dim) -> (BS, seq_len, model_dim)\n",
    "        if inputs[\"output_hidden_states\"]:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "        else:\n",
    "            all_hidden_states = None\n",
    "\n",
    "        all_self_attns = list(all_self_attns) if inputs[\"output_attentions\"] else None\n",
    "\n",
    "        present_key_values = (encoder_hidden_states, present_key_values) if inputs[\"use_cache\"] else None\n",
    "\n",
    "        if not inputs[\"return_dict\"]:\n",
    "            return hidden_states, present_key_values, all_hidden_states, all_self_attns\n",
    "        else:\n",
    "            return TFBaseModelOutputWithPast(\n",
    "                last_hidden_state=hidden_states,\n",
    "                past_key_values=present_key_values,\n",
    "                hidden_states=all_hidden_states,\n",
    "                attentions=all_self_attns,\n",
    "            )\n",
    "\n",
    "\n",
    "[DOCS]@add_start_docstrings(\n",
    "    \"The bare BART Model outputting raw hidden-states without any specific head on top.\",\n",
    "    BART_START_DOCSTRING,\n",
    ")\n",
    "@keras_serializable\n",
    "class TFBartModel(TFBartPretrainedModel):\n",
    "    base_model_prefix = \"model\"\n",
    "\n",
    "    def __init__(self, config: BartConfig, *inputs, **kwargs):\n",
    "        super().__init__(config, *inputs, **kwargs)\n",
    "        self.shared = TFSharedEmbeddings(config.vocab_size, config.d_model, config.pad_token_id, name=\"model.shared\")\n",
    "\n",
    "        with tf.compat.v1.variable_scope(\"model.shared\") as shared_abs_scope_name:\n",
    "            pass\n",
    "\n",
    "        # Wraps layer to avoid problems with weight restoring and ensuring we're in the correct TF scope.\n",
    "        embed_tokens = TFWrappedEmbeddings(self.shared, abs_scope_name=shared_abs_scope_name)\n",
    "        embed_tokens.vocab_size = self.shared.vocab_size\n",
    "        embed_tokens.hidden_size = self.shared.hidden_size\n",
    "\n",
    "        self.encoder = TFBartEncoder(config, embed_tokens, name=\"encoder\")\n",
    "        self.decoder = TFBartDecoder(config, embed_tokens, name=\"decoder\")\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.decoder\n",
    "\n",
    "[DOCS]    @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=\"facebook/bart-large\",\n",
    "        output_type=TFSeq2SeqModelOutput,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def call(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        decoder_input_ids=None,\n",
    "        decoder_attention_mask=None,\n",
    "        encoder_outputs: Optional[Union[Tuple, TFBaseModelOutput]] = None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "        decoder_inputs_embeds=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        training=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        inputs = input_processing(\n",
    "            func=self.call,\n",
    "            config=self.config,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            training=training,\n",
    "            kwargs_call=kwargs,\n",
    "        )\n",
    "\n",
    "        if inputs[\"decoder_input_ids\"] is None and inputs[\"decoder_inputs_embeds\"] is None:\n",
    "            inputs[\"use_cache\"] = False\n",
    "\n",
    "        inputs[\"output_hidden_states\"] = (\n",
    "            inputs[\"output_hidden_states\"]\n",
    "            if inputs[\"output_hidden_states\"] is not None\n",
    "            else self.config.output_hidden_states\n",
    "        )\n",
    "\n",
    "        if inputs[\"decoder_input_ids\"] is None and inputs[\"input_ids\"] is not None:\n",
    "            inputs[\"decoder_input_ids\"] = shift_tokens_right(\n",
    "                inputs[\"input_ids\"], self.config.pad_token_id, self.config.eos_token_id\n",
    "            )\n",
    "\n",
    "        if inputs[\"encoder_outputs\"] is None:\n",
    "            inputs[\"encoder_outputs\"] = self.encoder(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                inputs_embeds=inputs[\"inputs_embeds\"],\n",
    "                output_attentions=inputs[\"output_attentions\"],\n",
    "                output_hidden_states=inputs[\"output_hidden_states\"],\n",
    "                return_dict=inputs[\"return_dict\"],\n",
    "                training=inputs[\"training\"],\n",
    "            )\n",
    "        # If the user passed a tuple for encoder_outputs, we wrap it in a TFBaseModelOutput when return_dict=True\n",
    "        elif inputs[\"return_dict\"] and not isinstance(inputs[\"encoder_outputs\"], TFBaseModelOutput):\n",
    "            inputs[\"encoder_outputs\"] = TFBaseModelOutput(\n",
    "                last_hidden_state=inputs[\"encoder_outputs\"][0],\n",
    "                hidden_states=inputs[\"encoder_outputs\"][1] if len(inputs[\"encoder_outputs\"]) > 1 else None,\n",
    "                attentions=inputs[\"encoder_outputs\"][2] if len(inputs[\"encoder_outputs\"]) > 2 else None,\n",
    "            )\n",
    "        # If the user passed a TFBaseModelOutput for encoder_outputs, we wrap it in a tuple when return_dict=False\n",
    "        elif not inputs[\"return_dict\"] and not isinstance(inputs[\"encoder_outputs\"], tuple):\n",
    "            inputs[\"encoder_outputs\"] = inputs[\"encoder_outputs\"].to_tuple()\n",
    "\n",
    "        decoder_outputs = self.decoder(\n",
    "            inputs[\"decoder_input_ids\"],\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            encoder_hidden_states=inputs[\"encoder_outputs\"][0],\n",
    "            encoder_attention_mask=inputs[\"attention_mask\"],\n",
    "            past_key_values=inputs[\"past_key_values\"],\n",
    "            inputs_embeds=inputs[\"decoder_inputs_embeds\"],\n",
    "            use_cache=inputs[\"use_cache\"],\n",
    "            output_attentions=inputs[\"output_attentions\"],\n",
    "            output_hidden_states=inputs[\"output_hidden_states\"],\n",
    "            return_dict=inputs[\"return_dict\"],\n",
    "            training=inputs[\"training\"],\n",
    "        )\n",
    "\n",
    "        if not inputs[\"return_dict\"]:\n",
    "            return decoder_outputs + inputs[\"encoder_outputs\"]\n",
    "\n",
    "        return TFSeq2SeqModelOutput(\n",
    "            last_hidden_state=decoder_outputs.last_hidden_state,\n",
    "            past_key_values=decoder_outputs.past_key_values,\n",
    "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
    "            decoder_attentions=decoder_outputs.attentions,\n",
    "            encoder_last_hidden_state=inputs[\"encoder_outputs\"].last_hidden_state,\n",
    "            encoder_hidden_states=inputs[\"encoder_outputs\"].hidden_states,\n",
    "            encoder_attentions=inputs[\"encoder_outputs\"].attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.shared\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.shared = value\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.shared\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13-8. ALBERT(A Lite BERT for Self-supervised Learning of Language Representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class TFAlbertEmbeddings(tf.keras.layers.Layer):\n",
    "    \"\"\"word, position and token_type embeddings\"\"\"\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.embedding_size = config.embedding_size \n",
    "        self.initializer_range = config.initializer_range\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.type_vocab_size = config.type_vocab_size\n",
    "        self.layer_norm_eps = config.layer_norm_eps\n",
    "        self.hidden_dropout_prob = config.hidden_dropout_prob\n",
    "\n",
    "        ## Albert에서는 hidden_size가 아닌 embedding_size로 embedding을 함\n",
    "        self.position_embeddings = tf.keras.layers.Embedding(\n",
    "            self.max_position_embeddings,\n",
    "            self.embedding_size,\n",
    "            embeddings_initializer=get_initializer(self.initializer_range),\n",
    "            name=\"position_embeddings\",\n",
    "        )\n",
    "        self.token_type_embeddings = tf.keras.layers.Embedding(\n",
    "            self.type_vocab_size,\n",
    "            self.embedding_size,\n",
    "            embeddings_initializer=get_initializer(self.initializer_range),\n",
    "            name=\"token_type_embeddings\",\n",
    "        )\n",
    "\n",
    "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=self.layer_norm_eps, name=\"LayerNorm\")\n",
    "        self.dropout = tf.keras.layers.Dropout(self.hidden_dropout_prob)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"shared word embedding layer \"\"\"\n",
    "        with tf.name_scope(\"word_embeddings\"):\n",
    "            self.word_embeddings = self.add_weight(\n",
    "                \"weight\",\n",
    "                shape=[self.vocab_size, self.embedding_size],\n",
    "                initializer=get_initializer(self.initializer_range),\n",
    "            )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        position_ids=None,\n",
    "        token_type_ids=None,\n",
    "        inputs_embeds=None,\n",
    "        mode=\"embedding\",\n",
    "        training=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        input의 token embeddings\n",
    "        Args:\n",
    "            inputs: int64 tensors (shape [batch_size, length]) 3개를 담은 리스트: (input_ids, position_ids, token_type_ids)\n",
    "            mode: \"embedding\" | \"linear\"\n",
    "        Returns:\n",
    "            outputs: mode == \"embedding\"; output embedding tensor(float32, shape [batch_size, length, embedding_size])\n",
    "                                         mode == \"linear\", output linear tensor(float32, shape [batch_size, length, vocab_size])\n",
    "        Raises:\n",
    "            ValueError: if mode is not valid.\n",
    "                \"\"\"\n",
    "\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(input_ids, position_ids, token_type_ids, inputs_embeds, training=training)\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(input_ids)\n",
    "        else:\n",
    "            raise ValueError(\"mode {} is not valid.\".format(mode))\n",
    "\n",
    "    def _embedding(self, input_ids, position_ids, token_type_ids, inputs_embeds, training=False):\n",
    "        \"\"\"input tensor에 기반하여 임베딩 적용\"\"\"\n",
    "        assert not (input_ids is None and inputs_embeds is None)\n",
    "\n",
    "        if input_ids is not None:\n",
    "            input_shape = shape_list(input_ids)\n",
    "        else:\n",
    "            input_shape = shape_list(inputs_embeds)[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "        if position_ids is None:\n",
    "            position_ids = tf.range(seq_length, dtype=tf.int32)[tf.newaxis, :]\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = tf.fill(input_shape, 0)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = tf.gather(self.word_embeddings, input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = inputs_embeds + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings, training=training)\n",
    "        return embeddings\n",
    "\n",
    "    def _linear(self, inputs):\n",
    "        \"\"\"\n",
    "               linear layer를 통해서 input의 logit을 계산\n",
    "        Args:\n",
    "            inputs: float32 tensor (shape [batch_size, length, embedding_size])\n",
    "        Returns:\n",
    "            float32 tensor (shape [batch_size, length, vocab_size])\n",
    "        \"\"\"\n",
    "        batch_size = shape_list(inputs)[0]\n",
    "        length = shape_list(inputs)[1]\n",
    "        x = tf.reshape(inputs, [-1, self.embedding_size])\n",
    "        logits = tf.matmul(x, self.word_embeddings, transpose_b=True)\n",
    "        return tf.reshape(logits, [batch_size, length, self.vocab_size])\n",
    "\n",
    "\n",
    "class TFAlbertSelfOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n",
    "        )\n",
    "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n",
    "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def call(self, hidden_states, input_tensor, training=False):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states, training=training)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class TFAlbertAttention(tf.keras.layers.Layer):\n",
    "    \"\"\" dropouts and layer norm을 포함한 attention layer \"\"\"\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        assert config.hidden_size % config.num_attention_heads == 0\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        self.query = tf.keras.layers.Dense(\n",
    "            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n",
    "        )\n",
    "        self.key = tf.keras.layers.Dense(\n",
    "            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key\"\n",
    "        )\n",
    "        self.value = tf.keras.layers.Dense(\n",
    "            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"value\"\n",
    "        )\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n",
    "        )\n",
    "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n",
    "        self.pruned_heads = set()\n",
    "        # Two different dropout probabilities; see https://github.com/google-research/albert/blob/master/modeling.py#L971-L993\n",
    "        self.attention_dropout = tf.keras.layers.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.output_dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_attention_heads, self.attention_head_size))\n",
    "\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def call(self, input_tensor, attention_mask, head_mask, output_attentions, training=False):\n",
    "        batch_size = shape_list(input_tensor)[0]\n",
    "        mixed_query_layer = self.query(input_tensor)\n",
    "        mixed_key_layer = self.key(input_tensor)\n",
    "        mixed_value_layer = self.value(input_tensor)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\n",
    "\n",
    "        # \"query\"와 \"key\"의 dot product : raw attention scores\n",
    "        # (batch size, num_heads, seq_len_q, seq_len_k)\n",
    "        attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n",
    "        # scale attention_scores\n",
    "        dk = tf.cast(shape_list(key_layer)[-1], tf.float32)\n",
    "        attention_scores = attention_scores / tf.math.sqrt(dk)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = tf.nn.softmax(attention_scores, axis=-1)\n",
    "\n",
    "        attention_probs = self.attention_dropout(attention_probs, training=training)\n",
    "\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = tf.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = tf.transpose(context_layer, perm=[0, 2, 1, 3])\n",
    "        context_layer = tf.reshape(\n",
    "            context_layer, (batch_size, -1, self.all_head_size)\n",
    "        )  # (batch_size, seq_len_q, all_head_size)\n",
    "\n",
    "        self_outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        hidden_states = self_outputs[0]\n",
    "\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.output_dropout(hidden_states, training=training)\n",
    "        attention_output = self.LayerNorm(hidden_states + input_tensor)\n",
    "\n",
    "        outputs = (attention_output,) + self_outputs[1:]\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class TFAlbertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention = TFAlbertAttention(config, name=\"attention\")\n",
    "\n",
    "        self.ffn = tf.keras.layers.Dense(\n",
    "            config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range), name=\"ffn\"\n",
    "        )\n",
    "\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.activation = get_tf_activation(config.hidden_act)\n",
    "        else:\n",
    "            self.activation = config.hidden_act\n",
    "\n",
    "        self.ffn_output = tf.keras.layers.Dense(\n",
    "            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"ffn_output\"\n",
    "        )\n",
    "        self.full_layer_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "            epsilon=config.layer_norm_eps, name=\"full_layer_layer_norm\"\n",
    "        )\n",
    "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def call(self, hidden_states, attention_mask, head_mask, output_attentions, training=False):\n",
    "        attention_outputs = self.attention(\n",
    "            hidden_states, attention_mask, head_mask, output_attentions, training=training\n",
    "        )\n",
    "        ffn_output = self.ffn(attention_outputs[0])\n",
    "        ffn_output = self.activation(ffn_output)\n",
    "        ffn_output = self.ffn_output(ffn_output)\n",
    "        ffn_output = self.dropout(ffn_output, training=training)\n",
    "\n",
    "        hidden_states = self.full_layer_layer_norm(ffn_output + attention_outputs[0])\n",
    "\n",
    "        outputs = (hidden_states,) + attention_outputs[1:]\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class TFAlbertLayerGroup(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.output_hidden_states = config.output_hidden_states\n",
    "        self.albert_layers = [\n",
    "            TFAlbertLayer(config, name=\"albert_layers_._{}\".format(i)) for i in range(config.inner_group_num)\n",
    "        ]\n",
    "\n",
    "    def call(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, training=False):\n",
    "        layer_hidden_states = ()\n",
    "        layer_attentions = ()\n",
    "\n",
    "        for layer_index, albert_layer in enumerate(self.albert_layers):\n",
    "            layer_output = albert_layer(\n",
    "                hidden_states, attention_mask, head_mask[layer_index], output_attentions, training=training\n",
    "            )\n",
    "            hidden_states = layer_output[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                layer_attentions = layer_attentions + (layer_output[1],)\n",
    "\n",
    "            if output_hidden_states:\n",
    "                layer_hidden_states = layer_hidden_states + (hidden_states,)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "        if output_hidden_states:\n",
    "            outputs = outputs + (layer_hidden_states,)\n",
    "        if output_attentions:\n",
    "            outputs = outputs + (layer_attentions,)\n",
    "        # last-layer hidden state, (layer hidden states), (layer attentions)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class TFAlbertTransformer(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_hidden_layers = config.num_hidden_layers\n",
    "        self.num_hidden_groups = config.num_hidden_groups\n",
    "        self.embedding_hidden_mapping_in = tf.keras.layers.Dense(\n",
    "            config.hidden_size,\n",
    "            kernel_initializer=get_initializer(config.initializer_range),\n",
    "            name=\"embedding_hidden_mapping_in\",\n",
    "        )\n",
    "        self.albert_layer_groups = [\n",
    "            TFAlbertLayerGroup(config, name=\"albert_layer_groups_._{}\".format(i))\n",
    "            for i in range(config.num_hidden_groups)\n",
    "        ]\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask,\n",
    "        head_mask,\n",
    "        output_attentions,\n",
    "        output_hidden_states,\n",
    "        return_dict,\n",
    "        training=False,\n",
    "    ):\n",
    "        hidden_states = self.embedding_hidden_mapping_in(hidden_states)\n",
    "        all_attentions = () if output_attentions else None\n",
    "        all_hidden_states = (hidden_states,) if output_hidden_states else None\n",
    "\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            # Number of layers in a hidden group\n",
    "            layers_per_group = int(self.num_hidden_layers / self.num_hidden_groups)\n",
    "\n",
    "            # Index of the hidden group\n",
    "            group_idx = int(i / (self.num_hidden_layers / self.num_hidden_groups))\n",
    "\n",
    "            layer_group_output = self.albert_layer_groups[group_idx](\n",
    "                hidden_states,\n",
    "                attention_mask,\n",
    "                head_mask[group_idx * layers_per_group : (group_idx + 1) * layers_per_group],\n",
    "                output_attentions,\n",
    "                output_hidden_states,\n",
    "                training=training,\n",
    "            )\n",
    "            hidden_states = layer_group_output[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + layer_group_output[-1]\n",
    "\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, all_hidden_states, all_attentions] if v is not None)\n",
    "        return TFBaseModelOutput(\n",
    "            last_hidden_state=hidden_states, hidden_states=all_hidden_states, attentions=all_attentions\n",
    "        )\n",
    "\n",
    "@keras_serializable\n",
    "class TFAlbertMainLayer(tf.keras.layers.Layer):\n",
    "\"\"\"\n",
    "모델의 전체구조\n",
    "\"\"\"\n",
    "    config_class = AlbertConfig\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_hidden_layers = config.num_hidden_layers\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.output_hidden_states = config.output_hidden_states\n",
    "        self.return_dict = config.use_return_dict\n",
    "\n",
    "        self.embeddings = TFAlbertEmbeddings(config, name=\"embeddings\")\n",
    "        self.encoder = TFAlbertTransformer(config, name=\"encoder\")\n",
    "        self.pooler = tf.keras.layers.Dense(\n",
    "            config.hidden_size,\n",
    "            kernel_initializer=get_initializer(config.initializer_range),\n",
    "            activation=\"tanh\",\n",
    "            name=\"pooler\",\n",
    "        )\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "        self.embeddings.vocab_size = value.shape[0]\n",
    "\n",
    "    def _resize_token_embeddings(self, new_num_tokens):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        inputs,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        training=False,\n",
    "    ):\n",
    "        if isinstance(inputs, (tuple, list)):\n",
    "            input_ids = inputs[0]\n",
    "            attention_mask = inputs[1] if len(inputs) > 1 else attention_mask\n",
    "            token_type_ids = inputs[2] if len(inputs) > 2 else token_type_ids\n",
    "            position_ids = inputs[3] if len(inputs) > 3 else position_ids\n",
    "            head_mask = inputs[4] if len(inputs) > 4 else head_mask\n",
    "            inputs_embeds = inputs[5] if len(inputs) > 5 else inputs_embeds\n",
    "            output_attentions = inputs[6] if len(inputs) > 6 else output_attentions\n",
    "            output_hidden_states = inputs[7] if len(inputs) > 7 else output_hidden_states\n",
    "            return_dict = inputs[8] if len(inputs) > 8 else return_dict\n",
    "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
    "        elif isinstance(inputs, (dict, BatchEncoding)):\n",
    "            input_ids = inputs.get(\"input_ids\")\n",
    "            attention_mask = inputs.get(\"attention_mask\", attention_mask)\n",
    "            token_type_ids = inputs.get(\"token_type_ids\", token_type_ids)\n",
    "            position_ids = inputs.get(\"position_ids\", position_ids)\n",
    "            head_mask = inputs.get(\"head_mask\", head_mask)\n",
    "            inputs_embeds = inputs.get(\"inputs_embeds\", inputs_embeds)\n",
    "            output_attentions = inputs.get(\"output_attentions\", output_attentions)\n",
    "            output_hidden_states = inputs.get(\"output_hidden_states\", output_hidden_states)\n",
    "            return_dict = inputs.get(\"return_dict\", return_dict)\n",
    "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
    "        else:\n",
    "            input_ids = inputs\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n",
    "        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.output_hidden_states\n",
    "        return_dict = return_dict if return_dict is not None else self.return_dict\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = shape_list(input_ids)\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = shape_list(inputs_embeds)[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = tf.fill(input_shape, 1)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = tf.fill(input_shape, 0)\n",
    "\n",
    "        # 3D attention mask 만들기\n",
    "        # Sizes : [batch_size, 1, 1, to_seq_length]\n",
    "        # 3D attention mask를 [batch_size, num_heads, from_seq_length, to_seq_length]로 브로드캐스팅\n",
    "\n",
    "        extended_attention_mask = attention_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "                # attention_mask가 1.0이면 포함, 0.0이면 미포함 하여 attention 계산\n",
    "        extended_attention_mask = tf.cast(extended_attention_mask, tf.float32)\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        # head_mask가 1.0이면, head를 유지\n",
    "        # attention_probs : shape bsz x n_heads x N x N\n",
    "        # input head_mask : shape [num_heads] 또는 [num_hidden_layers x num_heads]\n",
    "        # head_mask : shape [num_hidden_layers x batch x num_heads x seq_length x seq_length\n",
    "        if head_mask is not None:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            head_mask = [None] * self.num_hidden_layers\n",
    "            # head_mask = tf.constant([0] * self.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings(input_ids, position_ids, token_type_ids, inputs_embeds, training=training)\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            extended_attention_mask,\n",
    "            head_mask,\n",
    "            output_attentions,\n",
    "            output_hidden_states,\n",
    "            return_dict,\n",
    "            training=training,\n",
    "        )\n",
    "\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output[:, 0])\n",
    "\n",
    "        if not return_dict:\n",
    "            return (\n",
    "                sequence_output,\n",
    "                pooled_output,\n",
    "            ) + encoder_outputs[1:]\n",
    "\n",
    "        return TFBaseModelOutputWithPooling(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13-9. T5(Text-to-Text Transfer Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class TFT5LayerNorm(tf.keras.layers.Layer):\n",
    "    def __init__(self, epsilon=1e-6, **kwargs):\n",
    "        \"\"\"\n",
    "        Construct a layernorm module in the T5 style No bias and no subtraction of mean.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.variance_epsilon = epsilon\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Build shared word embedding layer\"\"\"\n",
    "        self.weight = self.add_weight(\"weight\", shape=(input_shape[-1],), initializer=\"ones\")\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, hidden_states):\n",
    "        variance = tf.math.reduce_mean(tf.math.square(hidden_states), axis=-1, keepdims=True)\n",
    "        hidden_states = hidden_states * tf.math.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states\n",
    "\n",
    "\n",
    "class TFT5DenseReluDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.wi = tf.keras.layers.Dense(config.d_ff, use_bias=False, name=\"wi\")\n",
    "        self.wo = tf.keras.layers.Dense(config.d_model, use_bias=False, name=\"wo\")\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout_rate)\n",
    "        self.act = tf.keras.activations.relu\n",
    "\n",
    "    def call(self, hidden_states, training=False):\n",
    "        hidden_states = self.wi(hidden_states)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states, training=training)\n",
    "        hidden_states = self.wo(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class TFT5GatedGeluDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.wi_0 = tf.keras.layers.Dense(config.d_ff, use_bias=False, name=\"wi_0\")\n",
    "        self.wi_1 = tf.keras.layers.Dense(config.d_ff, use_bias=False, name=\"wi_1\")\n",
    "        self.wo = tf.keras.layers.Dense(config.d_model, use_bias=False, name=\"wo\")\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout_rate)\n",
    "        self.act = get_tf_activation(\"gelu_new\")\n",
    "\n",
    "    def call(self, hidden_states, training=False):\n",
    "        hidden_gelu = self.act(self.wi_0(hidden_states))\n",
    "        hidden_linear = self.wi_1(hidden_states)\n",
    "        hidden_states = hidden_gelu * hidden_linear\n",
    "        hidden_states = self.dropout(hidden_states, training=training)\n",
    "        hidden_states = self.wo(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class TFT5LayerFF(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if config.feed_forward_proj == \"relu\":\n",
    "            self.DenseReluDense = TFT5DenseReluDense(config, name=\"DenseReluDense\")\n",
    "        elif config.feed_forward_proj == \"gated-gelu\":\n",
    "            self.DenseReluDense = TFT5GatedGeluDense(config, name=\"DenseReluDense\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"{self.config.feed_forward_proj} is not supported. Choose between `relu` and `gated-gelu`\"\n",
    "            )\n",
    "        self.layer_norm = TFT5LayerNorm(epsilon=config.layer_norm_epsilon, name=\"layer_norm\")\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout_rate)\n",
    "\n",
    "    def call(self, hidden_states, training=False):\n",
    "        normed_hidden_states = self.layer_norm(hidden_states)\n",
    "        dense_output = self.DenseReluDense(normed_hidden_states, training=training)\n",
    "        hidden_states = hidden_states + self.dropout(dense_output, training=training)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class TFT5Attention(tf.keras.layers.Layer):\n",
    "    NEW_ID = itertools.count()\n",
    "\n",
    "    def __init__(self, config, has_relative_attention_bias=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layer_id = next(TFT5Attention.NEW_ID)\n",
    "        self.is_decoder = config.is_decoder\n",
    "        self.use_cache = config.use_cache\n",
    "        self.has_relative_attention_bias = has_relative_attention_bias\n",
    "        self.output_attentions = config.output_attentions\n",
    "\n",
    "        self.relative_attention_num_buckets = config.relative_attention_num_buckets\n",
    "        self.d_model = config.d_model\n",
    "        self.key_value_proj_dim = config.d_kv\n",
    "        self.n_heads = config.num_heads\n",
    "        self.inner_dim = self.n_heads * self.key_value_proj_dim\n",
    "\n",
    "        # Mesh TensorFlow initialization to avoid scaling before softmax\n",
    "        self.q = tf.keras.layers.Dense(self.inner_dim, use_bias=False, name=\"q\")\n",
    "        self.k = tf.keras.layers.Dense(self.inner_dim, use_bias=False, name=\"k\")\n",
    "        self.v = tf.keras.layers.Dense(self.inner_dim, use_bias=False, name=\"v\")\n",
    "        self.o = tf.keras.layers.Dense(self.d_model, use_bias=False, name=\"o\")\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout_rate)\n",
    "\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.has_relative_attention_bias:\n",
    "            with tf.name_scope(\"relative_attention_bias\"):\n",
    "                self.relative_attention_bias = self.add_weight(\n",
    "                    name=\"embeddings\",\n",
    "                    shape=[self.relative_attention_num_buckets, self.n_heads],\n",
    "                )\n",
    "\n",
    "        return super().build(input_shape)\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n",
    "        \"\"\"\n",
    "        Adapted from Mesh Tensorflow:\n",
    "        https://github.com/tensorflow/mesh/blob/0cb87fe07da627bf0b7e60475d59f95ed6b5be3d/mesh_tensorflow/transformer/transformer_layers.py#L593\n",
    "\n",
    "        Translate relative position to a bucket number for relative attention. The relative position is defined as\n",
    "        memory_position - query_position, i.e. the distance in tokens from the attending position to the attended-to\n",
    "        position. If bidirectional=False, then positive relative positions are invalid. We use smaller buckets for\n",
    "        small absolute relative_position and larger buckets for larger absolute relative_positions. All relative\n",
    "        positions >=max_distance map to the same bucket. All relative positions <=-max_distance map to the same bucket.\n",
    "        This should allow for more graceful generalization to longer sequences than the model has been trained on\n",
    "\n",
    "        Args:\n",
    "            relative_position: an int32 Tensor\n",
    "            bidirectional: a boolean - whether the attention is bidirectional\n",
    "            num_buckets: an integer\n",
    "            max_distance: an integer\n",
    "\n",
    "        Returns:\n",
    "            a Tensor with the same shape as relative_position, containing int32 values in the range [0, num_buckets)\n",
    "        \"\"\"\n",
    "        relative_buckets = 0\n",
    "        #        n = -relative_position\n",
    "        if bidirectional:\n",
    "            num_buckets //= 2\n",
    "            relative_buckets += (\n",
    "                tf.cast(tf.math.greater(relative_position, 0), dtype=relative_position.dtype) * num_buckets\n",
    "            )\n",
    "            relative_position = tf.math.abs(relative_position)\n",
    "        else:\n",
    "            relative_position = -tf.math.minimum(relative_position, 0)\n",
    "        # now n is in the range [0, inf)\n",
    "        max_exact = num_buckets // 2\n",
    "        is_small = tf.math.less(relative_position, max_exact)\n",
    "        relative_position_if_large = max_exact + tf.cast(\n",
    "            tf.math.log(relative_position / max_exact)\n",
    "            / math.log(max_distance / max_exact)\n",
    "            * (num_buckets - max_exact),\n",
    "            dtype=relative_position.dtype,\n",
    "        )\n",
    "        relative_position_if_large = tf.math.minimum(relative_position_if_large, num_buckets - 1)\n",
    "        relative_buckets += tf.where(is_small, relative_position, relative_position_if_large)\n",
    "        return relative_buckets\n",
    "\n",
    "    def compute_bias(self, query_length, key_length):\n",
    "        \"\"\"Compute binned relative position bias\"\"\"\n",
    "        context_position = tf.range(query_length)[:, None]\n",
    "        memory_position = tf.range(key_length)[None, :]\n",
    "        relative_position = memory_position - context_position  # shape (query_length, key_length)\n",
    "        relative_position_bucket = self._relative_position_bucket(\n",
    "            relative_position,\n",
    "            bidirectional=(not self.is_decoder),\n",
    "            num_buckets=self.relative_attention_num_buckets,\n",
    "        )\n",
    "        values = tf.gather(\n",
    "            self.relative_attention_bias, relative_position_bucket\n",
    "        )  # shape (query_length, key_length, num_heads)\n",
    "        values = tf.expand_dims(\n",
    "            tf.transpose(values, [2, 0, 1]), axis=0\n",
    "        )  # shape (1, num_heads, query_length, key_length)\n",
    "        return values\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        mask=None,\n",
    "        key_value_states=None,\n",
    "        position_bias=None,\n",
    "        past_key_value=None,\n",
    "        layer_head_mask=None,\n",
    "        query_length=None,\n",
    "        use_cache=False,\n",
    "        training=False,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).\n",
    "        \"\"\"\n",
    "        # Input is (batch_size, query_length, dim)\n",
    "        # Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)\n",
    "        # past_key_value[0] is (batch_size, n_heads, q_len - 1, dim_per_head)\n",
    "        batch_size, seq_length = shape_list(hidden_states)[:2]\n",
    "\n",
    "        real_seq_length = seq_length\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            assert (\n",
    "                len(past_key_value) == 2\n",
    "            ), f\"past_key_value should have 2 past states: keys and values. Got {len(past_key_value)} past states\"\n",
    "            real_seq_length += shape_list(past_key_value[0])[2] if query_length is None else query_length\n",
    "\n",
    "        key_length = real_seq_length if key_value_states is None else shape_list(key_value_states)[1]\n",
    "\n",
    "        def shape(hidden_states):\n",
    "            \"\"\"projection\"\"\"\n",
    "            return tf.transpose(\n",
    "                tf.reshape(hidden_states, (batch_size, -1, self.n_heads, self.key_value_proj_dim)), perm=(0, 2, 1, 3)\n",
    "            )\n",
    "\n",
    "        def unshape(hidden_states):\n",
    "            \"\"\"compute context\"\"\"\n",
    "            return tf.reshape(tf.transpose(hidden_states, perm=(0, 2, 1, 3)), (batch_size, -1, self.inner_dim))\n",
    "\n",
    "        def project(hidden_states, proj_layer, key_value_states, past_key_value):\n",
    "            \"\"\"projects hidden states correctly to key/query states\"\"\"\n",
    "            if key_value_states is None:\n",
    "                # self-attn\n",
    "                # (batch_size, n_heads, seq_length, dim_per_head)\n",
    "                hidden_states = shape(proj_layer(hidden_states))\n",
    "            elif past_key_value is None:\n",
    "                # cross-attn\n",
    "                # (batch_size, n_heads, seq_length, dim_per_head)\n",
    "                hidden_states = shape(proj_layer(key_value_states))\n",
    "\n",
    "            if past_key_value is not None:\n",
    "                if key_value_states is None:\n",
    "                    # self-attn\n",
    "                    # (batch_size, n_heads, key_length, dim_per_head)\n",
    "                    hidden_states = tf.concat([past_key_value, hidden_states], axis=2)\n",
    "                else:\n",
    "                    # cross-attn\n",
    "                    hidden_states = past_key_value\n",
    "            return hidden_states\n",
    "\n",
    "        # get query\n",
    "        query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, query_length, dim_per_head)\n",
    "\n",
    "        # get key/value\n",
    "        key_states = project(\n",
    "            hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None\n",
    "        )\n",
    "        value_states = project(\n",
    "            hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None\n",
    "        )\n",
    "\n",
    "        # to cope with keras serialization\n",
    "        if self.is_decoder and use_cache:\n",
    "            present_key_value_state = (key_states, value_states)\n",
    "        else:\n",
    "            present_key_value_state = None\n",
    "\n",
    "        scores = tf.einsum(\n",
    "            \"bnqd,bnkd->bnqk\", query_states, key_states\n",
    "        )  # (batch_size, n_heads, query_length, key_length)\n",
    "\n",
    "        if position_bias is None:\n",
    "            if not self.has_relative_attention_bias:\n",
    "                position_bias = tf.zeros((1, self.n_heads, real_seq_length, key_length))\n",
    "            else:\n",
    "                position_bias = self.compute_bias(real_seq_length, key_length)\n",
    "\n",
    "            # if key and values are already calculated\n",
    "            # we want only the last query position bias\n",
    "            if past_key_value is not None:\n",
    "                position_bias = position_bias[:, :, -seq_length:, :]\n",
    "\n",
    "            if mask is not None:\n",
    "                position_bias = tf.cast(position_bias, dtype=mask.dtype)\n",
    "                position_bias = position_bias + mask  # (batch_size, n_heads, query_length, key_length)\n",
    "\n",
    "        scores += position_bias\n",
    "        weights = tf.nn.softmax(scores, axis=-1)  # (batch_size, n_heads, query_length, key_length)\n",
    "        weights = self.dropout(weights, training=training)  # (batch_size, n_heads, query_length, key_length)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if layer_head_mask is not None:\n",
    "            tf.debugging.assert_equal(\n",
    "                shape_list(layer_head_mask),\n",
    "                [self.n_heads],\n",
    "                message=f\"Head mask for a single layer should be of size {(self.n_heads)}, but is {shape_list(layer_head_mask)}\",\n",
    "            )\n",
    "            weights = tf.reshape(layer_head_mask, (1, -1, 1, 1)) * weights\n",
    "\n",
    "        attn_output = tf.matmul(weights, value_states)  # (batch_size, n_heads, query_length, dim_per_head)\n",
    "\n",
    "        attn_output = self.o(unshape(attn_output))\n",
    "\n",
    "        outputs = (attn_output,) + (present_key_value_state,) + (position_bias,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs = outputs + (weights,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class TFT5LayerSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, has_relative_attention_bias=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.SelfAttention = TFT5Attention(\n",
    "            config,\n",
    "            has_relative_attention_bias=has_relative_attention_bias,\n",
    "            name=\"SelfAttention\",\n",
    "        )\n",
    "        self.layer_norm = TFT5LayerNorm(epsilon=config.layer_norm_epsilon, name=\"layer_norm\")\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout_rate)\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        position_bias=None,\n",
    "        layer_head_mask=None,\n",
    "        past_key_value=None,\n",
    "        use_cache=False,\n",
    "        output_attentions=False,\n",
    "        training=False,\n",
    "    ):\n",
    "        normed_hidden_states = self.layer_norm(hidden_states)\n",
    "        attention_output = self.SelfAttention(\n",
    "            normed_hidden_states,\n",
    "            mask=attention_mask,\n",
    "            position_bias=position_bias,\n",
    "            layer_head_mask=layer_head_mask,\n",
    "            past_key_value=past_key_value,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            training=training,\n",
    "        )\n",
    "        hidden_states = hidden_states + self.dropout(attention_output[0], training=training)\n",
    "        outputs = (hidden_states,) + attention_output[1:]  # add attentions if we output them\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class TFT5LayerCrossAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.EncDecAttention = TFT5Attention(\n",
    "            config,\n",
    "            has_relative_attention_bias=False,\n",
    "            name=\"EncDecAttention\",\n",
    "        )\n",
    "        self.layer_norm = TFT5LayerNorm(epsilon=config.layer_norm_epsilon, name=\"layer_norm\")\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout_rate)\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        key_value_states,\n",
    "        attention_mask=None,\n",
    "        position_bias=None,\n",
    "        layer_head_mask=None,\n",
    "        past_key_value=None,\n",
    "        query_length=None,\n",
    "        use_cache=False,\n",
    "        output_attentions=False,\n",
    "        training=False,\n",
    "    ):\n",
    "        normed_hidden_states = self.layer_norm(hidden_states)\n",
    "        attention_output = self.EncDecAttention(\n",
    "            normed_hidden_states,\n",
    "            mask=attention_mask,\n",
    "            key_value_states=key_value_states,\n",
    "            position_bias=position_bias,\n",
    "            layer_head_mask=layer_head_mask,\n",
    "            past_key_value=past_key_value,\n",
    "            query_length=query_length,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            training=training,\n",
    "        )\n",
    "        hidden_states = hidden_states + self.dropout(attention_output[0], training=training)\n",
    "        outputs = (hidden_states,) + attention_output[1:]  # add attentions if we output them\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class TFT5Block(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, has_relative_attention_bias=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.is_decoder = config.is_decoder\n",
    "        self.layer = []\n",
    "        self.layer.append(\n",
    "            TFT5LayerSelfAttention(\n",
    "                config,\n",
    "                has_relative_attention_bias=has_relative_attention_bias,\n",
    "                name=\"layer_._0\",\n",
    "            )\n",
    "        )\n",
    "        if self.is_decoder:\n",
    "            self.layer.append(\n",
    "                TFT5LayerCrossAttention(\n",
    "                    config,\n",
    "                    name=\"layer_._1\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.layer.append(TFT5LayerFF(config, name=f\"layer_._{len(self.layer)}\"))\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        position_bias=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        encoder_decoder_position_bias=None,\n",
    "        layer_head_mask=None,\n",
    "        encoder_layer_head_mask=None,\n",
    "        past_key_value=None,\n",
    "        use_cache=False,\n",
    "        output_attentions=False,\n",
    "        training=False,\n",
    "    ):\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            assert self.is_decoder, \"Only decoder can use `past_key_values`\"\n",
    "            expected_num_past_key_values = 2 if encoder_hidden_states is None else 4\n",
    "\n",
    "            if len(past_key_value) != expected_num_past_key_values:\n",
    "                raise ValueError(\n",
    "                    f\"There should be {expected_num_past_key_values} past states. \"\n",
    "                    f\"{'2 (past / key) for cross attention' if expected_num_past_key_values == 4 else ''}.\"\n",
    "                    f\"Got {len(past_key_value)} past key / value states\"\n",
    "                )\n",
    "\n",
    "            self_attn_past_key_value = past_key_value[:2]\n",
    "            cross_attn_past_key_value = past_key_value[2:]\n",
    "        else:\n",
    "            self_attn_past_key_value, cross_attn_past_key_value = None, None\n",
    "\n",
    "        self_attention_outputs = self.layer[0](\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_bias=position_bias,\n",
    "            layer_head_mask=layer_head_mask,\n",
    "            past_key_value=self_attn_past_key_value,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            training=training,\n",
    "        )\n",
    "        hidden_states, present_key_value_state = self_attention_outputs[:2]\n",
    "        attention_outputs = self_attention_outputs[2:]  # Keep self-attention outputs and relative position weights\n",
    "\n",
    "        if self.is_decoder and encoder_hidden_states is not None:\n",
    "            # the actual query length is unknown for cross attention\n",
    "            # if using past key value states. Need to inject it here\n",
    "            if present_key_value_state is not None:\n",
    "                query_length = shape_list(present_key_value_state[0])[2]\n",
    "            else:\n",
    "                query_length = None\n",
    "\n",
    "            cross_attention_outputs = self.layer[1](\n",
    "                hidden_states,\n",
    "                key_value_states=encoder_hidden_states,\n",
    "                attention_mask=encoder_attention_mask,\n",
    "                position_bias=encoder_decoder_position_bias,\n",
    "                layer_head_mask=encoder_layer_head_mask,\n",
    "                past_key_value=cross_attn_past_key_value,\n",
    "                query_length=query_length,\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "                training=training,\n",
    "            )\n",
    "            hidden_states = cross_attention_outputs[0]\n",
    "            # Combine self attn and cross attn key value states\n",
    "            if present_key_value_state is not None:\n",
    "                present_key_value_state = present_key_value_state + cross_attention_outputs[1]\n",
    "\n",
    "            # Keep cross-attention outputs and relative position weights\n",
    "            attention_outputs = attention_outputs + cross_attention_outputs[2:]\n",
    "\n",
    "        # Apply Feed Forward layer\n",
    "        hidden_states = self.layer[-1](hidden_states, training=training)\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        # Add attentions if we output them\n",
    "        outputs = outputs + (present_key_value_state,) + attention_outputs\n",
    "        return outputs  # hidden-states, present_key_value_states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)\n",
    "\n",
    "@keras_serializable\n",
    "class TFT5MainLayer(tf.keras.layers.Layer):\n",
    "    config_class = T5Config\n",
    "\n",
    "    def __init__(self, config, embed_tokens=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.config = config\n",
    "        self.output_hidden_states = config.output_hidden_states\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.use_cache = config.use_cache\n",
    "\n",
    "        self.embed_tokens = embed_tokens\n",
    "        self.is_decoder = config.is_decoder\n",
    "\n",
    "        self.config = config\n",
    "        self.num_hidden_layers = config.num_layers\n",
    "\n",
    "        self.block = [\n",
    "            TFT5Block(config, has_relative_attention_bias=bool(i == 0), name=f\"block_._{i}\")\n",
    "            for i in range(config.num_layers)\n",
    "        ]\n",
    "        self.final_layer_norm = TFT5LayerNorm(epsilon=config.layer_norm_epsilon, name=\"final_layer_norm\")\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout_rate)\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        raise NotImplementedError  # Not implemented yet in the library fr TF 2.0 models\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        head_mask=None,\n",
    "        encoder_head_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        training=False,\n",
    "        **kwargs,\n",
    "    ) -> Tuple:\n",
    "        inputs = input_processing(\n",
    "            func=self.call,\n",
    "            config=self.config,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            head_mask=head_mask,\n",
    "            encoder_head_mask=encoder_head_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            training=training,\n",
    "            kwargs_call=kwargs,\n",
    "        )\n",
    "\n",
    "        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\n",
    "            err_msg_prefix = \"decoder_\" if self.is_decoder else \"\"\n",
    "            raise ValueError(\n",
    "                f\"You cannot specify both {err_msg_prefix}input_ids and {err_msg_prefix}inputs_embeds at the same time\"\n",
    "            )\n",
    "        elif inputs[\"input_ids\"] is not None:\n",
    "            input_shape = shape_list(inputs[\"input_ids\"])\n",
    "            inputs[\"input_ids\"] = tf.reshape(inputs[\"input_ids\"], (-1, input_shape[-1]))\n",
    "        elif inputs[\"inputs_embeds\"] is not None:\n",
    "            input_shape = shape_list(inputs[\"inputs_embeds\"])[:-1]\n",
    "        else:\n",
    "            err_msg_prefix = \"decoder_\" if self.is_decoder else \"\"\n",
    "            raise ValueError(f\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\")\n",
    "\n",
    "        if inputs[\"inputs_embeds\"] is None:\n",
    "            assert self.embed_tokens is not None, \"You have to initialize the model with valid token embeddings\"\n",
    "            inputs[\"inputs_embeds\"] = self.embed_tokens(inputs[\"input_ids\"])\n",
    "\n",
    "        batch_size, seq_length = input_shape\n",
    "\n",
    "        # required mask seq length can be calculated via length of past\n",
    "        mask_seq_length = (\n",
    "            shape_list(inputs[\"past_key_values\"][0][0])[2] + seq_length\n",
    "            if inputs[\"past_key_values\"] is not None\n",
    "            else seq_length\n",
    "        )\n",
    "\n",
    "        if inputs[\"attention_mask\"] is None:\n",
    "            inputs[\"attention_mask\"] = tf.fill((batch_size, mask_seq_length), 1)\n",
    "        if (\n",
    "            self.is_decoder\n",
    "            and inputs[\"encoder_attention_mask\"] is None\n",
    "            and inputs[\"encoder_hidden_states\"] is not None\n",
    "        ):\n",
    "            encoder_seq_length = shape_list(inputs[\"encoder_hidden_states\"])[1]\n",
    "            inputs[\"encoder_attention_mask\"] = tf.fill((batch_size, encoder_seq_length), 1)\n",
    "\n",
    "        # initialize past_key_values with `None` if past does not exist\n",
    "        if inputs[\"past_key_values\"] is None:\n",
    "            inputs[\"past_key_values\"] = [None] * len(self.block)\n",
    "\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        inputs[\"attention_mask\"] = tf.cast(inputs[\"attention_mask\"], dtype=inputs[\"inputs_embeds\"].dtype)\n",
    "        num_dims_attention_mask = len(shape_list(inputs[\"attention_mask\"]))\n",
    "        if num_dims_attention_mask == 3:\n",
    "            extended_attention_mask = inputs[\"attention_mask\"][:, None, :, :]\n",
    "        elif num_dims_attention_mask == 2:\n",
    "            # Provided a padding mask of dimensions [batch_size, mask_seq_length]\n",
    "            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n",
    "            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, mask_seq_length, mask_seq_length]\n",
    "            if self.is_decoder:\n",
    "                seq_ids = tf.range(mask_seq_length)\n",
    "                causal_mask = tf.less_equal(\n",
    "                    tf.tile(seq_ids[None, None, :], (batch_size, mask_seq_length, 1)),\n",
    "                    seq_ids[None, :, None],\n",
    "                )\n",
    "                causal_mask = tf.cast(causal_mask, dtype=inputs[\"attention_mask\"].dtype)\n",
    "                extended_attention_mask = causal_mask[:, None, :, :] * inputs[\"attention_mask\"][:, None, None, :]\n",
    "                if inputs[\"past_key_values\"][0] is not None:\n",
    "                    extended_attention_mask = extended_attention_mask[:, :, -seq_length:, :]\n",
    "            else:\n",
    "                extended_attention_mask = inputs[\"attention_mask\"][:, None, None, :]\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and  -1e9 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "\n",
    "        # T5 has a mask that can compare sequence ids, we can simulate this here with this transposition\n",
    "        # Cf. https://github.com/tensorflow/mesh/blob/8d2465e9bc93129b913b5ccc6a59aa97abd96ec6/mesh_tensorflow/transformer/transformer_layers.py#L270\n",
    "        # extended_attention_mask = tf.math.equal(extended_attention_mask,\n",
    "        #                                         tf.transpose(extended_attention_mask, perm=(-1, -2)))\n",
    "\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -1e9\n",
    "\n",
    "        if self.is_decoder and inputs[\"encoder_attention_mask\"] is not None:\n",
    "            # If a 2D ou 3D attention mask is provided for the cross-attention\n",
    "            # we need to make broadcastable to [batch_size, num_heads, mask_seq_length, mask_seq_length]\n",
    "            # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "            inputs[\"encoder_attention_mask\"] = tf.cast(\n",
    "                inputs[\"encoder_attention_mask\"], dtype=extended_attention_mask.dtype\n",
    "            )\n",
    "            num_dims_encoder_attention_mask = len(shape_list(inputs[\"encoder_attention_mask\"]))\n",
    "            if num_dims_encoder_attention_mask == 3:\n",
    "                encoder_extended_attention_mask = inputs[\"encoder_attention_mask\"][:, None, :, :]\n",
    "            if num_dims_encoder_attention_mask == 2:\n",
    "                encoder_extended_attention_mask = inputs[\"encoder_attention_mask\"][:, None, None, :]\n",
    "\n",
    "            # T5 has a mask that can compare sequence ids, we can simulate this here with this transposition\n",
    "            # Cf. https://github.com/tensorflow/mesh/blob/8d2465e9bc93129b913b5ccc6a59aa97abd96ec6/mesh_tensorflow/transformer/transformer_layers.py#L270\n",
    "            # encoder_extended_attention_mask = tf.math.equal(encoder_extended_attention_mask,\n",
    "            #                                         tf.transpose(encoder_extended_attention_mask, perm=(-1, -2)))\n",
    "\n",
    "            encoder_extended_attention_mask = (1.0 - encoder_extended_attention_mask) * -1e9\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        present_key_value_states = () if inputs[\"use_cache\"] and self.is_decoder else None\n",
    "        all_hidden_states = () if inputs[\"output_hidden_states\"] else None\n",
    "        all_attentions = () if inputs[\"output_attentions\"] else None\n",
    "        position_bias = None\n",
    "        encoder_decoder_position_bias = None\n",
    "\n",
    "        hidden_states = self.dropout(inputs[\"inputs_embeds\"], training=inputs[\"training\"])\n",
    "\n",
    "        for idx, (layer_module, past_key_value) in enumerate(zip(self.block, inputs[\"past_key_values\"])):\n",
    "            if inputs[\"output_hidden_states\"]:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "            layer_outputs = layer_module(\n",
    "                hidden_states,\n",
    "                attention_mask=extended_attention_mask,\n",
    "                position_bias=position_bias,\n",
    "                encoder_hidden_states=inputs[\"encoder_hidden_states\"],\n",
    "                encoder_attention_mask=encoder_extended_attention_mask,\n",
    "                encoder_decoder_position_bias=encoder_decoder_position_bias,\n",
    "                layer_head_mask=inputs[\"head_mask\"][idx] if inputs[\"head_mask\"] is not None else None,\n",
    "                encoder_layer_head_mask=inputs[\"encoder_head_mask\"][idx]\n",
    "                if inputs[\"encoder_head_mask\"] is not None\n",
    "                else None,\n",
    "                past_key_value=past_key_value,\n",
    "                use_cache=inputs[\"use_cache\"],\n",
    "                output_attentions=inputs[\"output_attentions\"],\n",
    "                training=inputs[\"training\"],\n",
    "            )\n",
    "\n",
    "            # layer_outputs is a tuple with:\n",
    "            # hidden-states, key-value-states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)\n",
    "            hidden_states, present_key_value_state = layer_outputs[:2]\n",
    "\n",
    "            # We share the position biases between the layers - the first layer store them\n",
    "            # layer_outputs = hidden-states, past_key_values, (self-attention weights),\n",
    "            # (self-attention position bias), (cross-attention position bias), (cross-attention weights),\n",
    "            position_bias = layer_outputs[2]\n",
    "\n",
    "            if self.is_decoder and inputs[\"encoder_hidden_states\"] is not None:\n",
    "                encoder_decoder_position_bias = layer_outputs[4 if inputs[\"output_attentions\"] else 3]\n",
    "\n",
    "            # append next layer key value states\n",
    "            if present_key_value_state is not None and inputs[\"use_cache\"] and self.is_decoder:\n",
    "                present_key_value_states = present_key_value_states + (present_key_value_state,)\n",
    "\n",
    "            if inputs[\"output_attentions\"]:\n",
    "                all_attentions = all_attentions + (layer_outputs[3],)\n",
    "\n",
    "        hidden_states = self.final_layer_norm(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states, training=inputs[\"training\"])\n",
    "\n",
    "        # Add last layer\n",
    "        if inputs[\"output_hidden_states\"]:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not inputs[\"return_dict\"]:\n",
    "            outputs = (hidden_states,)\n",
    "            # need to check if is decoder here as well for special cases when using keras compile\n",
    "            if inputs[\"use_cache\"] and self.is_decoder:\n",
    "                outputs = outputs + (present_key_value_states,)\n",
    "            if inputs[\"output_hidden_states\"]:\n",
    "                outputs = outputs + (all_hidden_states,)\n",
    "            if inputs[\"output_attentions\"]:\n",
    "                outputs = outputs + (all_attentions,)\n",
    "            return outputs  # last-layer hidden state, (all hidden states), (all attentions)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            return TFBaseModelOutputWithPast(\n",
    "                last_hidden_state=hidden_states,\n",
    "                past_key_values=present_key_value_states,\n",
    "                hidden_states=all_hidden_states,\n",
    "                attentions=all_attentions,\n",
    "            )\n",
    "        else:\n",
    "            return TFBaseModelOutput(\n",
    "                last_hidden_state=hidden_states,\n",
    "                hidden_states=all_hidden_states,\n",
    "                attentions=all_attentions,\n",
    "            )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13-10. Switch Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13-11. ERNIE"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ce9abe337a9e694d01ea52d504102083454ad8bd4b0e3a574e4432f4229329"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('aiffel_3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
