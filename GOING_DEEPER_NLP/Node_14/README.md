# 14. BERT pretrained model 제작

**가장 대표적인 pretrained ㅣlanguage model인 BERT의 pretrain 전과정을 진행해 보면서 BERT의 핵심원리를 깊이 이해해 본다.**

---

|-|목차|⏲ 420분|
|:---:|---|:---:|
|14-1| 들어가며 | 10분|
|14-2| Tokenizer 준비 | 30분|
|14-3| 데이터 전처리 (1) MASK 생성 | 30분|
|14-4| 데이터 전처리 (2) NSP pair 생성 | 30분|
|14-5| 데이터 전처리 (3) 데이터셋 완성 | 30분|
|14-6| BERT 모델 구현 | 50분
|14-7| pretrain 진행 | 60분
|14-8| 프로젝트 : mini BERT 만들기 | 180분
|14-9| 프로젝트 제출 |-|

---

## 목차

1. 들어가며
2. Tokenizer 준비
3. 데이터 전처리 (1) MASK 생성
4. 데이터 전처리 (2) NSP pair 생성
5. 데이터 전처리 (3) 데이터셋 완성
6. BERT 모델 구현
7. pretrain 진행
8. 프로젝트 : mini BERT 만들기

---

## 14-8. 프로젝트 : mini BERT 만들기

1. Tokenizer 준비
2. 데이터 전처리 (1) MASK 생성
3. 데이터 전처리 (2) NSP pair 생성
4. 데이터 전처리 (3) 데이터셋 완성
5. BERT 모델 구현
6. pretrain 진행
7. 프로젝트 결과

---

>## **루브릭**
>
>|번호|평가문항|상세기준|
>|:---:|---|---|
>|1|한글 코퍼스를 가공하여 BERT pretrain용 데이터셋을 잘 생성하였다.|MLM, NSP task의 특징이 잘 반영된 pretrain용 데이터셋 생성과정이 체계적으로 진행되었다.|
>|2|구현한 BERT 모델의 학습이 안정적으로 진행됨을 확인하였다.|학습진행 과정 중에 MLM, NSP loss의 안정적인 감소가 확인되었다.|
>|3|1M짜리 mini BERT 모델의 제작과 학습이 정상적으로 진행되었다.|학습된 모델 및 학습과정의 시각화 내역이 제출되었다.|
