<p align='center'>
  <a href='https://aiffel.oopy.io/' target='_blank'>
    <img src='https://aiffel-front-prod-asset.s3.ap-northeast-2.amazonaws.com/img/logo/aiffel_logo.png'>
  </a>
  <br>
  <br>
  <a href="https://modulabs.co.kr/" target='_blank'>
    <img src='https://image.rocketpunch.com/company/93527/modulabs-1_logo_1554094674.png?s=400x400&t=inside' style="width: 30px; height: auto;">
    &nbsp;
    <img src='https://modulabs.co.kr/wp-content/uploads/2021/04/modulabs_logo.png'>
  </a>
</p>

<h2 align="right">
  <a href="https://github.com/HRPzz/AIFFEL/tree/main/GOING_DEEPER_NLP"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2FHRPzz%2FAIFFEL%2Ftree%2Fmain%2FGOING_DEEPER_NLP&count_bg=%23000000&title_bg=%23000000&icon=github.svg&icon_color=%23E7E7E7&title=GOING_DEEPER_NLP&edge_flat=false"/></a>
  <br>
  <a href="https://www.daegu.go.kr/intro.jsp" target='_blank'>
    <img src='https://www.daegu.go.kr/cmsh/daegu.go.kr/dgcontent/images/intro2/ico03.png' style="width: 30px; height: auto;">
  </a>
  <span style="color: rgb(1,134,64)">DAEGU 1</span>
</h2>

---

## 📌 TABLE OF CONTENTS

- [🔒 LMS](#-lms)
  - [🔑 GOING DEEPER NLP](#-going-deeper-nlp)
  - [✔ GOING DEEPER NLP CONTENTS SUMMARY](#-going-deeper-nlp-contents-summary)

---

## 🔒 LMS

### 🔑 GOING DEEPER NLP

|N|Node Title|Author|Type|Link|W|Open|
|:---:|---|:---:|:---:|:---:|:---:|:---:|
|1|<b>텍스트 데이터 다루기</b><hr><i>#Tags: Distributed Representation, Tokenization, WPM, BPE, SentencePiece, Word2Vec, FastText, ELMo</i>|문성원|Lecture|[📋][NLP-01]|12|03.15|
|2|<b>멋진 단어사전 만들기</b><hr><i>#Tags: Tokenization, SentencePiece</i>|문성원|**Project**|[📋][NLP-02]|12|03.16|
|3|<b>텍스트의 분포로 벡터화 하기</b><hr><i>#Tags: BoW, DTM, TF-IDF, LSA, LDA</i>|유원준|Lecture|[📋][NLP-03]|12|03.18|
|4|<b>뉴스 카테고리 다중분류</b><hr><i>#Tags: Precision, Recall, F1 Score, Confusion Matrix, MultinomialNB, CNB</i>|유원준|**Project**|[📋][NLP-04]|13|03.21|
|5|<b>워드 임베딩</b><hr><i>#Tags: Word Embedding Vector, Word2Vec, FastText, Glove</i>|유원준|Lecture|[📋][NLP-05]|13|03.23|
|6|<b>임베딩 내 편향성 알아보기</b><hr><i>#Tags: Word Embedding, WEAT</i>|박윤진|**Project**|[📋][NLP-06]|13|03.24|
|7|<b>Seq2seq와 Attention</b><hr><i>#Tags: Seq2Seq, Bahdanau Attention, Luong Attention</i>|문성원|Lecture|[📋][NLP-07]|14|03.28|
|8|<b>Seq2seq으로 번역기 만들기</b><hr><i>#Tags: Seq2Seq, Attention</i>|문성원|**Project**|[📋][NLP-08]|14|03.29|
|9|<b>Transformer가 나오기까지</b><hr><i>#Tags: Seq2Seq, Attention, Transformer, BERT, GPT</i>|문성원|Lecture|[📋][NLP-09]|14|03.31|
|10|<b>Transformer로 번역기 만들기</b><hr><i>#Tags: Transformer</i>|문성원|**Project**|[📋][NLP-10]|15|04.04|
|11|<b>기계 번역이 걸어온 길</b><hr><i>#Tags: Greedy Decoding, Beam Search, Sampling, Data Augmentation, Lexical Substitution, Back Translation, Random Noise Injection</i>|문성원|Lecture|[📋][NLP-11]|15|04.05|
|12|<b>번역가는 대화에도 능하다</b><hr><i>#Tags: Transformer, BLEU Score, Beam Search Decoder</i>|문성원|**Project**|[📋][NLP-12]|15|04.07|
|13|<b>modern NLP의 흐름에 올라타보자</b><hr><i>#Tags: Transformer, ELMO, GPT, BERT, Transformer-XL, XLNet, BART, ALBERT, T5, Switch Transformer, ERNIE</i>|정민지|Lecture|[📋][NLP-13]|16|04.11|
|14|<b>BERT pretrained model 제작</b><hr><i>#Tags: BERT</i>|현청천|**Project**|[📋][NLP-14]|17|04.12|
|15|<b>NLP Framework의 활용</b><hr><i>#Tags: Hugging Face Transformers</i>|정민지|Lecture|[📋][NLP-15]|17|04.18|
|16|<b>HuggingFace 커스텀 프로젝트 만들기</b><hr><i>#Tags: Hugging Face Transformers</i>|정민지|**Project**|[📋][NLP-16]|17|04.19|

[NLP-01]: Node_01/README.md
[NLP-02]: Node_02/README.md
[NLP-03]: Node_03/README.md
[NLP-04]: Node_04/README.md
[NLP-05]: Node_05/README.md
[NLP-06]: Node_06/README.md
[NLP-07]: Node_07/README.md
[NLP-08]: Node_08/README.md
[NLP-09]: Node_09/README.md
[NLP-10]: Node_10/README.md
[NLP-11]: Node_11/README.md
[NLP-12]: Node_12/README.md
[NLP-13]: Node_13/README.md
[NLP-14]: Node_14/README.md
[NLP-15]: Node_15/README.md
[NLP-16]: Node_16/README.md

**[⬆ back to top](#-table-of-contents)**

## ✔ GOING DEEPER NLP CONTENTS SUMMARY

|N|Node Title|Author|Contents Summary|
|:---:|---|:---:|---|
|1|<b>텍스트 데이터 다루기</b>|문성원|텍스트 처리성능에 큰 영향을 주는 다양한 텍스트 데이터 전처리 기법을 소개한다. Word나 형태소 레벨의 tokenizer 및 Subword 레벨 tokenizing 기법(BPE, sentencepiece)을 소개하고 사용법을 익힌다.|
|2|<b>멋진 단어사전 만들기</b>|문성원|소설책 1권 분량의 텍스트를 lecture에서 다룬 다양한 기법으로 전처리하여 단어가전을 만들어 본다. 각각의 단어사전을 토대로 language model을 생성하여 perplexity를 측정해 보고 어떤 기법의 성능이 우수한지 평가해 본다.|
|3|<b>텍스트의 분포로 벡터화 하기</b>|유원준|텍스트 분포를 이용한 텍스트의 벡터화 방법들(BoW, DTM, TF-IDF, LSA, LDA)을 실습을 통해 익혀보고, 텍스트 분포 기반으로 구현된 토큰화 기법에 대해서도 살펴본다.|
|4|<b>뉴스 카테고리 다중분류</b>|유원준|Exploration stage에서 다루었던 뉴스 텍스트의 주제를 분류하는 태스크를 다양한 기법으로 다시 시도해 보고 어떤 방법이 가장 우수한 성능을 보이는지 실험해 본다.|
|5|<b>워드 임베딩</b>|유원준|워드 임베딩 벡터를 만드는 다양한 기법에 대해 알아보고 널리 사용되는 Word2Vec, FastText, Glove의 원리와 사용법을 실습을 통해 알아본다.|
|6|<b>임베딩 내 편향성 알아보기</b>|박윤진|데이터의 편향성에 대한 문제의식을 갖습니다. 편향성을 알아보기 위한 WEAT 기법을 이해하고, 워드 임베딩에 직접 적용합니다.|
|7|<b>Seq2seq와 Attention</b>|문성원|언어 모델이 발전해 온 과정에 대해 개략적으로 공부하고, NLP의 큰 흐름 중 하나인 Sequence to Sequence(Seq2seq)에 대해 공부합니다. 이를 발전시키기 위한 기법이자 지금은 없어선 안 될 중요한 메커니즘인 Attention에 대해서도 살펴볼 거고요. 부디 즐거운 시간이 되시길 바랍니다!|
|8|<b>Seq2seq으로 번역기 만들기</b>|문성원|Seq2seq 기반 번역기를 직접 만들어보며 구조를 이해해 본다. Attention 기법을 추가하여 성능을 높여볼 수 있다. 영어-스페인어 말뭉치와 한국어-영어 말뭉치를 활용해 본다.|
|9|<b>Transformer가 나오기까지</b>|문성원|Attention 복습 및 트랜스포머에 포함된 모듈을 심층적으로 이해하는 단계|
|10|<b>Transformer로 번역기 만들기</b>|문성원|트랜스포머를 이용해 번역기를 만들어 본다. 먼저 내부 모듈을 하나하나 구현한 후, 조립하여 완성한다. 완성된 번역기를 테스트해 본다.|
|11|<b>기계 번역이 걸어온 길</b>|문성원|번역 모델이 발전해 온 과정을 살펴보고, 번역을 생성하는 여러 가지 방법을 소개합니다. 자연어 처리에서 DATA Augmentation을 어떻게 할 수 있는지, 자연어 처리 성능은 어떻게 측정할 수 있는지 알아봅니다. 기계 번역과 뗄 수 없는 챗봇도 간단히 공부합니다.|
|12|<b>번역가는 대화에도 능하다</b>|문성원|다양한 디코딩 방식을 활용해 번역 모델을 직접 만들어 본 후, 해당 모델을 BLEU Score을 이용해 성능을 평가해 본다. 트랜스포머 구조를 활용해 한국어 챗봇을 직접 구현해 보는 프로젝트를 진행한다.|
|13|<b>modern NLP의 흐름에 올라타보자</b>|정민지|Transformer를 바탕으로 한 최근 NLP의 모델에 대해서 알아보겠습니다.|
|14|<b>BERT pretrained model 제작</b>|현청천|가장 대표적인 pretrainedㅣlanguage model인 BERT의 pretrain 전과정을 진행해 보면서 BERT의 핵심원리를 깊이 이해해 본다.|
|15|<b>NLP Framework의 활용</b>|정민지|최신 NLP 기술발전을 선도하는 다양한 NLP Framework에 대해 알아보고, 가장 대표적인 Huggingface transformer를 중심으로 설계구조와 활용법을 공부해 본다.|
|16|<b>HuggingFace 커스텀 프로젝트 만들기</b>|정민지|대표적인 NLP framework인 Huggingface transformers를 활용하여 자기만의 커스텀 프로젝트를 제작하는 실습을 진행해 본다.|

**[⬆ back to top](#-table-of-contents)**
