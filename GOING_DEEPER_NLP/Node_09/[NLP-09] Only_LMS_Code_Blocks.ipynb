{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Transformer가 나오기까지\n",
    "\n",
    "**어텐션(Attention) 기법을 간단히 복습한 후, 이를 활용한 트랜스포머(Transformer)에 포함된 모듈을 심층적으로 이해한다. 트랜스포머를 발전시키기 위해 적용된 테크닉도 간략히 알아본다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9-1. 들어가며"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9-2. Attention의 역사"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9-3. Attention Is All You Need!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9-4. Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table\n",
    "\n",
    "pos = 7\n",
    "d_model = 4\n",
    "i = 0\n",
    "\n",
    "print(\"Positional Encoding 값:\\n\", positional_encoding(pos, d_model))\n",
    "\n",
    "print(\"\")\n",
    "print(\"if pos == 0, i == 0: \", np.sin(0 / np.power(10000, 2 * i / d_model)))\n",
    "print(\"if pos == 1, i == 0: \", np.sin(1 / np.power(10000, 2 * i / d_model)))\n",
    "print(\"if pos == 2, i == 0: \", np.sin(2 / np.power(10000, 2 * i / d_model)))\n",
    "print(\"if pos == 3, i == 0: \", np.sin(3 / np.power(10000, 2 * i / d_model)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"if pos == 0, i == 1: \", np.cos(0 / np.power(10000, 2 * i + 1 / d_model)))\n",
    "print(\"if pos == 1, i == 1: \", np.cos(1 / np.power(10000, 2 * i + 1 / d_model)))\n",
    "print(\"if pos == 2, i == 1: \", np.cos(2 / np.power(10000, 2 * i + 1 / d_model)))\n",
    "print(\"if pos == 3, i == 1: \", np.cos(3 / np.power(10000, 2 * i + 1 / d_model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(positional_encoding(100, 300), cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9-5. Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_dot_product_tensor(shape):\n",
    "    A = tf.random.uniform(shape, minval=-3, maxval=3)\n",
    "    B = tf.transpose(tf.random.uniform(shape, minval=-3, maxval=3), [1, 0])\n",
    "\n",
    "    return tf.tensordot(A, B, axes=1)\n",
    "\n",
    "length = 30\n",
    "big_dim = 1024.\n",
    "small_dim = 10.\n",
    "\n",
    "big_tensor = make_dot_product_tensor((length, int(big_dim)))\n",
    "scaled_big_tensor = big_tensor / tf.sqrt(big_dim)\n",
    "small_tensor = make_dot_product_tensor((length, int(small_dim)))\n",
    "scaled_small_tensor = small_tensor / tf.sqrt(small_dim)\n",
    "\n",
    "fig = plt.figure(figsize=(13, 6))\n",
    "\n",
    "ax1 = fig.add_subplot(141)\n",
    "ax2 = fig.add_subplot(142)\n",
    "ax3 = fig.add_subplot(143)\n",
    "ax4 = fig.add_subplot(144)\n",
    "\n",
    "ax1.set_title('1) Big Tensor')\n",
    "ax2.set_title('2) Big Tensor(Scaled)')\n",
    "ax3.set_title('3) Small Tensor')\n",
    "ax4.set_title('4) Small Tensor(Scaled)')\n",
    "\n",
    "ax1.imshow(tf.nn.softmax(big_tensor, axis=-1).numpy(), cmap='cividis')\n",
    "ax2.imshow(tf.nn.softmax(scaled_big_tensor, axis=-1).numpy(), cmap='cividis')\n",
    "ax3.imshow(tf.nn.softmax(small_tensor, axis=-1).numpy(), cmap='cividis')\n",
    "ax4.imshow(tf.nn.softmax(scaled_small_tensor, axis=-1).numpy(), cmap='cividis')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_dot_product_tensor(shape):\n",
    "    A = tf.random.uniform(shape, minval=-3, maxval=3)\n",
    "    B = tf.transpose(tf.random.uniform(shape, minval=-3, maxval=3), [1, 0])\n",
    "\n",
    "    return tf.tensordot(A, B, axes=1)\n",
    "\n",
    "def generate_causality_mask(seq_len):\n",
    "    mask = 1 - np.cumsum(np.eye(seq_len, seq_len), 0)\n",
    "    return mask\n",
    "\n",
    "sample_tensor = make_dot_product_tensor((20, 512))\n",
    "sample_tensor = sample_tensor / tf.sqrt(512.)\n",
    "\n",
    "mask = generate_causality_mask(sample_tensor.shape[0])\n",
    "\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "ax1.set_title('1) Causality Mask')\n",
    "ax2.set_title('2) Masked Attention')\n",
    "\n",
    "ax1.imshow((tf.ones(sample_tensor.shape) + mask).numpy(), cmap='cividis')\n",
    "\n",
    "mask *= -1e9\n",
    "ax2.imshow(tf.nn.softmax(sample_tensor + mask, axis=-1).numpy(), cmap='cividis')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9-6. Position-wise Feed-Forward Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9-7. Additional Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "d_model = 512\n",
    "warmup_steps = 4000\n",
    "\n",
    "lrates = []\n",
    "for step_num in range(1, 50000):\n",
    "    lrate = (np.power(d_model, -0.5)) * np.min(\n",
    "        [np.power(step_num, -0.5), step_num * np.power(warmup_steps, -1.5)])\n",
    "    lrates.append(lrate)\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(lrates)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9-8. 트랜스포머, 그 후엔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9-9. 마무리"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ce9abe337a9e694d01ea52d504102083454ad8bd4b0e3a574e4432f4229329"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('aiffel_3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
