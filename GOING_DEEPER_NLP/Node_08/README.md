# 8. Seq2seq으로 번역기 만들기

**Seq2seq 기반 번역기를 직접 만들어보며 구조를 이해해 본다. Attention 기법을 추가하여 성능을 높여볼 수 있다. 영어-스페인어 말뭉치와 한국어-영어 말뭉치를 활용해 본다.**

---

|-|목차|⏲ 240분|
|:---:|---|:---:|
|7-1| 들어가며 | 5분|
|7-2| 우리가 만드는 언어 모델 | 30분|
|7-3| Sequence to Sequence 문제 | 40분|
|7-4| Sequence to Sequence 구현 | 40분|
|7-5| Attention! (1) Bahdanau Attention | 60분|
|7-6| Attention! (2) Luong Attention | 30분|
|7-7| 트랜스포머로 가기 전 징검다리? | 30분|
|7-8| 마무리 | 5분|

---

## 8-7. 프로젝트: 한영 번역기 만들기

- Step 1. 데이터 다운로드
- Step 2. 데이터 정제
- Step 3. 데이터 토큰화
- Step 4. 모델 설계
- Step 5. 훈련하기

---

>## **루브릭**
>
>|번호|평가문항|상세기준|
>|:---:|---|---|
>|1|번역기 모델 학습에 필요한 텍스트 데이터 전처리가 한국어 포함하여 잘 이루어졌다.|구두점, 대소문자, 띄어쓰기, 한글 형태소분석 등 번역기 모델에 요구되는 전처리가 정상적으로 진행되었다.|
>|2|Attentional Seq2seq 모델이 정상적으로 구동된다.|seq2seq 모델 훈련 과정에서 training loss가 안정적으로 떨어지면서 학습이 진행됨이 확인되었다.|
>|3|테스트 결과 의미가 통하는 수준의 번역문이 생성되었다.|테스트용 디코더 모델이 정상적으로 만들어져서, 정답과 어느 정도 유사한 영어 번역이 진행됨을 확인하였다.|
