{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 나를 찾아줘 - Class Activation Map 만들기\n",
    "\n",
    "**CAM, Grad-CAM을 위한 모델을 직접 만들고, CAM을 추출해 시각화 해본다. CAM을 Object detection에 적용해 결과를 평가해 본다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-1. 들어가며"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-2. CAM, Grad-CAM용 모델 준비하기 (1) 데이터셋 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import copy\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최초 수행시에는 다운로드가 진행됩니다. 오래 걸릴 수 있으니 유의해 주세요.  \n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'stanford_dogs',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    with_info=True,\n",
    ")\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds.show_examples(ds_train, ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds.show_examples(ds_test, ds_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-3. CAM, Grad-CAM용 모델 준비하기 (2) 물체의 위치정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_info.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-4. CAM, Grad-CAM용 모델 준비하기 (3) CAM을 위한 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = ds_info.features[\"label\"].num_classes\n",
    "base_model = keras.applications.resnet50.ResNet50(\n",
    "    include_top=False,    # Imagenet 분류기  fully connected layer 제거\n",
    "    weights='imagenet',\n",
    "    input_shape=(224, 224, 3),\n",
    "    pooling='avg',      # GAP를 적용  \n",
    ")\n",
    "x = base_model.output\n",
    "preds = keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "cam_model = keras.Model(inputs=base_model.input, outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-5. CAM, Grad-CAM용 모델 준비하기 (4) CAM 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_resize_img(input):\n",
    "    # Normalizes images: `uint8` -> `float32`\n",
    "    image = tf.image.resize(input['image'], [224, 224])\n",
    "    input['image'] = tf.cast(image, tf.float32) / 255.\n",
    "    return input['image'], input['label']\n",
    "\n",
    "def apply_normalize_on_dataset(ds, is_test=False, batch_size=16):\n",
    "    ds = ds.map(\n",
    "        normalize_and_resize_img, \n",
    "        num_parallel_calls=2\n",
    "    )\n",
    "    ds = ds.batch(batch_size)\n",
    "    if not is_test:\n",
    "        ds = ds.repeat()\n",
    "        ds = ds.shuffle(200)\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋에 전처리와 배치처리를 적용합니다. \n",
    "ds_train_norm = apply_normalize_on_dataset(ds_train)\n",
    "ds_test_norm = apply_normalize_on_dataset(ds_test)\n",
    "\n",
    "# 구성된 배치의 모양을 확인해 봅니다. \n",
    "for input in ds_train_norm.take(1):\n",
    "    image, label = input\n",
    "    print(image.shape)\n",
    "    print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.SGD(lr=0.01),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cam_model = cam_model.fit(\n",
    "    ds_train_norm,\n",
    "    steps_per_epoch=int(ds_info.splits['train'].num_examples/16),\n",
    "    validation_steps=int(ds_info.splits['test'].num_examples/16),\n",
    "    epochs=2,\n",
    "    validation_data=ds_test_norm,\n",
    "    verbose=1,\n",
    "    use_multiprocessing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cam_model_path = os.getenv('HOME')+'/aiffel/class_activation_map/cam_model1.h5'\n",
    "cam_model.save(cam_model_path)\n",
    "print(\"저장 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-6. CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커널 재시작 이후 실습을 위해, 이전 스텝의 코드를 모아서 한꺼번에 실행합니다.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import copy\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'stanford_dogs',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "def normalize_and_resize_img(input):\n",
    "    # Normalizes images: `uint8` -> `float32`\n",
    "    image = tf.image.resize(input['image'], [224, 224])\n",
    "    input['image'] = tf.cast(image, tf.float32) / 255.\n",
    "    return input['image'], input['label']\n",
    "\n",
    "def apply_normalize_on_dataset(ds, is_test=False, batch_size=16):\n",
    "    ds = ds.map(\n",
    "        normalize_and_resize_img, \n",
    "        num_parallel_calls=2\n",
    "    )\n",
    "    ds = ds.batch(batch_size)\n",
    "    if not is_test:\n",
    "        ds = ds.repeat()\n",
    "        ds = ds.shuffle(200)\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one(ds):\n",
    "    ds = ds.take(1)\n",
    "    sample_data = list(ds.as_numpy_iterator())\n",
    "    bbox = sample_data[0]['objects']['bbox']\n",
    "    image = sample_data[0]['image']\n",
    "    label = sample_data[0]['label']\n",
    "    return sample_data[0]\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = get_one(ds_test)\n",
    "print(item['label'])\n",
    "plt.imshow(item['image'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cam_model_path = os.getenv('HOME')+'/aiffel/class_activation_map/cam_model.h5'\n",
    "cam_model = tf.keras.models.load_model(cam_model_path)\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cam(model, item):\n",
    "    item = copy.deepcopy(item)\n",
    "    width = item['image'].shape[1]\n",
    "    height = item['image'].shape[0]\n",
    "    \n",
    "    img_tensor, class_idx = normalize_and_resize_img(item)\n",
    "    \n",
    "    # 학습한 모델에서 원하는 Layer의 output을 얻기 위해서 모델의 input과 output을 새롭게 정의해줍니다.\n",
    "    # model.layers[-3].output에서는 우리가 필요로 하는 GAP 이전 Convolution layer의 output을 얻을 수 있습니다.\n",
    "    cam_model = tf.keras.models.Model([model.inputs], [model.layers[-3].output, model.output])\n",
    "    conv_outputs, predictions = cam_model(tf.expand_dims(img_tensor, 0))\n",
    "    \n",
    "    conv_outputs = conv_outputs[0, :, :, :]\n",
    "    class_weights = model.layers[-1].get_weights()[0] #마지막 모델의 weight activation을 가져옵니다.\n",
    "    \n",
    "    cam_image = np.zeros(dtype=np.float32, shape=conv_outputs.shape[0:2])\n",
    "    for i, w in enumerate(class_weights[:, class_idx]):\n",
    "        # W * f 를 통해 class별 activation map을 계산합니다.\n",
    "        cam_image += w * conv_outputs[:, :, i]\n",
    "\n",
    "    cam_image /= np.max(cam_image) # activation score를 normalize합니다.\n",
    "    cam_image = cam_image.numpy()\n",
    "    cam_image = cv2.resize(cam_image, (width, height)) # 원래 이미지의 크기로 resize합니다.\n",
    "    return cam_image\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_image = generate_cam(cam_model, item)\n",
    "plt.imshow(cam_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cam_on_image(src1, src2, alpha=0.5):\n",
    "    beta = (1.0 - alpha)\n",
    "    merged_image = cv2.addWeighted(src1, alpha, src2, beta, 0.0)\n",
    "    return merged_image\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_image = item['image'].astype(np.uint8)\n",
    "cam_image_3channel = np.stack([cam_image*255]*3, axis=-1).astype(np.uint8)\n",
    "\n",
    "blended_image = visualize_cam_on_image(cam_image_3channel, origin_image)\n",
    "plt.imshow(blended_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-7. Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = get_one(ds_test)\n",
    "print(item['label'])\n",
    "plt.imshow(item['image'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grad_cam(model, activation_layer, item):\n",
    "    item = copy.deepcopy(item)\n",
    "    width = item['image'].shape[1]\n",
    "    height = item['image'].shape[0]\n",
    "    img_tensor, class_idx = normalize_and_resize_img(item)\n",
    "    \n",
    "    # Grad cam에서도 cam과 같이 특정 레이어의 output을 필요로 하므로 모델의 input과 output을 새롭게 정의합니다.\n",
    "    # 이때 원하는 레이어가 다를 수 있으니 해당 레이어의 이름으로 찾은 후 output으로 추가합니다.\n",
    "    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(activation_layer).output, model.output])\n",
    "    \n",
    "    # Gradient를 얻기 위해 tape를 사용합니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_output, pred = grad_model(tf.expand_dims(img_tensor, 0))\n",
    "    \n",
    "        loss = pred[:, class_idx] # 원하는 class(여기서는 정답으로 활용) 예측값을 얻습니다.\n",
    "        output = conv_output[0] # 원하는 layer의 output을 얻습니다.\n",
    "        grad_val = tape.gradient(loss, conv_output)[0] # 예측값에 따른 Layer의 gradient를 얻습니다.\n",
    "\n",
    "    weights = np.mean(grad_val, axis=(0, 1)) # gradient의 GAP으로 class별 weight를 구합니다.\n",
    "    grad_cam_image = np.zeros(dtype=np.float32, shape=conv_output.shape[0:2])\n",
    "    for k, w in enumerate(weights):\n",
    "        # 각 class별 weight와 해당 layer의 output을 곱해 class activation map을 얻습니다.\n",
    "        grad_cam_image += w * output[:, :, k]\n",
    "        \n",
    "    grad_cam_image /= np.max(grad_cam_image)\n",
    "    grad_cam_image = grad_cam_image.numpy()\n",
    "    grad_cam_image = cv2.resize(grad_cam_image, (width, height))\n",
    "    return grad_cam_image\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_image = generate_grad_cam(cam_model, 'conv5_block3_out', item)\n",
    "plt.imshow(grad_cam_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_image = generate_grad_cam(cam_model, 'conv4_block3_out', item)\n",
    "plt.imshow(grad_cam_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_image = generate_grad_cam(cam_model, 'conv3_block3_out', item)\n",
    "plt.imshow(grad_cam_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-8. Detection with CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = get_one(ds_test)\n",
    "print(item['label'])\n",
    "plt.imshow(item['image'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_image = generate_cam(cam_model, item)\n",
    "plt.imshow(cam_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox(cam_image, score_thresh=0.05):\n",
    "    low_indicies = cam_image <= score_thresh\n",
    "    cam_image[low_indicies] = 0\n",
    "    cam_image = (cam_image*255).astype(np.uint8)\n",
    "    \n",
    "    contours,_ = cv2.findContours(cam_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnt = contours[0]\n",
    "    rotated_rect = cv2.minAreaRect(cnt)\n",
    "    rect = cv2.boxPoints(rotated_rect)\n",
    "    rect = np.int0(rect)\n",
    "    return rect\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rect = get_bbox(cam_image)\n",
    "rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = copy.deepcopy(item['image'])\n",
    "image = cv2.drawContours(image, [rect], 0, (0,0,255), 2)\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rect의 좌표는 (x, y) 형태로, bbox는 (y_min, x_min, y_max, x_max)의 normalized 형태로 주어집니다. \n",
    "def rect_to_minmax(rect, image):\n",
    "    bbox = [\n",
    "        rect[:,1].min()/float(image.shape[0]),  #bounding box의 y_min\n",
    "        rect[:,0].min()/float(image.shape[1]),  #bounding box의 x_min\n",
    "        rect[:,1].max()/float(image.shape[0]), #bounding box의 y_max\n",
    "        rect[:,0].max()/float(image.shape[1]) #bounding box의 x_max\n",
    "    ]\n",
    "    return bbox\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bbox = rect_to_minmax(rect, item['image'])\n",
    "pred_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item['objects']['bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou(boxA, boxB):\n",
    "    y_min = max(boxA[0], boxB[0])\n",
    "    x_min= max(boxA[1], boxB[1])\n",
    "    y_max = min(boxA[2], boxB[2])\n",
    "    x_max = min(boxA[3], boxB[3])\n",
    "    \n",
    "    interArea = max(0, x_max - x_min) * max(0, y_max - y_min)\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "    return iou\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_iou(pred_bbox, item['objects']['bbox'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-9. 프로젝트: CAM을 만들고 평가해 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import PIL\n",
    "\n",
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "print(cv2.__version__)\n",
    "print(PIL.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def generate_cam(model, item):\n",
    "    cam_image = None\n",
    "    # TODO: generate cam image\n",
    "    return cam_image\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "cam_image = generate_cam(cam_model, item)\n",
    "plt.imshow(cam_image)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def visualize_cam_on_image(image, cam_image):\n",
    "    # TODO: blend image\n",
    "    merged_image = None\n",
    "    return merged_image\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# visualize_cam_on_image() 함수 사용\n",
    "\n",
    "type(item['image'].numpy())\n",
    "\n",
    "origin_image = item['image'].numpy().astype(np.uint8)\n",
    "cam_image_3channel = np.stack([cam_image*255]*3, axis=-1).astype(np.uint8)\n",
    "\n",
    "blended_image = visualize_cam_on_image(cam_image_3channel, origin_image)\n",
    "plt.imshow(blended_image)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def generate_grad_cam(model, activation_layer, item):\n",
    "    grad_cam_image = None\n",
    "    # TODO: generate grad_cam_image\n",
    "\n",
    "    return grad_cam_image\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def get_iou(gt_bbox, pred_bbox):\n",
    "    iou = None\n",
    "    # TODO: get iou between two bbox\n",
    "    return iou\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">## **루브릭**\n",
    ">\n",
    ">|번호|평가문항|상세기준|\n",
    ">|:---:|---|---|\n",
    ">|1|CAM을 얻기 위한 기본모델의 구성과 학습이 정상 진행되었는가?|ResNet50 + GAP + DenseLayer 결합된 CAM 모델의 학습과정이 안정적으로 수렴하였다.|\n",
    ">|2|분류근거를 설명 가능한 Class activation map을 얻을 수 있는가?|CAM 방식과 Grad-CAM 방식의 class activation map이 정상적으로 얻어지며, 시각화하였을 때 해당 object의 주요 특징 위치를 잘 반영한다.|\n",
    ">|3|인식결과의 시각화 및 성능 분석을 적절히 수행하였는가?|CAM과 Grad-CAM 각각에 대해 원본이미지합성, 바운딩박스, IoU 계산 과정을 통해 CAM과 Grad-CAM의 object localization 성능이 비교분석되었다.|"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ce9abe337a9e694d01ea52d504102083454ad8bd4b0e3a574e4432f4229329"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('aiffel_3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
