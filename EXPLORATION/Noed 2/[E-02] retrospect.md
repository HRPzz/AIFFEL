# 회고

- ## 이번 프로젝트에서 **어려웠던 점**.
    모델 성능 평가를 할 때 어떤 지표를 선택해야 좋을지 선택하는 것이 어려웠다.

- ## 프로젝트를 진행하면서 **알아낸 점** 혹은 **아직 모호한 점**.
  1. 모델 성능을 평가할 때 accuracy, precision, recall, f1 score를 지표로 쓸 수 있다.
  2. accuracy 만으로 평가하기에는 한계가 있다. label이 불균형하게 이루어져 있을 경우 예측이 제대로 이루어지지 않는다.
  3. 그러므로 오차 행렬의 성능 지표(precision, recall, f1 score) 중 f1 score 를 통해 성능을 평가할 수 있다.
  4. 대부분 accuracy 와 f1 score 를 통해 성능을 평가한다고 생각하면 될 듯 하다.

- ## 루브릭 평가 지표를 맞추기 위해 **시도한 것들**.

>### **루브릭**
>|평가문항|상세기준|
>|:---:|:---:|
>|1. 3가지 데이터셋의 구성이 합리적으로 진행되었는가?|feature와 label 선정을 위한 데이터 분석과정이 체계적으로 전개됨|
>|2. 3가지 데이터셋에 대해 각각 5가지 모델을 성공적으로 적용하였는가?|모델학습 및 테스트가 정상적으로 수행되었음|
>|3. 3가지 데이터셋에 대해 모델의 평가지표가 적절히 선택되었는가?|평가지표 선택 및 이유 설명이 타당함|

루브릭 평가 지표 중에서 1번과 2번은 사실상 lms 코드를 보면서 복사 붙여넣기하면 해결되는 부분이라 어렵지 않았다. 3번이 어려웠던 이유로 오차 행렬의 성능 지표 중에서 f1 score를 도대체 어느 상황에서 쓰느냐에 대한 것이었다. FP와 FN 중에서 어느 것이 더 값이 작아야 할까 고민하면 precision인지 recall인지 고를 수 있었지만 f1 score(recall과 precision의 조화 평균)는 도대체 언제 쓰이는 것인지 확실하게 알지 못했다.

구글링하고 lms를 다시 읽어본 결과, f1 score는 label이 불균형하게 이루어져 있을 경우에 쓴다고 결론 내렸다. 그래서 데이터셋의 클래스별로 샘플이 총 몇 개씩 분포해있는지 확인할 필요가 있었고 균형적인지 비율을 따져서 성능 지표를 결정했다.

- ## 만약에 루브릭 평가 관련 지표를 **달성 하지 못했을 때, 이유에 관한 추정**.
    없음

- ## **자기 다짐**
    머신러닝 모델이 어떤 방식으로 동작하는가, 어떤 상황에서 어떤 모델을 사용하는 것이 좋은가, 모델 성능 평가 지표로 무엇을 사용해야 좋을 것인가, 모델 성능 평가 지표로 어떤 것들이 있는가 등 아직 알아야 할 부분이 많다고 느껴졌다.

