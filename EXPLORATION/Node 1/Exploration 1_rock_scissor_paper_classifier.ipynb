{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 인공지능과 가위바위보 하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 미니 프로젝트 : 가위바위보 분류기를 만들자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러오기 + Resize 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "from PIL import Image\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "100  images to be resized.\n",
      "100  images resized.\n",
      "100  images to be resized.\n",
      "100  images resized.\n"
     ]
    }
   ],
   "source": [
    "# 이미지 resize\n",
    "def resize_images(img_path):\n",
    "\timages=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "\tprint(len(images), \" images to be resized.\")\n",
    "\n",
    "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "\ttarget_size=(28,28)\n",
    "\tfor img in images:\n",
    "\t\told_img=Image.open(img)\n",
    "\t\tnew_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "\t\tnew_img.save(img, \"JPEG\")\n",
    "    \n",
    "\tprint(len(images), \" images resized.\")\n",
    "\t\n",
    "# 가위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = \"rock_scissor_paper/scissor\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "# 바위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = \"rock_scissor_paper/rock\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "# 보 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = \"rock_scissor_paper/paper\"\n",
    "resize_images(image_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터의 이미지 개수는 300 입니다.\n",
      "x_train shape: (300, 28, 28, 3)\n",
      "y_train shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "# 라벨링\n",
    "def load_data(img_path, number_of_data=300):  # 가위바위보 이미지 개수 총합에 주의하세요.\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"데이터의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = \"rock_scissor_paper\"\n",
    "(x_train, y_train)=load_data(image_dir_path)\n",
    "x_train_norm = x_train/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 참고. 이미지 불러와서 확인\n",
    "plt.imshow(x_train[0])\n",
    "print('라벨: ', y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝 네트워크 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model에 추가된 Layer 개수:  7\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_28 (Conv2D)          (None, 26, 26, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_28 (MaxPoolin  (None, 13, 13, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_29 (Conv2D)          (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_29 (MaxPoolin  (None, 5, 5, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_14 (Flatten)        (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 64)                102464    \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122,051\n",
      "Trainable params: 122,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# tf.keras의 Sequential API를 이용하여 LeNet이라는 딥러닝 네트워크를 설계\n",
    "n_channel_1=32\n",
    "n_channel_2=64\n",
    "n_dense=64\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', input_shape=(28,28,3)))  # 독립 변수 컬럼 2352\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(n_dense, activation='relu'))  # 히든 레이어\n",
    "\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))  # 종속 변수 컬럼 3\n",
    "\n",
    "print('Model에 추가된 Layer 개수: ', len(model.layers))\n",
    "\n",
    "# 만든 딥러닝 네트워크 모델 확인\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝 네트워크 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 1.0678 - accuracy: 0.4375 - val_loss: 1.5881 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1.0327 - accuracy: 0.3875 - val_loss: 1.8619 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1.0269 - accuracy: 0.4125 - val_loss: 1.5850 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1.0305 - accuracy: 0.4667 - val_loss: 1.9410 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1.0408 - accuracy: 0.4583 - val_loss: 1.7204 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/20\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1.0400 - accuracy: 0.4250 - val_loss: 1.5709 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/20\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 1.0072 - accuracy: 0.4958 - val_loss: 1.8874 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/20\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.9943 - accuracy: 0.4542 - val_loss: 1.6665 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/20\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.9592 - accuracy: 0.5917 - val_loss: 1.6915 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/20\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.9156 - accuracy: 0.6500 - val_loss: 1.3303 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/20\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.8310 - accuracy: 0.6708 - val_loss: 1.6301 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/20\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.7347 - accuracy: 0.7250 - val_loss: 1.6046 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/20\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.6825 - accuracy: 0.7292 - val_loss: 2.1022 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/20\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.6508 - accuracy: 0.7542 - val_loss: 1.4540 - val_accuracy: 0.0667\n",
      "Epoch 15/20\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.5466 - accuracy: 0.7708 - val_loss: 1.9440 - val_accuracy: 0.0500\n",
      "Epoch 16/20\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.5099 - accuracy: 0.8125 - val_loss: 2.0877 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/20\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.5224 - accuracy: 0.7833 - val_loss: 0.9540 - val_accuracy: 0.3000\n",
      "Epoch 18/20\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.4468 - accuracy: 0.8292 - val_loss: 0.7607 - val_accuracy: 0.5833\n",
      "Epoch 19/20\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.4263 - accuracy: 0.8458 - val_loss: 0.5204 - val_accuracy: 0.8500\n",
      "Epoch 20/20\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.4169 - accuracy: 0.8625 - val_loss: 1.3630 - val_accuracy: 0.1167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a332657b80>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_reshaped=x_train_norm.reshape(-1, 28, 28, 3)  # 데이터갯수에 -1을 쓰면 reshape시 자동계산됩니다.\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# 학습(Fit)\n",
    "n_train_epoch = 20\n",
    "model.fit(x_train_reshaped, y_train, epochs=n_train_epoch, validation_split=0.2, batch_size=10)  # epochs=n # n번 반복해서 학습시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "100  images to be resized.\n",
      "100  images resized.\n",
      "100  images to be resized.\n",
      "100  images resized.\n",
      "데이터의 이미지 개수는 300 입니다.\n",
      "x_test shape: (300, 28, 28, 3)\n",
      "y_test shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "# x_test, y_test 생성\n",
    "image_dir_path = \"rock_scissor_paper/test/scissor\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "image_dir_path = \"rock_scissor_paper/test/rock\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "image_dir_path = \"rock_scissor_paper/test/paper\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "image_dir_path = \"rock_scissor_paper/test\"  # \"rock_scissor_paper/test-seona\"\n",
    "(x_test, y_test)=load_data(image_dir_path)  # number_of_data of test-seona: 320\n",
    "x_test_norm = x_test/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_test shape: {}\".format(x_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - loss: 2.9591 - accuracy: 0.3600 - 114ms/epoch - 11ms/step\n",
      "test_loss: 2.9591150283813477 \n",
      "test_accuracy: 0.36000001430511475\n"
     ]
    }
   ],
   "source": [
    "x_test_reshaped=x_test_norm.reshape(-1, 28, 28, 3)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped, y_test, verbose=2)\n",
    "\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lms에서 내용이 충분하지 않아서 어려웠다. 그래서 Opentutorials.org 사이트에서 Tensorflow 101, Tensorflow 102 유튜브 강의를 들었더니 도움이 되었다. 모델을 설계할 때 쓰이는 함수들이 각각 어떤 역할인지 몰라서 힘들었다. 공부가 많이 필요하다고 느껴졌다.\n",
    "\n",
    "루브릭 평가 지표를 달성하기 위해 인식률을 60% 넘기려고 해봤지만 실패했다. 딥러닝 네트워크를 설계할 때 기존의 lms 코드 중에서 n_channel 또는 n_epoch 값을 변경하면 인식률을 높일 수 있다고 해서 이리저리 값을 변경해보았다. 별로 효과는 없었다. 이번에는 히든 레이어를 추가해보았다. 여전히 인식률은 낮았다. 그 다음 취해본 방식으로는 model.fit() 함수 안에 인자값으로 validation_split=0.2와 batch_size=10를 설정해보았다. 똑같이 인식률은 0.3으로 처참했다.\n",
    "\n",
    "클라우드에서 제공하는 기본 데이터로만 가위바위보 분류기를 만들고 테스트해서 그런 것으로 결론이 나왔다. 데이터의 양이 적고 다양하지 않아서 무슨 짓을 하더라도 인식률이 낮을 수 밖에 없다는 생각이 들었다. 인식률을 높이기 위해 데이터 샘플이 많아야 한다는 것을 알 수 있었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
