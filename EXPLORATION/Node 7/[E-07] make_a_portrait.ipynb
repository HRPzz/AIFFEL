{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 인물사진을 만들어 보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-8. 프로젝트: 인물 모드 문제점 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목차\n",
    "\n",
    "### Step 1. 인물모드 직접 해 보기\n",
    "\n",
    "### Step 2. 사진에서 문제점 찾기\n",
    "\n",
    "### Step 3. 해결 방법을 제안해 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pixellib.semantic import semantic_segmentation\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DeepLab 모델 불러온 후 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = 'human_segmentation/models'\n",
    "MODEL_FILE = os.path.join(MODEL_DIR, 'deeplabv3_xception_tf_dim_ordering_tf_kernels.h5')\n",
    "\n",
    "if not os.path.exists(MODEL_FILE):\n",
    "    # PixelLib가 제공하는 모델의 url입니다\n",
    "    MODEL_URL = 'https://github.com/ayoolaolafenwa/PixelLib/releases/download/1.1/deeplabv3_xception_tf_dim_ordering_tf_kernels.h5'\n",
    "\n",
    "    # 다운로드를 시작합니다\n",
    "    urllib.request.urlretrieve(MODEL_URL, MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세그멘테이션 모델 생성\n",
    "MODEL = semantic_segmentation()\n",
    "MODEL.load_pascalvoc_model(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PASCAL VOC 데이터 라벨 종류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_NAMES = [\n",
    "    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    "    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tv'\n",
    "]\n",
    "len(LABEL_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. 인물모드 직접 해 보기\n",
    "\n",
    "- 여러분의 셀카를 이용해서 오늘 배운 내용을 수행해 봅시다. 아래와 같은 이미지를 얻어야 합니다. 최소 3장 이상의 인물모드 사진을 만들어 봅시다.\n",
    "- 인물이 주인공이 아닌, 귀여운 고양이에 대한 아웃포커싱 사진도 만들어 볼 수 있을 것입니다. 시맨틱 세그멘테이션 스텝에서 힌트를 찾아봅시다.\n",
    "- 배경을 blur하는 인물모드 사진이 아니라 배경사진을 다른 이미지로 교체하는 크로마키 배경 합성을 시도해 볼 수도 있을 것입니다. 여러분만의 환상적인 사진을 만들어 보면 어떨까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_colormap(label_name):\n",
    "    # 아래 코드를 이해하지 않아도 좋습니다\n",
    "    # PixelLib에서 그대로 가져온 코드입니다\n",
    "    # 주목해야 할 것은 생상 코드 결과물이예요!\n",
    "    colormap = np.zeros((256, 3), dtype = int)\n",
    "    ind = np.arange(256, dtype=int)\n",
    "\n",
    "    for shift in reversed(range(8)):\n",
    "        for channel in range(3):\n",
    "            colormap[:, channel] |= ((ind >> channel) & 1) << shift\n",
    "        ind >>= 3\n",
    "\n",
    "    # colormap[:20]\n",
    "\n",
    "    index = LABEL_NAMES.index(label_name)  # 데이터 라벨 인덱스\n",
    "    seg_color = colormap[index].tolist()  # BGR\n",
    "    seg_color = seg_color[::-1]  # BGR2RGB\n",
    "    return seg_color  # RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_img(img, img_title='', cmap=None):\n",
    "    print(img.shape)\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.imshow(img, cmap)\n",
    "    plt.title(img_title)\n",
    "    plt.rc('axes', titlesize=20)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_imgs(img_1, img_2, img_1_title='', img_2_title=''):\n",
    "    plt.figure(figsize=(20,20))\n",
    "\n",
    "    plt.subplot(211)\n",
    "    plt.imshow(img_1)\n",
    "    plt.title(img_1_title)\n",
    "    plt.rc('axes', titlesize=20)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.imshow(img_2)\n",
    "    plt.title(img_2_title)\n",
    "    plt.rc('axes', titlesize=20)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 인물 사진"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이미지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSON_IMG_PATH = 'human_segmentation/images/couple.jpg'\n",
    "PERSON_IMG_ORIG = cv2.imread(PERSON_IMG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_img(cv2.cvtColor(PERSON_IMG_ORIG, cv2.COLOR_BGR2RGB), 'PERSON_IMG_ORIG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DeepLab 모델에 이미지 입력 후 출력 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 이미지 입력\n",
    "PERSON_SEGVALUES, PERSON_OUTPUT = MODEL.segmentAsPascalvoc(PERSON_IMG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영역 검출된 클래스 출력\n",
    "for class_id in PERSON_SEGVALUES['class_ids']:\n",
    "    print(LABEL_NAMES[class_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 출력값 확인\n",
    "draw_img(PERSON_OUTPUT, 'PERSON_OUTPUT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- seg_color로만 이루어진 마스크 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output의 픽셀 별로 색상이 seg_color와 같다면 1(True), 다르다면 0(False)이 됩니다\n",
    "seg_color = get_colormap('person')\n",
    "person_seg_map = np.all(PERSON_OUTPUT==seg_color, axis=-1)\n",
    "\n",
    "draw_img(person_seg_map, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 원본 이미지와 겹쳐서 세그멘테이션 적용 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_img_show = PERSON_IMG_ORIG.copy()\n",
    "\n",
    "# True과 False인 값을 각각 255과 0으로 바꿔줍니다\n",
    "person_img_mask = person_seg_map.astype(np.uint8) * 255\n",
    "\n",
    "# 255와 0을 적당한 색상으로 바꿔봅니다\n",
    "person_color_mask = cv2.applyColorMap(person_img_mask, cv2.COLORMAP_JET)\n",
    "\n",
    "# 원본 이미지와 마스트를 적당히 합쳐봅니다\n",
    "# 0.6과 0.4는 두 이미지를 섞는 비율입니다.\n",
    "person_img_show = cv2.addWeighted(person_img_show, 0.6, person_color_mask, 0.4, 0.0)\n",
    "\n",
    "draw_img(cv2.cvtColor(person_img_show, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이미지 흐리게 만들기\n",
    "  - cv2.blur(img, blurring_kernel_size)\n",
    "    - OpenCV의 blur 함수는 평균 블러링(Averaging Blurring)이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blurring kernel size = (17,17)\n",
    "person_img_orig_blur = cv2.blur(PERSON_IMG_ORIG, (17,17))\n",
    "\n",
    "draw_img(cv2.cvtColor(person_img_orig_blur, cv2.COLOR_BGR2RGB), 'person_img_orig_blur')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 흐려진 이미지에서 세그멘테이션 마스크를 이용해서 배경만 추출\n",
    "  - 색이 없으면 검게 보이고(0) 색이 있으면 흰색(1)으로 표현됨\n",
    "    - cv2.bitwise_not(img): 색 반전 => 0은 1로, 1은 0으로 변경\n",
    "    - cv2.bitwise_and(img_1, img_2): 모두 흰색(1)인 부분만 1로 나타남"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-not 연산\n",
    "\n",
    "|A|not A|\n",
    "|:---:|:---:|\n",
    "|1|0|\n",
    "|0|1|\n",
    "\n",
    "-and 연산\n",
    "\n",
    "|A|B|A and B|\n",
    "|:---:|:---:|:---:|\n",
    "|0|0|0|\n",
    "|1|0|0|\n",
    "|0|1|0|\n",
    "|1|1|1|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_img_mask_color = cv2.cvtColor(person_img_mask, cv2.COLOR_GRAY2BGR)\n",
    "person_img_bg_mask = cv2.bitwise_not(person_img_mask_color)\n",
    "person_img_bg_blur = cv2.bitwise_and(person_img_orig_blur, person_img_bg_mask)\n",
    "\n",
    "draw_img(cv2.cvtColor(person_img_bg_blur, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 배경 이미지와 사람 이미지 합치기\n",
    "  - numpy.where(condition,[x,y])\n",
    "    - 입력 조건을 충족하는 배열의 색인을 생성\n",
    "    - 조건이 True이면 출력에 x의 요소가 포함되고, 그렇지 않으면 출력에 y의 요소가 포함된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_img_concat = np.where(person_img_mask_color==255, PERSON_IMG_ORIG, person_img_bg_blur)\n",
    "\n",
    "draw_img(cv2.cvtColor(person_img_concat, cv2.COLOR_BGR2RGB), 'person_img_concat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이미지 처리 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cf. plt.savefig() 는 plt.show() 직전에 해야 처리가 끝난 이미지 저장이 제대로 된다.\n",
    "# plt.savefig('image.png', bbox_inches='tight',pad_inches = 0) 로 저장했더니 공백 및 테두리 숨기기가 제대로 안 됨 => plt.imsave('image.png', img)를 사용함\n",
    "plt.imsave('result_imgs/person_blur.png', cv2.cvtColor(person_img_concat, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- blur 적용 전후 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_imgs(cv2.cvtColor(PERSON_IMG_ORIG, cv2.COLOR_BGR2RGB), cv2.cvtColor(person_img_concat, cv2.COLOR_BGR2RGB), 'PERSON_IMG_ORIG', 'person_img_concat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 고양이 사진"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이미지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT_IMG_PATH = 'human_segmentation/images/cat_1280x853.jpg'  \n",
    "CAT_IMG_ORIG = cv2.imread(CAT_IMG_PATH) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_img(cv2.cvtColor(CAT_IMG_ORIG, cv2.COLOR_BGR2RGB), 'CAT_IMG_ORIG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DeepLab 모델에 이미지 입력 후 출력 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 이미지 입력\n",
    "CAT_SEGVALUES, CAT_OUTPUT = MODEL.segmentAsPascalvoc(CAT_IMG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영역 검출된 클래스 출력\n",
    "for class_id in CAT_SEGVALUES['class_ids']:\n",
    "    print(LABEL_NAMES[class_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 출력값 확인\n",
    "draw_img(CAT_OUTPUT, 'CAT_OUTPUT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- seg_color로만 이루어진 마스크 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output의 픽셀 별로 색상이 seg_color와 같다면 1(True), 다르다면 0(False)이 됩니다\n",
    "seg_color = get_colormap('cat')\n",
    "cat_seg_map = np.all(CAT_OUTPUT==seg_color, axis=-1)\n",
    "\n",
    "draw_img(cat_seg_map, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 원본 이미지와 겹쳐서 세그멘테이션 적용 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_img_show = CAT_IMG_ORIG.copy()\n",
    "\n",
    "# True과 False인 값을 각각 255과 0으로 바꿔줍니다\n",
    "cat_img_mask = cat_seg_map.astype(np.uint8) * 255\n",
    "\n",
    "# 255와 0을 적당한 색상으로 바꿔봅니다\n",
    "cat_color_mask = cv2.applyColorMap(cat_img_mask, cv2.COLORMAP_JET)\n",
    "\n",
    "# 원본 이미지와 마스트를 적당히 합쳐봅니다\n",
    "# 0.6과 0.4는 두 이미지를 섞는 비율입니다.\n",
    "cat_img_show = cv2.addWeighted(cat_img_show, 0.6, cat_color_mask, 0.4, 0.0)\n",
    "\n",
    "draw_img(cv2.cvtColor(cat_img_show, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이미지 흐리게 만들기\n",
    "  - cv2.blur(img, blurring_kernel_size)\n",
    "    - OpenCV의 blur 함수는 평균 블러링(Averaging Blurring)이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blurring kernel size = (17,17)\n",
    "cat_img_orig_blur = cv2.blur(CAT_IMG_ORIG, (17,17))\n",
    "\n",
    "draw_img(cv2.cvtColor(cat_img_orig_blur, cv2.COLOR_BGR2RGB), 'cat_img_orig_blur')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 흐려진 이미지에서 세그멘테이션 마스크를 이용해서 배경만 추출\n",
    "  - 색이 없으면 검게 보이고(0) 색이 있으면 흰색(1)으로 표현됨\n",
    "    - cv2.bitwise_not(img): 색 반전 => 0은 1로, 1은 0으로 변경\n",
    "    - cv2.bitwise_and(img_1, img_2): 모두 흰색(1)인 부분만 1로 나타남"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-not 연산\n",
    "\n",
    "|A|not A|\n",
    "|:---:|:---:|\n",
    "|1|0|\n",
    "|0|1|\n",
    "\n",
    "-and 연산\n",
    "\n",
    "|A|B|A and B|\n",
    "|:---:|:---:|:---:|\n",
    "|0|0|0|\n",
    "|1|0|0|\n",
    "|0|1|0|\n",
    "|1|1|1|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_img_mask_color = cv2.cvtColor(cat_img_mask, cv2.COLOR_GRAY2BGR)\n",
    "cat_img_bg_mask = cv2.bitwise_not(cat_img_mask_color)\n",
    "cat_img_bg_blur = cv2.bitwise_and(cat_img_orig_blur, cat_img_bg_mask)\n",
    "\n",
    "draw_img(cv2.cvtColor(cat_img_bg_blur, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 배경 이미지와 고양이 이미지 합치기\n",
    "  - numpy.where(condition,[x,y])\n",
    "    - 입력 조건을 충족하는 배열의 색인을 생성\n",
    "    - 조건이 True이면 출력에 x의 요소가 포함되고, 그렇지 않으면 출력에 y의 요소가 포함된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_img_concat = np.where(cat_img_mask_color==255, CAT_IMG_ORIG, cat_img_bg_blur)\n",
    "\n",
    "draw_img(cv2.cvtColor(cat_img_concat, cv2.COLOR_BGR2RGB), 'cat_img_concat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이미지 처리 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cf. plt.savefig() 는 plt.show() 직전에 해야 처리가 끝난 이미지 저장이 제대로 된다.\n",
    "# plt.savefig('image.png', bbox_inches='tight',pad_inches = 0) 로는 공백 및 테두리 숨기기가 제대로 안 됐기에 plt.imsave('image.png', img)를 사용함\n",
    "plt.imsave(\"result_imgs/cat_blur.png\", cv2.cvtColor(cat_img_concat, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- blur 적용 전후 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_imgs(cv2.cvtColor(CAT_IMG_ORIG, cv2.COLOR_BGR2RGB), cv2.cvtColor(cat_img_concat, cv2.COLOR_BGR2RGB), 'CAT_IMG_ORIG', 'cat_img_concat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 배경전환 크로마키 사진"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**나의 요구사항: 배경 이미지에 있는 커튼에 고양이가 매달려 있게 만들 것이다. 매우 귀여울 것이기 때문!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 배경 이미지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACK_IMG_PATH = 'human_segmentation/images/pforphoto.jpg'  \n",
    "BACK_IMG_ORIG = cv2.imread(BACK_IMG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_img(cv2.cvtColor(BACK_IMG_ORIG, cv2.COLOR_BGR2RGB), 'BACK_IMG_ORIG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 고양이 사진 크기 변경\n",
    "  - cv2.resize(img, (w,h), interpolation)\n",
    "    - interpolation 종류: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_LANCZOS4, cv2.INTER_AREA\n",
    "\n",
    "|interpolation|설명|\n",
    "|:---:|:---:|\n",
    "|cv2.INTER_NEAREST|최근방 이웃 보간법|\n",
    "|cv2.INTER_LINEAR|양선형 보간법(2x2 이웃 픽셀 참조)|\n",
    "|cv2.INTER_CUBIC|3차회선 보간법(4x4 이웃 픽셀 참조)|\n",
    "|cv2.INTER_LANCZOS4|Lanczos 보간법 (8x8 이웃 픽셀 참조)|\n",
    "|cv2.INTER_AREA|영상 축소시 효과적|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고양이만 남기고 나머지는 0(검은색) 처리\n",
    "cat_only_img_concat = np.where(cat_img_mask_color==255, CAT_IMG_ORIG, 0)\n",
    "\n",
    "# 고양이 resize\n",
    "h, w, c = cat_only_img_concat.shape\n",
    "cat_img_resize = cv2.resize(cat_only_img_concat, (w//4, h//4), cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_img(cv2.cvtColor(cat_img_resize, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 배경 이미지와 고양이 이미지 합치기\n",
    "  - numpy.where(condition,[x,y])\n",
    "    - 입력 조건을 충족하는 배열의 색인을 생성\n",
    "    - 조건이 True이면 출력에 x의 요소가 포함되고, 그렇지 않으면 출력에 y의 요소가 포함된다.\n",
    "  - cv2.addWeighted(img_1, a, img_2, b, c)\n",
    "    - result = img_1 * a + img_2 * b + c\n",
    "    - b = 1 - a\n",
    "  - [E-03]에서 사용했던 코드 재활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_img_show = BACK_IMG_ORIG.copy()\n",
    "\n",
    "refined_y = 200\n",
    "refined_x = 400\n",
    "\n",
    "alpha = 0.2  # 투명도\n",
    "sticker_area = back_img_show[refined_y:refined_y+cat_img_resize.shape[0], refined_x:refined_x+cat_img_resize.shape[1]]\n",
    "back_img_show[refined_y:refined_y +cat_img_resize.shape[0], refined_x:refined_x+cat_img_resize.shape[1]] = \\\n",
    "cv2.addWeighted(sticker_area, alpha, np.where(cat_img_resize==0,sticker_area,cat_img_resize).astype(np.uint8), 1-alpha, 0)  # 컬러 값 0(검은색)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_img(cv2.cvtColor(back_img_show, cv2.COLOR_BGR2RGB), 'back_n_cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이미지 처리 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cf. plt.savefig() 는 plt.show() 직전에 해야 처리가 끝난 이미지 저장이 제대로 된다.\n",
    "# plt.savefig('image.png', bbox_inches='tight',pad_inches = 0) 로는 공백 및 테두리 숨기기가 제대로 안 됐기에 plt.imsave('image.png', img)를 사용함\n",
    "plt.imsave(\"result_imgs/back_n_cat.png\", cv2.cvtColor(back_img_show, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 합치기 전후 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_imgs(cv2.cvtColor(BACK_IMG_ORIG, cv2.COLOR_BGR2RGB), cv2.cvtColor(back_img_show, cv2.COLOR_BGR2RGB), 'BACK_IMG_ORIG', 'back_n_cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. 사진에서 문제점 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지의 경계선 처리가 깔끔하지 못하다.\n",
    "\n",
    "![](result_imgs/person_blur.png)\n",
    "\n",
    "![](result_imgs/cat_blur.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. 해결 방법을 제안해 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인물모드 사진의 문제점을 해결할 방안 제시하기\n",
    "- 단순히 'XXX 기술을 사용한다.' 정도의 선언적 솔루션 X\n",
    "- => 내가 선택한 기술이 DeepLab 모델의 Semantic Segmentation 이 만들어 낸 Mask 영역에 어떻게 적용되어 문제점을 보완하게 되는지의 메커니즘이 포함된 솔루션 O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### My Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경계선이 자연스러워 보일 수 있는 방법 생각/검색/시도한 목록\n",
    "\n",
    "1. 엣지 검출 전 샤프닝 적용\n",
    "2. 엣지 검출하기 위해 캐니엣지 적용: cv2.Canny(img, 임계값, 임계값)\n",
    "3. 블러링 함수로 cv2.blur() 대신 다른 함수 사용해보기\n",
    "4. 배경 제거 함수로 cv2.bgsegm.createBackgroundSubtractorMOG(history, nmixtures, backgroundRatio, noiseSigma) 사용해보기\n",
    "\n",
    "=> 전부 실패했다. \n",
    "\n",
    "5. 아예 다른 모델 사용하기\n",
    "- MODNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from git import Repo\n",
    "\n",
    "# clone the repository\n",
    "if not os.path.exists('MODNet'):\n",
    "  !git clone https://github.com/ZHKKKe/MODNet\n",
    "%cd MODNet/\n",
    "\n",
    "# dowload the pre-trained ckpt for image matting\n",
    "pretrained_ckpt = 'pretrained/modnet_photographic_portrait_matting.ckpt'\n",
    "if not os.path.exists(pretrained_ckpt):\n",
    "  !gdown --id 1mcr7ALciuAsHCpLnrtG_eop5-EYhbCmz \\\n",
    "          -O pretrained/modnet_photographic_portrait_matting.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "# clean and rebuild the image folders\n",
    "input_folder = 'demo/image_matting/colab/input'\n",
    "if os.path.exists(input_folder):\n",
    "  shutil.rmtree(input_folder)\n",
    "os.makedirs(input_folder)\n",
    "\n",
    "output_folder = 'demo/image_matting/colab/output'\n",
    "if os.path.exists(output_folder):\n",
    "  shutil.rmtree(output_folder)\n",
    "os.makedirs(output_folder)\n",
    "\n",
    "# upload images (PNG or JPG)\n",
    "image_names = list(files.upload().keys())\n",
    "for image_name in image_names:\n",
    "  shutil.move(image_name, os.path.join(input_folder, image_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "# clean and rebuild the image folders\n",
    "input_folder = 'demo/image_matting/colab/input'\n",
    "if os.path.exists(input_folder):\n",
    "  shutil.rmtree(input_folder)\n",
    "os.makedirs(input_folder)\n",
    "\n",
    "output_folder = 'demo/image_matting/colab/output'\n",
    "if os.path.exists(output_folder):\n",
    "  shutil.rmtree(output_folder)\n",
    "os.makedirs(output_folder)\n",
    "\n",
    "# upload images (PNG or JPG)\n",
    "image_names = list(files.upload().keys())\n",
    "for image_name in image_names:\n",
    "  shutil.move(image_name, os.path.join(input_folder, image_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m demo.image_matting.colab.inference \\\n",
    "        --input-path demo/image_matting/colab/input \\\n",
    "        --output-path demo/image_matting/colab/output \\\n",
    "        --ckpt-path ./pretrained/modnet_photographic_portrait_matting.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def combined_display(image, matte):\n",
    "  # calculate display resolution\n",
    "  w, h = image.width, image.height\n",
    "  rw, rh = 800, int(h * 800 / (3 * w))\n",
    "  \n",
    "  # obtain predicted foreground\n",
    "  image = np.asarray(image)\n",
    "  if len(image.shape) == 2:\n",
    "    image = image[:, :, None]\n",
    "  if image.shape[2] == 1:\n",
    "    image = np.repeat(image, 3, axis=2)\n",
    "  elif image.shape[2] == 4:\n",
    "    image = image[:, :, 0:3]\n",
    "  matte = np.repeat(np.asarray(matte)[:, :, None], 3, axis=2) / 255\n",
    "  foreground = image * matte + np.full(image.shape, 255) * (1 - matte)\n",
    "  \n",
    "  # combine image, foreground, and alpha into one line\n",
    "  combined = np.concatenate((image, foreground, matte * 255), axis=1)\n",
    "  combined = Image.fromarray(np.uint8(combined)).resize((rw, rh))\n",
    "  return combined\n",
    "\n",
    "# visualize all images\n",
    "image_names = os.listdir(input_folder)\n",
    "for image_name in image_names:\n",
    "  matte_name = image_name.split('.')[0] + '.png'\n",
    "  image = Image.open(os.path.join(input_folder, image_name))\n",
    "  matte = Image.open(os.path.join(output_folder, matte_name))\n",
    "  display(combined_display(image, matte))\n",
    "  print(image_name, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_filename = 'matte.zip'\n",
    "if os.path.exists(zip_filename):\n",
    "  os.remove(zip_filename)\n",
    "\n",
    "os.system(f\"zip -r -j {zip_filename} {output_folder}/*\")\n",
    "files.download(zip_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 엣지 검출 전 샤프닝 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 언샤프 마스크 필터링 https://deep-learning-study.tistory.com/155\n",
    "\n",
    "img = cv2.imread(PERSON_IMG_PATH, cv2.COLOR_BGR2GRAY)\n",
    "blr = cv2.GaussianBlur(img, (0, 0), 2) # 표준편차 2, 필터 크기는 자동 설정\n",
    "\n",
    "a = 2.0 # 샤프닝 정도 결정하는 변수\n",
    "dst = np.clip((1+a)*img - a * blr, 0, 255).astype(np.uint8) # 계산은 정수, 출력은 실수\n",
    "\n",
    "draw_imgs(cv2.cvtColor(PERSON_IMG_ORIG, cv2.COLOR_BGR2RGB), cv2.cvtColor(dst, cv2.COLOR_BGR2RGB), 'before', 'after')\n",
    "\n",
    "plt.imsave('result_imgs/person_sharpening.png', cv2.cvtColor(dst, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 대조 색상 강조하기 https://ansan-survivor.tistory.com/311\n",
    "\n",
    "''' 함수) 이미지를 더 선명하게 Contrast(대조) 기법을 적용시킴.\n",
    "param : 컬러 이미지\n",
    "return : 대조된 이미지\n",
    "'''\n",
    "\n",
    "def img_Contrast(img):\n",
    "    # -----Converting image to LAB Color model-----------------------------------\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    \n",
    "    # -----Splitting the LAB image to different channels-------------------------\n",
    "    l, a, b = cv2.split(lab)\n",
    "    \n",
    "    # -----Applying CLAHE to L-channel-------------------------------------------\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l)\n",
    "    \n",
    "    # -----Merge the CLAHE enhanced L-channel with the a and b channel-----------\n",
    "    limg = cv2.merge((cl, a, b))\n",
    "    \n",
    "    # -----Converting image from LAB Color model to RGB model--------------------\n",
    "    final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "    return final\n",
    "\n",
    "img = cv2.imread('result_imgs/person_sharpening.png', cv2.COLOR_BGR2GRAY)\n",
    "img = img_Contrast(img)\n",
    "\n",
    "plt.imsave(\"result_imgs/person_contrast.png\", cv2.cvtColor(img, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 보행자 검출을 위한 HOG\n",
    "    - 이미지가 아니라 영상에서 사람을 검출하기 위한 용도의 코드지만 혹시나 사용해봤다. 역시 제대로 작동하지 않았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "# 동영상 불러오기\n",
    "# img = cv2.imread('human_segmentation/images/couple.jpg', cv2.COLOR_BGR2RGB)\n",
    "img = dst.copy()\n",
    "\n",
    "# 보행자 검출을 위한 HOG 기술자 설정\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "# 매 프레임마다 보행자 검출\n",
    "detected, _ = hog.detectMultiScale(img) # 사각형 정보를 받아옴\n",
    "print(detected)\n",
    "\n",
    "# 검출 결과 화면 표시\n",
    "for (x, y, w, h) in detected:\n",
    "    c = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "    cv2.rectangle(img, (x, y, w, h), c, 3)\n",
    "\n",
    "####\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 엣지 검출 좀 더 정확히 해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 캐니 엣지 적용\n",
    "edged = cv2.Canny(img, 300, 600)  # 하단 임계값과 상단 임계값은 실험적으로 결정하기\n",
    "\n",
    "draw_img(cv2.cvtColor(edged, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "plt.imsave(\"result_imgs/person_canny_edge.png\", cv2.cvtColor(edged, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 블러링 함수로 cv2.blur() 대신 다른 함수 사용해보기\n",
    "    - cv2.GaussianBlur()\n",
    "    - cv2.MedianBlur()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 이미지 입력\n",
    "dst_gray = cv2.cvtColor(dst, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "edged_path = 'result_imgs/person_edged.png'\n",
    "person_2_segvalues, person_2_output = MODEL.segmentAsPascalvoc(edged_path)\n",
    "for class_id in person_2_segvalues['class_ids']:\n",
    "    print(LABEL_NAMES[class_id])\n",
    "draw_img(person_2_output, 'person_2_output')\n",
    "\n",
    "seg_color = get_colormap('person')\n",
    "person_2_seg_map = np.all(person_2_output==seg_color, axis=-1)\n",
    "\n",
    "person_2_img_mask = person_2_seg_map.astype(np.uint8) * 255\n",
    "\n",
    "# person_2_img_orig_blur = cv2.medianBlur(dst_gray, 21)\n",
    "person_2_img_orig_blur = cv2.GaussianBlur(dst_gray, (13,13), 0)\n",
    "\n",
    "# person_2_img_mask_color = cv2.cvtColor(person_2_img_mask, cv2.COLOR_GRAY2BGR)\n",
    "person_2_img_mask_color = person_2_img_mask.copy()\n",
    "person_2_img_bg_mask = cv2.bitwise_not(person_2_img_mask)\n",
    "person_2_img_bg_blur = cv2.bitwise_and(person_2_img_orig_blur, person_2_img_bg_mask)\n",
    "\n",
    "draw_img(cv2.cvtColor(person_2_img_bg_blur, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "person_2_img_concat = np.where(person_2_img_mask_color==255, dst_gray, person_2_img_bg_blur)\n",
    "\n",
    "draw_img(cv2.cvtColor(person_2_img_concat, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "plt.imsave(\"result_imgs/person_gaussian_blur.png\", cv2.cvtColor(person_2_img_concat, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = img.copy()\n",
    "image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blur = cv2.GaussianBlur(image_gray, ksize=(3,3), sigmaX=0)\n",
    "ret, thresh1 = cv2.threshold(blur, 127, 255, cv2.THRESH_BINARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blur = cv2.GaussianBlur(image_gray, ksize=(5,5), sigmaX=0)\n",
    "ret, thresh1 = cv2.threshold(blur, 127, 255, cv2.THRESH_BINARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edged = cv2.Canny(blur, 10, 250)\n",
    "plt.imshow(cv2.cvtColor(edged, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "plt.imsave('result_imgs/person_edged.png', cv2.cvtColor(edged, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (7,7))\n",
    "closed = cv2.morphologyEx(edged, cv2.MORPH_CLOSE, kernel)\n",
    "plt.imshow(cv2.cvtColor(closed, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contours, _ = cv2.findContours(closed.copy(),cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 외곽선 그리는 용도. 이미지에 그리기 때문에 이 코드 적용하면 원래 이미지에\n",
    "# 초록색 선 생김\n",
    "contours_image = cv2.drawContours(image, contours, -1, (0,255,0), 3)\n",
    "plt.imshow(cv2.cvtColor(contours_image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 배경 제거 함수로 cv2.bgsegm.createBackgroundSubtractorMOG(history, nmixtures, backgroundRatio, noiseSigma) 사용해보기\n",
    "    - 작동하지 않음!\n",
    "    - 원래 이미지가 아니라 영상에서 사용하던 함수여서 그런 듯하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "fgmask = fgbg.apply(img)\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.imshow(cv2.cvtColor(fgmask, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 회고\n",
    "\n",
    "### - 이번 프로젝트에서 **어려웠던 점**.\n",
    "저작권 없는 이미지 사이트에서 사진 고르느라 시간 다 뺏겼다. 경계선을 자연스럽게 만드는 방법을 못 찾겠다.\n",
    "\n",
    "### - 프로젝트를 진행하면서 **알아낸 점** 혹은 **아직 모호한 점**.\n",
    "맨 아래에 내용 정리를 해놓았음\n",
    "\n",
    "### - 루브릭 평가 지표를 맞추기 위해 **시도한 것들**.\n",
    "\n",
    ">#### **루브릭**\n",
    ">|평가문항|상세기준|\n",
    ">|:---:|:---:|\n",
    ">|1. 인물모드 사진을 성공적으로 제작하였다.|아웃포커싱 효과가 적용된 본인의 인물모드 사진과 고양이 사진, 배경전환 크로마키사진을 각각 1장 이상 성공적으로 제작하였다.|\n",
    ">|2. 제작한 인물모드 사진들에서 나타나는 문제점을 정확히 지적하였다.|인물사진에서 발생한 문제점을 정확히 지적한 사진을 제출하였다.|\n",
    ">|3. 인물모드 사진의 문제점을 개선할 수 있는 솔루션을 적절히 제시하였다.|추가적인 depth 정보를 활용하여 semantic segmentation mask의 오류를 보완할 수 있는 좋은 솔루션을 구체적으로 제시하였다.|\n",
    "\n",
    "사람 사진, 고양이 사진으로 아웃포커싱 효과를 적용했다. 배경 이미지 속 커튼에 고양이가 매달려있는 귀여운 모습을 완성시켰다.\n",
    "\n",
    "다른 문제점은 잘 모르겠고 경계선이 부자연스러운 것만 해결하면 괜찮다고 생각했다.\n",
    "\n",
    "원본 이미지의 경계선을 샤프닝을 통해 더 뚜렷하게 만들면 어떨까 시도해봤지만 DeepLab 모델에 넣어봤더니 딱히 달라진 것은 없었다. 블러링 함수를 바꿔봤지만 여전히 달라진 것은 없었다. 배경 제거 함수를 사용해 봤지만 동작하지 않았다. 짜잘한 건 효과가 없는 모양이다. 그래서 모델을 바꾸는 것이 좋다고 결론내렸다. 찾아보니 MODNet 이라는 모델이 DeepLab처럼 사람 검출을 잘하는 것으로 나와서 이걸 사용하면 좋겠다고 적었다.\n",
    "\n",
    "\n",
    "### - 만약에 루브릭 평가 관련 지표를 **달성 하지 못했을 때, 이유에 관한 추정**.\n",
    "없음\n",
    "\n",
    "### - **자기 다짐**\n",
    "이제 안 되는 것은 타협하고 아이펠 선배의 우수 노드를 참고해서 넘어가야겠다.\n",
    "\n",
    "---\n",
    "- 대나무숲\n",
    "\n",
    "라이브러리 때문에 환장하겠다. 이놈의 아나콘다 또 꼬여서 빨간 에러만 잔뜩 뜨길래 revision 했더니 vs code에서 python 실행이 아예 막혀버렸다. 정확히 말하자면 실행이 먹히지가 않고 그냥 무응답이다. vs code의 python interpreter 설정 리셋을 눌러도 반응이 없다. 너무 답답해서 터미널로 conda info 쳐봤더니 conda env가 activate가 안 됐다고 나온다. 내가 몇 번을 activate 했는데도 저러길래 vs code 삭제 & 재설치까지 했다. 그래도 안 된다. 구글에 검색해봐도 나와 같은 현상을 겪는 사람은 아무도 없다. vs code에 설정된 python interpreter가 vs code 삭제했는데도 그대로 남아있어서 이걸 제거해야 하는데 어디에 있는지 찾을 수가 없다. 해결 방법이 없다... 결국 env 삭제하고 새로 만들어서 패키지 설치하는 중인데 또 다시 새빨간 에러로 도배됐다. 주말동안 이거 때문에 주피터 노트북 삭제 & 재설치를 몇 번을 했는데 또또또 원점이다. 다시 처음부터 주피터 노트북 삭제하고 설치해야 한다. 쓸데없는 걸로 시간 다 날리고 스트레스가 너무 심하다. 왜 그렇게 화나는지 빨간 색 에러의 일부를 보여주겠다. '일부'라서 저것보다 더 많다는 것을 염두해 둬야 할 것이다.\n",
    "\n",
    "![](result_imgs/error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 내용 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Image Blurring\n",
    "  - Image Blurring == Image Smoothing\n",
    "  - 용도: 노이즈 제거, 경계선 흐리게 만들기\n",
    "  - OpenCV에는 4가지 blurring 방법 제공\n",
    "    - Averaging\n",
    "      - Box형태의 kernel을 이미지에 적용한 후 평균값을 box의 중심점에 적용하는 형태\n",
    "      - 커널\n",
    "        - box filter는 동일한 값으로 구성된 kernel 사용\n",
    "        - e.g. 3x3 box kernel: $K = \\frac{1}{9} \\begin{bmatrix}1&1&1\\\\1&1&1\\\\1&1&1 \\end{bmatrix}$\n",
    "      - 함수\n",
    "        - cv2.blur(src, ksize) → dst: 정규화된 box filter로 이미지 컨볼루젼\n",
    "        - cv2.boxFilter(src, ksize): 정규화되지 않은 box filter로 이미지 컨볼루젼\n",
    "    - Gaussian Filtering\n",
    "      - kernel 행렬의 값을 Gaussian 함수를 통해서 수학적으로 생성하여 적용\n",
    "      - kernel의 사이즈 = 양수 + 홀수로 지정\n",
    "      - 이미지의 [Gaussian Noise](https://en.wikipedia.org/wiki/Gaussian_noise) (전체적으로 밀도가 동일한 노이즈, 백색노이즈)를 제거하는 데 가장 효과적\n",
    "      - cv2.GaussianBlur(img, ksize, sigmaX)\n",
    "    - Median Filtering\n",
    "      - kernel window와 pixel의 값들을 정렬한 후에 중간값을 선택하여 적용\n",
    "      - [salt-and-pepper noise](https://ko.wikipedia.org/wiki/%EC%A0%90%EC%9E%A1%EC%9D%8C) 제거에 가장 효과적\n",
    "      - cv2.medianBlur(src, ksize)\n",
    "    - Bilateral Filtering\n",
    "      - Bilateral Filtering(양방향 필터)은 경계선을 유지(경계선 흐려짐X)하면서 Gaussian Blur처리\n",
    "      - cv2.bilateralFilter(src, d, sigmaColor, sigmaSpace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ce9abe337a9e694d01ea52d504102083454ad8bd4b0e3a574e4432f4229329"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('aiffel_3.8': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
