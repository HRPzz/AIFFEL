{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. 문자를 읽을 수 있는 딥러닝\n",
    "\n",
    "**문자를 읽는 OCR 모델의 구조를 배우고, keras-ocr과 테서랙트 엔진을 써본다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15-1. 들어가며"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ mkdir -p ~/aiffel/ocr_python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15-2. 기계가 읽을 수 있나요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ pip install --upgrade google-api-python-client\n",
    "$ pip install google-cloud-vision\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text(path):\n",
    "    \"\"\"Detects text in the file.\"\"\"\n",
    "    from google.cloud import vision\n",
    "    import io\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    with io.open(path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "        \n",
    "    image = vision.Image(content=content)\n",
    "\n",
    "    response = client.text_detection(image=image)\n",
    "    texts = response.text_annotations\n",
    "    print('Texts:')\n",
    "\n",
    "    for text in texts:\n",
    "       print('\\n\"{}\"'.format(text.description))\n",
    "\n",
    "    vertices = (['({},{})'.format(vertex.x, vertex.y)\n",
    "                 for vertex in text.bounding_poly.vertices])\n",
    "\n",
    "    print('bounds: {}'.format(','.join(vertices)))\n",
    "\n",
    "    if response.error.message:\n",
    "        raise Exception(\n",
    "            '{}\\nFor more info on error messages, check: '\n",
    "            'https://cloud.google.com/apis/design/errors'.format(\n",
    "                response.error.message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬 환경에서는 다운받은 인증키 경로가 정확하게 지정되어 있어야 합니다. \n",
    "# 클라우드 환경에서는 무시해도 좋습니다\n",
    "!ls -l $GOOGLE_APPLICATION_CREDENTIALS\n",
    "\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] =  os.getenv('HOME')+'/aiffel/ocr_python/my_google_api_key.json'\n",
    "\n",
    "# 입력 이미지 경로를 지정해 주세요.\n",
    "# (예시) path = os.getenv('HOME')+'/aiffel/ocr_python/test_image.png'\n",
    "# path = # [[YOUR IMAGE FILE PATH]]   \n",
    "path = os.getenv('HOME')+'/aiffel/ocr_python/E-19-test.jpg'\n",
    "\n",
    "# 위에서 정의한 OCR API 이용 함수를 호출해 봅시다.\n",
    "detect_text(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15-3. 어떤 과정으로 읽을까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15-4. 딥러닝 문자인식의 시작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15-5. 사진 속 문자 찾아내기 - detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15-6. 사진 속 문자 읽어내기 - recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15-7. keras-ocr 써보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import keras_ocr\n",
    "\n",
    "# keras-ocr이 detector과 recognizer를 위한 모델을 자동으로 다운로드받게 됩니다. \n",
    "pipeline = keras_ocr.pipeline.Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트에 사용할 이미지 url을 모아 봅니다. 추가로 더 모아볼 수도 있습니다. \n",
    "image_urls = [\n",
    "  'https://source.unsplash.com/M7mu6jXlcns/640x460',\n",
    "  'https://source.unsplash.com/6jsp4iHc8hI/640x460',\n",
    "  'https://source.unsplash.com/98uYQ-KupiE',\n",
    "  'https://source.unsplash.com/j9JoYpaJH3A',\n",
    "  'https://source.unsplash.com/eBkEJ9cH5b4'\n",
    "]\n",
    "\n",
    "images = [ keras_ocr.tools.read(url) for url in image_urls]\n",
    "prediction_groups = [pipeline.recognize([url]) for url in image_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predictions\n",
    "fig, axs = plt.subplots(nrows=len(images), figsize=(20, 20))\n",
    "for idx, ax in enumerate(axs):\n",
    "    keras_ocr.tools.drawAnnotations(image=images[idx], \n",
    "                                    predictions=prediction_groups[idx][0], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15-8. 테서랙트 써보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ sudo apt install tesseract-ocr\n",
    "$ sudo apt install libtesseract-dev\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ pip list | grep pytesseract\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from pytesseract import Output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# OCR Engine modes(–oem):\n",
    "# 0 - Legacy engine only.\n",
    "# 1 - Neural nets LSTM engine only.\n",
    "# 2 - Legacy + LSTM engines.\n",
    "# 3 - Default, based on what is available.\n",
    "\n",
    "# Page segmentation modes(–psm):\n",
    "# 0 - Orientation and script detection (OSD) only.\n",
    "# 1 - Automatic page segmentation with OSD.\n",
    "# 2 - Automatic page segmentation, but no OSD, or OCR.\n",
    "# 3 - Fully automatic page segmentation, but no OSD. (Default)\n",
    "# 4 - Assume a single column of text of variable sizes.\n",
    "# 5 - Assume a single uniform block of vertically aligned text.\n",
    "# 6 - Assume a single uniform block of text.\n",
    "# 7 - Treat the image as a single text line.\n",
    "# 8 - Treat the image as a single word.\n",
    "# 9 - Treat the image as a single word in a circle.\n",
    "# 10 - Treat the image as a single character.\n",
    "# 11 - Sparse text. Find as much text as possible in no particular order.\n",
    "# 12 - Sparse text with OSD.\n",
    "# 13 - Raw line. Treat the image as a single text line, bypassing hacks that are Tesseract-specific.\n",
    "\n",
    "def crop_word_regions(image_path='./images/sample.png', output_path='./output'):\n",
    "    if not os.path.exists(output_path):\n",
    "        os.mkdir(output_path)\n",
    "    custom_oem_psm_config = r'--oem 3 --psm 3'\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    recognized_data = pytesseract.image_to_data(\n",
    "        image, lang='eng',    # 한국어라면 lang='kor'\n",
    "        config=custom_oem_psm_config,\n",
    "        output_type=Output.DICT\n",
    "    )\n",
    "    \n",
    "    top_level = max(recognized_data['level'])\n",
    "    index = 0\n",
    "    cropped_image_path_list = []\n",
    "    for i in range(len(recognized_data['level'])):\n",
    "        level = recognized_data['level'][i]\n",
    "    \n",
    "        if level == top_level:\n",
    "            left = recognized_data['left'][i]\n",
    "            top = recognized_data['top'][i]\n",
    "            width = recognized_data['width'][i]\n",
    "            height = recognized_data['height'][i]\n",
    "            \n",
    "            output_img_path = os.path.join(output_path, f\"{str(index).zfill(4)}.png\")\n",
    "            print(output_img_path)\n",
    "            cropped_image = image.crop((\n",
    "                left,\n",
    "                top,\n",
    "                left+width,\n",
    "                top+height\n",
    "            ))\n",
    "            cropped_image.save(output_img_path)\n",
    "            cropped_image_path_list.append(output_img_path)\n",
    "            index += 1\n",
    "    return cropped_image_path_list\n",
    "\n",
    "\n",
    "work_dir = os.getenv('HOME')+'/aiffel/ocr_python'\n",
    "img_file_path = work_dir + '/test_image.png'   #테스트용 이미지 경로입니다. 본인이 선택한 파일명으로 바꿔주세요. \n",
    "\n",
    "cropped_image_path_list = crop_word_regions(img_file_path, work_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ sudo apt install tesseract-ocr-kor\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_images(cropped_image_path_list):\n",
    "    custom_oem_psm_config = r'--oem 3 --psm 7'\n",
    "    \n",
    "    for image_path in cropped_image_path_list:\n",
    "        image = Image.open(image_path)\n",
    "        recognized_data = pytesseract.image_to_string(\n",
    "            image, lang='eng',    # 한국어라면 lang='kor'\n",
    "            config=custom_oem_psm_config,\n",
    "            output_type=Output.DICT\n",
    "        )\n",
    "        print(recognized_data['text'])\n",
    "    print(\"Done\")\n",
    "\n",
    "# 위에서 준비한 문자 영역 파일들을 인식하여 얻어진 텍스트를 출력합니다.\n",
    "recognize_images(cropped_image_path_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15-9. 프로젝트 : 다양한 OCR모델 비교하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_ocr\n",
    "import pytesseract\n",
    "\n",
    "print(keras_ocr.__version__)\n",
    "print(pytesseract.__version__)\n",
    "# pytesseract는 tesseract-ocr에 종속적입니다. 아래 명령어를 통해 설치해야 할 수도 있습니다.\n",
    "# !sudo apt install tesseract-ocr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">## **루브릭**\n",
    "\n",
    ">|번호|평가문항|상세기준|\n",
    ">|:---:|---|---|\n",
    ">|1|OCR을 활용하여 구현하려는 서비스의 기획이 타당한가?|목표로 하는 서비스가 OCR를 적용 가능하며, OCR을 활용했을 때 더욱 유용해진다.|\n",
    ">|2|모델 평가기준이 명확하고 체계적으로 세워졌는가?|평가 기준에 부합하는 테스트 데이터의 특징이 무엇인지 명확하게 제시되었다.|\n",
    ">|3|평가기준에 따라 충분한 분량의 테스트가 진행되고 그 결과가 잘 정리되었는가?|최대 20장까지의 테스트 이미지를 사용해 제시된 평가 기준에 따른 테스트 결과가 잘 정리되어 결론이 도출되었다.|"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ce9abe337a9e694d01ea52d504102083454ad8bd4b0e3a574e4432f4229329"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('aiffel_3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
