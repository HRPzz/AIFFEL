# 4. 작사가 인공지능 만들기

**LSTM 모델과 셰익스피어 데이터셋을 사용해 간단한 작사가 인공지능을 만들어 본다.**

---

|-|목차|⏲ 300분|
|:---:|---|:---:|
|4-1| 들어가며 | 10분|
|4-2| 시퀀스? 스퀀스! | 10분|
|4-3| I 다음 am을 쓰면 반 이상은 맞더라 | 20분|
|4-4| 실습 (1) 데이터 다듬기 | 30분|
|4-5| 실습 (2) 인공지능 학습시키기 | 30분|
|4-6| 실습 (3) 잘 만들어졌는지 평가하기 | 20분|
|4-7| 프로젝트: 멋진 작사가 만들기 | 180분|
|4-8| 프로젝트 제출|-|

---

## 목차

- 시퀀스? 스퀀스!
- I 다음 am을 쓰면 반 이상은 맞더라
- 실습
  - 1) 데이터 다듬기
  - 2) 인공지능 학습시키기
  - 3) 잘 만들어졌는지 평가하기
- 프로젝트 : 멋진 인공지능 작사가 만들기

---

- 문장: 생각이나 감정을 말과 글로 표현할 때 완결된 내용을 나타내는 최소의 단위
- 최고의 인공지능 작문가: 구글의 GPT-3
- 시퀀스(Sequence) 데이터: 나열된 데이터를 의미
  - 파이썬 시퀀스 자료형: List, String, Set, Dictionary
  - 인공지능이 예측하려면 어느 정도는 연관성이 있어야 함
- 통계 기반 방법으로 문법 익히기
  - 많은 데이터가 곧 좋은 결과를 만듦
- 순환신경망(RNN)
  - 생성한 단어를 다시 입력으로 사용
  - 필요 문장 데이터
    - 소스 문장(Source Sentence)
      - 모델의 입력이 되는 문장(변수 X_train)
      - <start> 가 문장의 시작에 더해진 입력 데이터(문제지)
      - source_sentence = "<start>" + sentence
    - 타겟 문장(Target Sentence)
      - 정답 역할을 하게 될 모델의 출력 문장(y_train)
      - <end> 가 문장의 끝에 더해진 출력 데이터(답안지)
      - target_sentence = sentence + "<end>"
  - ![rnn.png](https://d3s0tskafalll9.cloudfront.net/media/images/E-12-RNN2.max-800x600.png)
- 언어 모델(Language Model)
  - n−1개의 단어 시퀀스가 주어졌을 때, n번째 단어 $w_n$으로 무엇이 올지를 예측하는 확률 모델
  - 확률적 표현
    - 파라미터 θ로 모델링 하는 언어 모델: $P(w_n∣w_1​,...,w_{n−1};θ)$
  - RNN 의 개념과 같다!
  - 잘 학습된 언어 모델은 훌륭한 모델 생성기로 동작
- 데이터 전처리
  - 연극 대본에서 문장(대사)만 추출
  - 단어 사전 만들기: 토큰화(Tokenize) - 정규표현식(Regex)을 이용한 필터링
  - 토큰 <start>, <end> 추가
  - 자연어 처리를 위해 tf.keras.preprocessing.text.Tokenizer 패키지 사용
    - 벡터화(vectorize): 정제된 데이터 -> 토큰화 -> 단어 사전 생성 -> 데이터를 숫자로 변환(=텐서)
    - 위 과정을 한 방에 해줌
    - 입출력 데이터 모두 텐서로 변환되어 처리되는 것
  - 입력 시퀀스 길이보다 출력 문장이 짧을 경우 0 으로 패딩(padding) 채움 => <pad>
- 데이터 전처리 정리
  - 정규표현식을 이용한 corpus 생성
  - tf.keras.preprocessing.text.Tokenizer를 이용해 corpus를 텐서로 변환
  - tf.data.Dataset.from_tensor_slices()를 이용해 corpus 텐서를 tf.data.Dataset객체로 변환
- 텐서(tensor): 숫자로 변환된 데이터
  - scalar: 0차원 배열
  - vector: 1차원 배열
  - matrix: 2차원 배열
  - n-tensor: 3차원 이상인 배열
- 우리가 만들 모델 구조
  - tf.keras.Model을 Subclassing하는 방식으로 만들 것
  - 모델 구성
    - ![our_model.png](https://d3s0tskafalll9.cloudfront.net/media/images/E-12-4.max-800x600.png)
    - 1개의 Embedding 레이어
      - 입력 텐서에 있는 단어 사전의 인덱스 값을 해당 인덱스 번째의 워드 벡터로 변환
        - word vector: 의미 벡터 공간에서 단어의 추상적 표현(representation)으로 사용됨
        - embedding_size
          - 워드 벡터의 차원수 == 단어가 추상적으로 표현되는 크기
          - 값이 커질수록 단어의 추상적인 특징들을 더 잡아냄
          - 충분한 데이터가 주어지지 않으면 오히려 혼란만을 야기
    - 2개의 LSTM 레이어
      - hidden_size
        - hidden state 의 차원수
        - 모델에 얼마나 많은 일꾼을 둘 것인가?
        - 충분한 데이터가 주어지지 않으면 오히려 혼란만을 야기
    - 1개의 Dense 레이어
- Loss
  - 모델이 오답을 만들고 있는 정도
  - 오답률이 감소하면 학습이 잘 진행되고 있다고 해석할 수 있다.
- 모델 평가 방법
  - 요약/문제 풀이/번역 모델(정량적으로 측정할 대상이 있는 경우): BLEU, ROUGE 등
  - 작문 모델 평가 방법: 작문을 시켜보고 사람이 평가

---

## 4-7. 프로젝트: 멋진 작사가 만들기

- Step 1. 데이터 다운로드
- Step 2. 데이터 읽어오기
  - glob 모듈 사용하여 파일 읽기
- Step 3. 데이터 정제
  - 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거
  - => 문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외하기
- Step 4. 평가 데이터셋 분리
  - 훈련 데이터와 평가 데이터를 분리
  - 단어장의 크기는 12,000 이상, 총 데이터의 20% 를 평가 데이터셋으로 사용
- Step 5. 인공지능 만들기
  - 모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델을 설계! (Loss는 아래 제시된 Loss 함수를 그대로 사용!)

---

>## **루브릭**

>|번호|평가문항|상세기준|평가결과|
>|:---:|---|---|:---:|
>|1|가사 텍스트 생성 모델이 정상적으로 동작하는가?|텍스트 제너레이션 결과가 그럴듯한 문장으로 생성되는가?|⭐|
>|2|데이터의 전처리와 데이터셋 구성 과정이 체계적으로 진행되었는가?|특수문자 제거, 토크나이저 생성, 패딩처리 등의 과정이 빠짐없이 진행되었는가?|⭐|
>|3|텍스트 생성모델이 안정적으로 학습되었는가?|텍스트 생성모델의 validation loss가 2.2 이하로 낮아졌는가?|☆|
