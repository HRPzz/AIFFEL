{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzMUZVp9CyGZ"
      },
      "source": [
        "# 12. íŠ¸ëœìŠ¤í¬ë¨¸ë¡œ ë§Œë“œëŠ” ëŒ€í™”í˜• ì±—ë´‡\n",
        "\n",
        "**íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì¸ì½”ë” ë””ì½”ë” êµ¬ì¡°ì™€ ì…€í”„ ì–´í…ì…˜ì„ ì½”ë“œë¥¼ í†µí•´ ì´í•´í•´ ë³¸ë‹¤. ì´ë¥¼ ì˜ì–´ì™€ í•œêµ­ì–´ë¡œ ì´ë£¨ì–´ì§„ ì±—ë´‡ ë°ì´í„°ì— ì ìš©í•´ ë³¸ë‹¤.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pS6GeZ4CyGb"
      },
      "source": [
        "## 12-14. í”„ë¡œì íŠ¸: í•œêµ­ì–´ ë°ì´í„°ë¡œ ì±—ë´‡ ë§Œë“¤ê¸°\n",
        "\n",
        "ì˜ì–´ë¡œ ë§Œë“¤ì—ˆë˜ ì±—ë´‡ì„ í•œêµ­ì–´ ë°ì´í„°ë¡œ ë°”ê¿”ì„œ í›ˆë ¨ì‹œì¼œë´…ì‹œë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYkZMzXuCyGb"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5n6rg4FCyGb"
      },
      "source": [
        "### ëª©ì°¨\n",
        "- Step 1. ë°ì´í„° ìˆ˜ì§‘í•˜ê¸°\n",
        "- Step 2. ë°ì´í„° ì „ì²˜ë¦¬í•˜ê¸°\n",
        "- Step 3. SubwordTextEncoder ì‚¬ìš©í•˜ê¸°\n",
        "- Step 4. ëª¨ë¸ êµ¬ì„±í•˜ê¸°\n",
        "- Step 5. ëª¨ë¸ í‰ê°€í•˜ê¸°"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mzfPSW0CyGb"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbRbfV4WC4MF",
        "outputId": "5212b036-e307-4700-ff90-c970079aed33"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6EvMlleCyGc",
        "outputId": "2c88a919-d55d-4f61-d48d-53221ed0429a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìŠ=3\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"ìŠ=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSKb4nJcCyGd"
      },
      "source": [
        "### Step 1. ë°ì´í„° ìˆ˜ì§‘í•˜ê¸°\n",
        "- ì†¡ì˜ìˆ™ë‹˜ì´ ê³µê°œí•œ í•œêµ­ì–´ ì±—ë´‡ ë°ì´í„°: [songys/Chatbot_data](https://github.com/songys/Chatbot_data/blob/master/ChatbotData.csv)\n",
        "    - ì§ˆë¬¸ì— ì±—ë´‡ì´ ìœ„ë¡œí•œë‹¤ëŠ” ì·¨ì§€ë¡œ ë‹µë³€ì„ ì‘ì„±\n",
        "    - ì±—ë´‡ íŠ¸ë ˆì´ë‹ìš© ë¬¸ë‹µ í˜ì´ 11,823ê°œ\n",
        "    - ì¼ìƒë‹¤ë°˜ì‚¬(ì¤‘ë¦½) 0, ì´ë³„(ë¶€ì •) 1, ì‚¬ë‘(ê¸ì •) 2ë¡œ ë ˆì´ë¸”ë§\n",
        "      - ì¤‘ë¦½ 0: 5290ê°œ\n",
        "      - ë¶€ì • 1: 3570ê°œ\n",
        "      - ê¸ì • 2: 2963ê°œ\n",
        "    - ê²°ì¸¡ì¹˜ ì—†ìŒ"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°"
      ],
      "metadata": {
        "id": "2XfJycuwBa3F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "owzjbUFRCyGd",
        "outputId": "7bc0000b-9fc6-4925-9115-e8e5ffd82bb1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-774ff89a-7b58-475c-9e75-93cff69b19c1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12ì‹œ ë•¡!</td>\n",
              "      <td>í•˜ë£¨ê°€ ë˜ ê°€ë„¤ìš”.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1ì§€ë§ í•™êµ ë–¨ì–´ì¡Œì–´</td>\n",
              "      <td>ìœ„ë¡œí•´ ë“œë¦½ë‹ˆë‹¤.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3ë°•4ì¼ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤</td>\n",
              "      <td>ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3ë°•4ì¼ ì •ë„ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤</td>\n",
              "      <td>ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PPL ì‹¬í•˜ë„¤</td>\n",
              "      <td>ëˆˆì‚´ì´ ì°Œí‘¸ë ¤ì§€ì£ .</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11818</th>\n",
              "      <td>í›”ì³ë³´ëŠ” ê²ƒë„ ëˆˆì¹˜ ë³´ì„.</td>\n",
              "      <td>í‹°ê°€ ë‚˜ë‹ˆê¹Œ ëˆˆì¹˜ê°€ ë³´ì´ëŠ” ê±°ì£ !</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11819</th>\n",
              "      <td>í›”ì³ë³´ëŠ” ê²ƒë„ ëˆˆì¹˜ ë³´ì„.</td>\n",
              "      <td>í›”ì³ë³´ëŠ” ê±° í‹°ë‚˜ë‚˜ë´ìš”.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11820</th>\n",
              "      <td>í‘ê¸°ì‚¬ í•´ì£¼ëŠ” ì§ë‚¨.</td>\n",
              "      <td>ì„¤ë œê² ì–´ìš”.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11821</th>\n",
              "      <td>í˜ë“  ì—°ì•  ì¢‹ì€ ì—°ì• ë¼ëŠ”ê²Œ ë¬´ìŠ¨ ì°¨ì´ì¼ê¹Œ?</td>\n",
              "      <td>ì˜ í—¤ì–´ì§ˆ ìˆ˜ ìˆëŠ” ì‚¬ì´ ì—¬ë¶€ì¸ ê±° ê°™ì•„ìš”.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11822</th>\n",
              "      <td>í˜ë“¤ì–´ì„œ ê²°í˜¼í• ê¹Œë´</td>\n",
              "      <td>ë„í”¼ì„± ê²°í˜¼ì€ í•˜ì§€ ì•Šê¸¸ ë°”ë¼ìš”.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11823 rows Ã— 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-774ff89a-7b58-475c-9e75-93cff69b19c1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-774ff89a-7b58-475c-9e75-93cff69b19c1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-774ff89a-7b58-475c-9e75-93cff69b19c1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                             Q                         A  label\n",
              "0                       12ì‹œ ë•¡!                í•˜ë£¨ê°€ ë˜ ê°€ë„¤ìš”.      0\n",
              "1                  1ì§€ë§ í•™êµ ë–¨ì–´ì¡Œì–´                 ìœ„ë¡œí•´ ë“œë¦½ë‹ˆë‹¤.      0\n",
              "2                 3ë°•4ì¼ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤               ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .      0\n",
              "3              3ë°•4ì¼ ì •ë„ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤               ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .      0\n",
              "4                      PPL ì‹¬í•˜ë„¤                ëˆˆì‚´ì´ ì°Œí‘¸ë ¤ì§€ì£ .      0\n",
              "...                        ...                       ...    ...\n",
              "11818           í›”ì³ë³´ëŠ” ê²ƒë„ ëˆˆì¹˜ ë³´ì„.        í‹°ê°€ ë‚˜ë‹ˆê¹Œ ëˆˆì¹˜ê°€ ë³´ì´ëŠ” ê±°ì£ !      2\n",
              "11819           í›”ì³ë³´ëŠ” ê²ƒë„ ëˆˆì¹˜ ë³´ì„.             í›”ì³ë³´ëŠ” ê±° í‹°ë‚˜ë‚˜ë´ìš”.      2\n",
              "11820              í‘ê¸°ì‚¬ í•´ì£¼ëŠ” ì§ë‚¨.                    ì„¤ë œê² ì–´ìš”.      2\n",
              "11821  í˜ë“  ì—°ì•  ì¢‹ì€ ì—°ì• ë¼ëŠ”ê²Œ ë¬´ìŠ¨ ì°¨ì´ì¼ê¹Œ?  ì˜ í—¤ì–´ì§ˆ ìˆ˜ ìˆëŠ” ì‚¬ì´ ì—¬ë¶€ì¸ ê±° ê°™ì•„ìš”.      2\n",
              "11822               í˜ë“¤ì–´ì„œ ê²°í˜¼í• ê¹Œë´        ë„í”¼ì„± ê²°í˜¼ì€ í•˜ì§€ ì•Šê¸¸ ë°”ë¼ìš”.      2\n",
              "\n",
              "[11823 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "csv_path = 'drive/MyDrive/Colab Notebooks/transformer_chatbot/data/ChatbotData.csv'\n",
        "data = pd.read_csv(csv_path)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USbtyHsLCyGe",
        "outputId": "8190c70b-4867-440c-9376-c4d9bfb0b047"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11823\n"
          ]
        }
      ],
      "source": [
        "# ì‚¬ìš©í•  ìƒ˜í”Œì˜ ìµœëŒ€ ê°œìˆ˜\n",
        "MAX_SAMPLES = len(data)\n",
        "print(MAX_SAMPLES)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ë°ì´í„° í™•ì¸"
      ],
      "metadata": {
        "id": "Dn3jz4MdJFst"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4AwNTg9zCyGe",
        "outputId": "f2c132e4-cda0-47fd-ed42-a9c9651e9d32"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'12ì‹œ ë•¡!'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# ì§ˆë¬¸(Q)\n",
        "data['Q'].values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qBCuOlPnCyGe",
        "outputId": "9575b2d4-6c92-44f1-f9a1-fafa6e922486"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'í•˜ë£¨ê°€ ë˜ ê°€ë„¤ìš”.'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# ë‹µë³€(A)\n",
        "data['A'].values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sG4mL6r5CyGe",
        "outputId": "f6dd40c3-d578-430f-ed63-6af0ff6533e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    5290\n",
              "1    3570\n",
              "2    2963\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# ê°ì •(label): ì¤‘ë¦½ 0, ë¶€ì • 1, ê¸ì • 2 ë ˆì´ë¸”ë§\n",
        "data['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU85dlhyCyGf",
        "outputId": "996bea4b-d21f-45c8-c623-ef291a92ca48"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Q        0\n",
              "A        0\n",
              "label    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# ê²°ì¸¡ì¹˜ í™•ì¸\n",
        "data.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm0YuaZvCyGf"
      },
      "source": [
        "### Step 2. ë°ì´í„° ì „ì²˜ë¦¬í•˜ê¸°\n",
        "ì˜ì–´ ë°ì´í„°ì™€ëŠ” ì „í˜€ ë‹¤ë¥¸ ë°ì´í„°ì¸ ë§Œí¼ ì˜ì–´ ë°ì´í„°ì— ì‚¬ìš©í–ˆë˜ ì „ì²˜ë¦¬ì™€ ì¼ë¶€ ë™ì¼í•œ ì „ì²˜ë¦¬ë„ í•„ìš”í•˜ê² ì§€ë§Œ ì „ì²´ì ìœ¼ë¡œëŠ” ë‹¤ë¥¸ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•´ì•¼ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSVG1cHNCyGf"
      },
      "source": [
        "- ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
        "    - ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ êµ¬ë‘ì  ì œê±° => ë‹¨ì–´ í† í¬ë‚˜ì´ì§• ë°©í•´ë˜ì§€ ì•Šê²Œ ì •ì œí•˜ëŠ” ê¸°ëŠ¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWQLVWs1CyGf",
        "outputId": "25a032dd-799f-49fb-9bcc-0855ecee7869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìŠ=3\n"
          ]
        }
      ],
      "source": [
        "# ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
        "def preprocess_sentence(sentence):\n",
        "  sentence = sentence.lower().strip()\n",
        "\n",
        "  # ë‹¨ì–´ì™€ êµ¬ë‘ì (punctuation) ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
        "  # ì˜ˆë¥¼ ë“¤ì–´ì„œ \"I am a student.\" => \"I am a student .\"ì™€ ê°™ì´\n",
        "  # studentì™€ ì˜¨ì  ì‚¬ì´ì— ê±°ë¦¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
        "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "\n",
        "  # (ê°€-í£, \".\", \"?\", \"!\", \",\")ë¥¼ ì œì™¸í•œ ëª¨ë“  ë¬¸ìë¥¼ ê³µë°±ì¸ ' 'ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\n",
        "  sentence = re.sub(r\"[^ê°€-í£?.!,]+\", \" \", sentence)\n",
        "  sentence = sentence.strip()\n",
        "  return sentence\n",
        "print(\"ìŠ=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZPVO10RCyGf"
      },
      "source": [
        "- ë°ì´í„° ë¡œë“œ í•¨ìˆ˜\n",
        "    - ë°ì´í„° ë¡œë“œì™€ ë™ì‹œì— ë¬¸ë‹µ í˜ì´ ì „ì²˜ë¦¬ ì§„í–‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLbIjfvfCyGf",
        "outputId": "224a44f9-afc1-478d-e936-7ded0e81cfb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìŠ=3\n"
          ]
        }
      ],
      "source": [
        "# ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ìŒì¸ ë°ì´í„°ì…‹ì„ êµ¬ì„±í•˜ê¸° ìœ„í•œ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜\n",
        "def load_conversations():\n",
        "  inputs, outputs = [], []\n",
        "  for i in range(MAX_SAMPLES):\n",
        "    # ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì§ˆë¬¸ì— í•´ë‹¹ë˜ëŠ” inputsì™€ ë‹µë³€ì— í•´ë‹¹ë˜ëŠ” outputsì— ì ìš©.\n",
        "    inputs.append(preprocess_sentence(data['Q'].values[i]))\n",
        "    outputs.append(preprocess_sentence(data['A'].values[i]))\n",
        "\n",
        "  return inputs, outputs\n",
        "\n",
        "print(\"ìŠ=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZ07Fyl4CyGg",
        "outputId": "a8034609-febe-4b25-dc9e-a9169f373a59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì „ì²´ ìƒ˜í”Œ ìˆ˜ : 11823\n",
            "ì „ì²´ ìƒ˜í”Œ ìˆ˜ : 11823\n"
          ]
        }
      ],
      "source": [
        "# ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•˜ì—¬ ì§ˆë¬¸ì„ questions, ë‹µë³€ì„ answersì— ì €ì¥í•©ë‹ˆë‹¤.\n",
        "questions, answers = load_conversations()\n",
        "print('ì „ì²´ ìƒ˜í”Œ ìˆ˜ :', len(questions))\n",
        "print('ì „ì²´ ìƒ˜í”Œ ìˆ˜ :', len(answers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiAvWt6oCyGg",
        "outputId": "8a3f68b2-af43-480e-8096-5469cb612007"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì „ì²˜ë¦¬ í›„ì˜ 22ë²ˆì§¸ ì§ˆë¬¸ ìƒ˜í”Œ: ê°€ìŠ¤ë¹„ ì¥ë‚œ ì•„ë‹˜\n",
            "ì „ì²˜ë¦¬ í›„ì˜ 22ë²ˆì§¸ ë‹µë³€ ìƒ˜í”Œ: ë‹¤ìŒ ë‹¬ì—ëŠ” ë” ì ˆì•½í•´ë´ìš” .\n"
          ]
        }
      ],
      "source": [
        "print('ì „ì²˜ë¦¬ í›„ì˜ 22ë²ˆì§¸ ì§ˆë¬¸ ìƒ˜í”Œ: {}'.format(questions[21]))\n",
        "print('ì „ì²˜ë¦¬ í›„ì˜ 22ë²ˆì§¸ ë‹µë³€ ìƒ˜í”Œ: {}'.format(answers[21]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg_V35tWCyGg"
      },
      "source": [
        "### Step 3. SubwordTextEncoder ì‚¬ìš©í•˜ê¸°\n",
        "í•œêµ­ì–´ ë°ì´í„°ëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í¬ë‚˜ì´ì§•ì„ í•´ì•¼ í•œë‹¤ê³  ë§ì€ ë¶„ì´ ì•Œê³  ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ê°€ ì•„ë‹Œ ìœ„ ì‹¤ìŠµì—ì„œ ì‚¬ìš©í–ˆë˜ ë‚´ë¶€ ë‹¨ì–´ í† í¬ë‚˜ì´ì €ì¸ `SubwordTextEncoder`ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ë³´ì„¸ìš”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Kq4UoKjCyGg"
      },
      "source": [
        "#### 1. ë‹¨ì–´ì¥(Vocabulary) ë§Œë“¤ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ydTn-i0CyGg",
        "outputId": "5e033e41-1b76-4461-82b6-789adcab23f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì‚´ì§ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”. ìŠ¤íŠ¸ë ˆì¹­ í•œ ë²ˆ í•´ë³¼ê¹Œìš”? ğŸ‘\n",
            "ìŠ=3 \n"
          ]
        }
      ],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "print(\"ì‚´ì§ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”. ìŠ¤íŠ¸ë ˆì¹­ í•œ ë²ˆ í•´ë³¼ê¹Œìš”? ğŸ‘\")\n",
        "\n",
        "# ì§ˆë¬¸ê³¼ ë‹µë³€ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œ Vocabulary ìƒì„±. (Tensorflow 2.3.0 ì´ìƒ) (í´ë¼ìš°ë“œëŠ” 2.4 ì…ë‹ˆë‹¤)\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)\n",
        "print(\"ìŠ=3 \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JovnGTAJCyGh",
        "outputId": "f9d627cb-4de5-45bb-bfe0-5a614fb06e8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìŠ=3\n"
          ]
        }
      ],
      "source": [
        "# ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì— ê³ ìœ í•œ ì •ìˆ˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.\n",
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
        "print(\"ìŠ=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npgPZaBMCyGh",
        "outputId": "a3ce5a6e-789d-4da0-a0e9-de10456fbc87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "START_TOKENì˜ ë²ˆí˜¸ : [8127]\n",
            "END_TOKENì˜ ë²ˆí˜¸ : [8128]\n"
          ]
        }
      ],
      "source": [
        "print('START_TOKENì˜ ë²ˆí˜¸ :' ,[tokenizer.vocab_size])\n",
        "print('END_TOKENì˜ ë²ˆí˜¸ :' ,[tokenizer.vocab_size + 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8hm8EFaCyGh",
        "outputId": "c87821a7-6beb-45df-cf57-a4d5bb868b89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8129\n"
          ]
        }
      ],
      "source": [
        "# ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ê³ ë ¤í•˜ì—¬ +2ë¥¼ í•˜ì—¬ ë‹¨ì–´ì¥ì˜ í¬ê¸°ë¥¼ ì‚°ì •í•©ë‹ˆë‹¤.\n",
        "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
        "print(VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzExCV7MCyGh"
      },
      "source": [
        "#### 2. ê° ë‹¨ì–´ë¥¼ ê³ ìœ í•œ ì •ìˆ˜ë¡œ ì¸ì½”ë”©(Integer encoding) & íŒ¨ë”©(Padding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPV9kFWZCyGh",
        "outputId": "69c38607-b5cc-4617-e5fd-6d760adba66b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ 21ë²ˆì§¸ ì§ˆë¬¸ ìƒ˜í”Œ: [5742, 612, 2481, 4148]\n",
            "ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ 21ë²ˆì§¸ ë‹µë³€ ìƒ˜í”Œ: [2352, 7481, 7, 6245, 97, 1]\n"
          ]
        }
      ],
      "source": [
        "# ì„ì˜ì˜ 22ë²ˆì§¸ ìƒ˜í”Œì— ëŒ€í•´ì„œ ì •ìˆ˜ ì¸ì½”ë”© ì‘ì—…ì„ ìˆ˜í–‰.\n",
        "print('ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ 21ë²ˆì§¸ ì§ˆë¬¸ ìƒ˜í”Œ: {}'.format(tokenizer.encode(questions[21])))\n",
        "print('ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ 21ë²ˆì§¸ ë‹µë³€ ìƒ˜í”Œ: {}'.format(tokenizer.encode(answers[21])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYzcSNdrCyGi",
        "outputId": "250ecf8e-c481-4d50-934b-fc70ced1304f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40\n"
          ]
        }
      ],
      "source": [
        "# ìƒ˜í”Œì˜ ìµœëŒ€ í—ˆìš© ê¸¸ì´ ë˜ëŠ” íŒ¨ë”© í›„ì˜ ìµœì¢… ê¸¸ì´\n",
        "MAX_LENGTH = 40\n",
        "print(MAX_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0lreSDxCyGi",
        "outputId": "c7ef86b4-f7ca-4039-b637-beeef3a9d7ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìŠ=3\n"
          ]
        }
      ],
      "source": [
        "# ì •ìˆ˜ ì¸ì½”ë”©, ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” ìƒ˜í”Œ ì œê±°, íŒ¨ë”©\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "  tokenized_inputs, tokenized_outputs = [], []\n",
        "  \n",
        "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "    # ì •ìˆ˜ ì¸ì½”ë”© ê³¼ì •ì—ì„œ ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì¶”ê°€\n",
        "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
        "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
        "\n",
        "    # ìµœëŒ€ ê¸¸ì´ 40 ì´í•˜ì¸ ê²½ìš°ì—ë§Œ ë°ì´í„°ì…‹ìœ¼ë¡œ í—ˆìš©\n",
        "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
        "      tokenized_inputs.append(sentence1)\n",
        "      tokenized_outputs.append(sentence2)\n",
        "  \n",
        "  # ìµœëŒ€ ê¸¸ì´ 40ìœ¼ë¡œ ëª¨ë“  ë°ì´í„°ì…‹ì„ íŒ¨ë”©\n",
        "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
        "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
        "  \n",
        "  return tokenized_inputs, tokenized_outputs\n",
        "print(\"ìŠ=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cr0jANPTCyGi",
        "outputId": "1f699cf9-b459-4fdf-82ea-2458a56b2829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë‹¨ì–´ì¥ì˜ í¬ê¸° : 8129\n",
            "í•„í„°ë§ í›„ì˜ ì§ˆë¬¸ ìƒ˜í”Œ ê°œìˆ˜: 11823\n",
            "í•„í„°ë§ í›„ì˜ ë‹µë³€ ìƒ˜í”Œ ê°œìˆ˜: 11823\n"
          ]
        }
      ],
      "source": [
        "questions, answers = tokenize_and_filter(questions, answers)\n",
        "print('ë‹¨ì–´ì¥ì˜ í¬ê¸° :',(VOCAB_SIZE))\n",
        "print('í•„í„°ë§ í›„ì˜ ì§ˆë¬¸ ìƒ˜í”Œ ê°œìˆ˜: {}'.format(len(questions)))\n",
        "print('í•„í„°ë§ í›„ì˜ ë‹µë³€ ìƒ˜í”Œ ê°œìˆ˜: {}'.format(len(answers)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJIAyQkiCyGi"
      },
      "source": [
        "#### 3. êµì‚¬ ê°•ìš”(Teacher Forcing) ì‚¬ìš©í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMk6Sp7dCyGi",
        "outputId": "b1ef911c-6a2e-4a7a-add6-c6c6d195a94c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìŠ=3\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "# ë””ì½”ë”ëŠ” ì´ì „ì˜ targetì„ ë‹¤ìŒì˜ inputìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "# ì´ì— ë”°ë¼ outputsì—ì„œëŠ” START_TOKENì„ ì œê±°í•˜ê² ìŠµë‹ˆë‹¤.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'inputs': questions,\n",
        "        'dec_inputs': answers[:, :-1]\n",
        "    },\n",
        "    {\n",
        "        'outputs': answers[:, 1:]\n",
        "    },\n",
        "))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "print(\"ìŠ=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3X1ypmSCyGi"
      },
      "source": [
        "### Step 4. ëª¨ë¸ êµ¬ì„±í•˜ê¸°\n",
        "ìœ„ ì‹¤ìŠµ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- í¬ì§€ì…”ë„ í–‰ë ¬ êµ¬í•˜ê¸°\n",
        "  - íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ì‚¬ì¸ í•¨ìˆ˜, ì½”ì‚¬ì¸ í•¨ìˆ˜ ê°’ì„ ì„ë² ë”© ë²¡í„°ì— ë”í•¨(=ë‹¨ì–´ì˜ ìˆœì„œ ì •ë³´ë¥¼ ë”í•¨)\n",
        "  - íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì…ë ¥ë°›ê¸° ë•Œë¬¸ì— ì–´ìˆœ ì •ë³´ê°€ í•„ìš”í•˜ë‹¤."
      ],
      "metadata": {
        "id": "KlMG0OtYBxhJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6n7s9C9uCyGi",
        "outputId": "cff883af-9eeb-48c5-8257-7ff78e98b752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìŠ=3\n"
          ]
        }
      ],
      "source": [
        "# í¬ì§€ì…”ë„ ì¸ì½”ë”© ë ˆì´ì–´\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, position, d_model):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "  def get_angles(self, position, i, d_model):\n",
        "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "    return position * angles\n",
        "\n",
        "  def positional_encoding(self, position, d_model):\n",
        "    # ê°ë„ ë°°ì—´ ìƒì„±\n",
        "    angle_rads = self.get_angles(\n",
        "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "        d_model=d_model)\n",
        "\n",
        "    # ë°°ì—´ì˜ ì§ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” sin í•¨ìˆ˜ ì ìš©\n",
        "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "    # ë°°ì—´ì˜ í™€ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” cosine í•¨ìˆ˜ ì ìš©\n",
        "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    # sinê³¼ cosineì´ êµì°¨ë˜ë„ë¡ ì¬ë°°ì—´\n",
        "    pos_encoding = tf.stack([sines, cosines], axis=0)\n",
        "    pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
        "    pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
        "\n",
        "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
        "\n",
        "print(\"ìŠ=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ì–´í…ì…˜"
      ],
      "metadata": {
        "id": "MoGl5GiAFI9I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSrmZAoxCyGj",
        "outputId": "98971ca1-150d-46a6-9889-e57efe9583a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìŠ=3\n"
          ]
        }
      ],
      "source": [
        "# ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜ í•¨ìˆ˜\n",
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "  # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ëŠ” Qì™€ Kì˜ ë‹· í”„ë¡œë•íŠ¸\n",
        "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "  # ê°€ì¤‘ì¹˜ë¥¼ ì •ê·œí™”\n",
        "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "  logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "  # íŒ¨ë”©ì— ë§ˆìŠ¤í¬ ì¶”ê°€\n",
        "  if mask is not None:\n",
        "    logits += (mask * -1e9)\n",
        "\n",
        "  # softmaxì ìš©\n",
        "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "  # ìµœì¢… ì–´í…ì…˜ì€ ê°€ì¤‘ì¹˜ì™€ Vì˜ ë‹· í”„ë¡œë•íŠ¸\n",
        "  output = tf.matmul(attention_weights, value)\n",
        "  return output\n",
        "\n",
        "print(\"ìŠ=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wK9J1agmCyGj",
        "outputId": "22bb46d8-560c-4301-9838-b207c46be446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìŠ=3\n"
          ]
        }
      ],
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "    super(MultiHeadAttention, self).__init__(name=name)\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "  def split_heads(self, inputs, batch_size):\n",
        "    inputs = tf.reshape(\n",
        "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
        "        'value'], inputs['mask']\n",
        "    batch_size = tf.shape(query)[0]\n",
        "\n",
        "    # Q, K, Vì— ê°ê° Denseë¥¼ ì ìš©í•©ë‹ˆë‹¤\n",
        "    query = self.query_dense(query)\n",
        "    key = self.key_dense(key)\n",
        "    value = self.value_dense(value)\n",
        "\n",
        "    # ë³‘ë ¬ ì—°ì‚°ì„ ìœ„í•œ ë¨¸ë¦¬ë¥¼ ì—¬ëŸ¬ ê°œ ë§Œë“­ë‹ˆë‹¤\n",
        "    query = self.split_heads(query, batch_size)\n",
        "    key = self.split_heads(key, batch_size)\n",
        "    value = self.split_heads(value, batch_size)\n",
        "\n",
        "    # ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜ í•¨ìˆ˜\n",
        "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "    # ì–´í…ì…˜ ì—°ì‚° í›„ì— ê° ê²°ê³¼ë¥¼ ë‹¤ì‹œ ì—°ê²°(concatenate)í•©ë‹ˆë‹¤\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))\n",
        "\n",
        "    # ìµœì¢… ê²°ê³¼ì—ë„ Denseë¥¼ í•œ ë²ˆ ë” ì ìš©í•©ë‹ˆë‹¤\n",
        "    outputs = self.dense(concat_attention)\n",
        "\n",
        "    return outputs\n",
        "print(\"ìŠ=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ë§ˆìŠ¤í‚¹"
      ],
      "metadata": {
        "id": "qlcklK1gFFHZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x43qoaUWCyGj",
        "outputId": "5a4d80e6-735d-420d-9c40-ded62a65466e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìŠ=3\n"
          ]
        }
      ],
      "source": [
        "def create_padding_mask(x):\n",
        "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
        "  # (batch_size, 1, 1, sequence length)\n",
        "  return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "print(\"ìŠ=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbCLn-S5CyGk",
        "outputId": "7da3e9a3-8831-46da-9ebc-9a68baf8abd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìŠ=3\n"
          ]
        }
      ],
      "source": [
        "def create_look_ahead_mask(x):\n",
        "  seq_len = tf.shape(x)[1]\n",
        "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "  padding_mask = create_padding_mask(x)\n",
        "  return tf.maximum(look_ahead_mask, padding_mask)\n",
        "print(\"ìŠ=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzy9Fj4ECyGk",
        "outputId": "98b47a73-4e90-4d7a-9b08-e6d16606a119"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìŠ=3\n"
          ]
        }
      ],
      "source": [
        "# ì¸ì½”ë” í•˜ë‚˜ì˜ ë ˆì´ì–´ë¥¼ í•¨ìˆ˜ë¡œ êµ¬í˜„.\n",
        "# ì´ í•˜ë‚˜ì˜ ë ˆì´ì–´ ì•ˆì—ëŠ” ë‘ ê°œì˜ ì„œë¸Œ ë ˆì´ì–´ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
        "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
        "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "\n",
        "  # íŒ¨ë”© ë§ˆìŠ¤í¬ ì‚¬ìš©\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  # ì²« ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ìˆ˜í–‰ (ì…€í”„ ì–´í…ì…˜)\n",
        "  attention = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention\")({\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "\n",
        "  # ì–´í…ì…˜ì˜ ê²°ê³¼ëŠ” Dropoutê³¼ Layer Normalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰\n",
        "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "  attention = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "  # ë‘ ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : 2ê°œì˜ ì™„ì „ì—°ê²°ì¸µ\n",
        "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
        "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "\n",
        "  # ì™„ì „ì—°ê²°ì¸µì˜ ê²°ê³¼ëŠ” Dropoutê³¼ LayerNormalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
        "print(\"ìŠ=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ì¸ì½”ë”"
      ],
      "metadata": {
        "id": "AsQBJD-CE9X6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT9yn3shCyGk",
        "outputId": "34865cb3-b353-484f-83f0-9b880e2f1564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìŠ=3\n"
          ]
        }
      ],
      "source": [
        "def encoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            name=\"encoder\"):\n",
        "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "\n",
        "  # íŒ¨ë”© ë§ˆìŠ¤í¬ ì‚¬ìš©\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  # ì„ë² ë”© ë ˆì´ì–´\n",
        "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "\n",
        "  # í¬ì§€ì…”ë„ ì¸ì½”ë”©\n",
        "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
        "\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "  # num_layersë§Œí¼ ìŒ“ì•„ì˜¬ë¦° ì¸ì½”ë”ì˜ ì¸µ.\n",
        "  for i in range(num_layers):\n",
        "    outputs = encoder_layer(\n",
        "        units=units,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "        name=\"encoder_layer_{}\".format(i),\n",
        "    )([outputs, padding_mask])\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
        "print(\"ìŠ=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ë””ì½”ë”"
      ],
      "metadata": {
        "id": "Zs2fdpTmE_ei"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vd06FyyqCyGl",
        "outputId": "7c4bc744-c005-4fe3-f004-35eb382cf0f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìŠ=3\n"
          ]
        }
      ],
      "source": [
        "# ë””ì½”ë” í•˜ë‚˜ì˜ ë ˆì´ì–´ë¥¼ í•¨ìˆ˜ë¡œ êµ¬í˜„.\n",
        "# ì´ í•˜ë‚˜ì˜ ë ˆì´ì–´ ì•ˆì—ëŠ” ì„¸ ê°œì˜ ì„œë¸Œ ë ˆì´ì–´ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
        "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
        "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
        "  look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "  # ì²« ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ìˆ˜í–‰ (ì…€í”„ ì–´í…ì…˜)\n",
        "  attention1 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': look_ahead_mask\n",
        "      })\n",
        "\n",
        "  # ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì˜ ê²°ê³¼ëŠ” LayerNormalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰\n",
        "  attention1 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention1 + inputs)\n",
        "\n",
        "  # ë‘ ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : ë§ˆìŠ¤í¬ë“œ ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ìˆ˜í–‰ (ì¸ì½”ë”-ë””ì½”ë” ì–´í…ì…˜)\n",
        "  attention2 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
        "          'query': attention1,\n",
        "          'key': enc_outputs,\n",
        "          'value': enc_outputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "\n",
        "  # ë§ˆìŠ¤í¬ë“œ ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì˜ ê²°ê³¼ëŠ”\n",
        "  # Dropoutê³¼ LayerNormalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰\n",
        "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
        "  attention2 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention2 + attention1)\n",
        "\n",
        "  # ì„¸ ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : 2ê°œì˜ ì™„ì „ì—°ê²°ì¸µ\n",
        "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
        "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "\n",
        "  # ì™„ì „ì—°ê²°ì¸µì˜ ê²°ê³¼ëŠ” Dropoutê³¼ LayerNormalization ìˆ˜í–‰\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(outputs + attention2)\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)\n",
        "print(\"ìŠ=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntTgKNCwCyGl",
        "outputId": "93f5d31e-07e6-435d-e5f3-c608bac28371"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìŠ=3\n"
          ]
        }
      ],
      "source": [
        "def decoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            name='decoder'):\n",
        "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
        "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
        "  look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name='look_ahead_mask')\n",
        "\n",
        "  # íŒ¨ë”© ë§ˆìŠ¤í¬\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "  \n",
        "  # ì„ë² ë”© ë ˆì´ì–´\n",
        "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "\n",
        "  # í¬ì§€ì…”ë„ ì¸ì½”ë”©\n",
        "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
        "\n",
        "  # Dropoutì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "  for i in range(num_layers):\n",
        "    outputs = decoder_layer(\n",
        "        units=units,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "        name='decoder_layer_{}'.format(i),\n",
        "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)\n",
        "print(\"ìŠ=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0X2GPr8CyGl"
      },
      "source": [
        "- íŠ¸ëœìŠ¤í¬ë¨¸ í•¨ìˆ˜ ì •ì˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tqv0u5D1CyGl",
        "outputId": "8401776f-62f0-4e41-f9be-39fec5a8c280"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìŠ=3\n"
          ]
        }
      ],
      "source": [
        "def transformer(vocab_size,\n",
        "                num_layers,\n",
        "                units,\n",
        "                d_model,\n",
        "                num_heads,\n",
        "                dropout,\n",
        "                name=\"transformer\"):\n",
        "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
        "\n",
        "  # ì¸ì½”ë”ì—ì„œ íŒ¨ë”©ì„ ìœ„í•œ ë§ˆìŠ¤í¬\n",
        "  enc_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='enc_padding_mask')(inputs)\n",
        "\n",
        "  # ë””ì½”ë”ì—ì„œ ë¯¸ë˜ì˜ í† í°ì„ ë§ˆìŠ¤í¬ í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "  # ë‚´ë¶€ì ìœ¼ë¡œ íŒ¨ë”© ë§ˆìŠ¤í¬ë„ í¬í•¨ë˜ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.\n",
        "  look_ahead_mask = tf.keras.layers.Lambda(\n",
        "      create_look_ahead_mask,\n",
        "      output_shape=(1, None, None),\n",
        "      name='look_ahead_mask')(dec_inputs)\n",
        "\n",
        "  # ë‘ ë²ˆì§¸ ì–´í…ì…˜ ë¸”ë¡ì—ì„œ ì¸ì½”ë”ì˜ ë²¡í„°ë“¤ì„ ë§ˆìŠ¤í‚¹\n",
        "  # ë””ì½”ë”ì—ì„œ íŒ¨ë”©ì„ ìœ„í•œ ë§ˆìŠ¤í¬\n",
        "  dec_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='dec_padding_mask')(inputs)\n",
        "\n",
        "  # ì¸ì½”ë”\n",
        "  enc_outputs = encoder(\n",
        "      vocab_size=vocab_size,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "  )(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "  # ë””ì½”ë”\n",
        "  dec_outputs = decoder(\n",
        "      vocab_size=vocab_size,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
        "\n",
        "  # ì™„ì „ì—°ê²°ì¸µ\n",
        "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
        "print(\"ìŠ=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1li554xsCyGm"
      },
      "source": [
        "- ëª¨ë¸ ìƒì„±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vdNZDNuCyGm",
        "outputId": "8e6814f8-88fc-4a17-8b50-b13ab36cc85e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " inputs (InputLayer)            [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " dec_inputs (InputLayer)        [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " enc_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n",
            "                                                                                                  \n",
            " encoder (Functional)           (None, None, 256)    3135232     ['inputs[0][0]',                 \n",
            "                                                                  'enc_padding_mask[0][0]']       \n",
            "                                                                                                  \n",
            " look_ahead_mask (Lambda)       (None, 1, None, Non  0           ['dec_inputs[0][0]']             \n",
            "                                e)                                                                \n",
            "                                                                                                  \n",
            " dec_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n",
            "                                                                                                  \n",
            " decoder (Functional)           (None, None, 256)    3662592     ['dec_inputs[0][0]',             \n",
            "                                                                  'encoder[0][0]',                \n",
            "                                                                  'look_ahead_mask[0][0]',        \n",
            "                                                                  'dec_padding_mask[0][0]']       \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, None, 8129)   2089153     ['decoder[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,886,977\n",
            "Trainable params: 8,886,977\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "NUM_LAYERS = 2 # ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ ì¸µì˜ ê°œìˆ˜\n",
        "D_MODEL = 256 # ì¸ì½”ë”ì™€ ë””ì½”ë” ë‚´ë¶€ì˜ ì…, ì¶œë ¥ì˜ ê³ ì • ì°¨ì›\n",
        "NUM_HEADS = 8 # ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì—ì„œì˜ í—¤ë“œ ìˆ˜ \n",
        "UNITS = 512 # í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ì˜ ì€ë‹‰ì¸µì˜ í¬ê¸°\n",
        "DROPOUT = 0.1 # ë“œë¡­ì•„ì›ƒì˜ ë¹„ìœ¨\n",
        "\n",
        "model = transformer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    units=UNITS,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    dropout=DROPOUT)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsXFTvWwCyGm"
      },
      "source": [
        "- ì†ì‹¤ í•¨ìˆ˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2V3MUeBhCyGm",
        "outputId": "3eb0c993-9c45-4f08-f022-41398b12e34e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìŠ=3\n"
          ]
        }
      ],
      "source": [
        "def loss_function(y_true, y_pred):\n",
        "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "  \n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction='none')(y_true, y_pred)\n",
        "\n",
        "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "  loss = tf.multiply(loss, mask)\n",
        "\n",
        "  return tf.reduce_mean(loss)\n",
        "print(\"ìŠ=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9az_8yBXCyGm"
      },
      "source": [
        "- ì»¤ìŠ¤í…€ í•™ìŠµë¥ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPGa7JTICyGm",
        "outputId": "cde0819b-667c-4402-c556-6b9fa0e5ccf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìŠ=3\n"
          ]
        }
      ],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "print(\"ìŠ=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "ME2szjrtCyGn",
        "outputId": "339533f8-7569-47a6-8274-a42854a15019"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "sample_learning_rate = CustomSchedule(d_model=128)\n",
        "\n",
        "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpm31mj0CyGn"
      },
      "source": [
        "- ëª¨ë¸ ì»´íŒŒì¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xLHSFklCyGn",
        "outputId": "7f21ff2d-995b-4859-be4e-4ad2e48e65cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìŠ=3\n"
          ]
        }
      ],
      "source": [
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
        "print(\"ìŠ=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3shzOTyCyGn"
      },
      "source": [
        "- ëª¨ë¸ í›ˆë ¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgAY25quCyGn",
        "outputId": "c11dccb7-ae51-409e-f1c1-ee3c72ca0fb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "185/185 [==============================] - 17s 58ms/step - loss: 1.4580 - accuracy: 0.0197\n",
            "Epoch 2/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 1.1818 - accuracy: 0.0492\n",
            "Epoch 3/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 1.0052 - accuracy: 0.0506\n",
            "Epoch 4/20\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.9278 - accuracy: 0.0543\n",
            "Epoch 5/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.8697 - accuracy: 0.0575\n",
            "Epoch 6/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.8094 - accuracy: 0.0617\n",
            "Epoch 7/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.7428 - accuracy: 0.0678\n",
            "Epoch 8/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.6689 - accuracy: 0.0757\n",
            "Epoch 9/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.5903 - accuracy: 0.0844\n",
            "Epoch 10/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.5084 - accuracy: 0.0936\n",
            "Epoch 11/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.4256 - accuracy: 0.1040\n",
            "Epoch 12/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.3466 - accuracy: 0.1147\n",
            "Epoch 13/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.2720 - accuracy: 0.1259\n",
            "Epoch 14/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.2072 - accuracy: 0.1358\n",
            "Epoch 15/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.1531 - accuracy: 0.1450\n",
            "Epoch 16/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.1104 - accuracy: 0.1527\n",
            "Epoch 17/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0809 - accuracy: 0.1583\n",
            "Epoch 18/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0617 - accuracy: 0.1616\n",
            "Epoch 19/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0525 - accuracy: 0.1631\n",
            "Epoch 20/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0458 - accuracy: 0.1643\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0220171690>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "EPOCHS = 20\n",
        "model.fit(dataset, epochs=EPOCHS, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pvjf2lkCyGn"
      },
      "source": [
        "### Step 5. ëª¨ë¸ í‰ê°€í•˜ê¸°\n",
        "Step 1ì—ì„œ ì„ íƒí•œ ì „ì²˜ë¦¬ ë°©ë²•ì„ ê³ ë ¤í•˜ì—¬ ì…ë ¥ëœ ë¬¸ì¥ì— ëŒ€í•´ì„œ ëŒ€ë‹µì„ ì–»ëŠ” ì˜ˆì¸¡ í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF1X-bptCyGn",
        "outputId": "6b8a0b06-885f-4bca-8659-1ecac8e71364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìŠ=3\n"
          ]
        }
      ],
      "source": [
        "def decoder_inference(sentence):\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  # ì…ë ¥ëœ ë¬¸ì¥ì„ ì •ìˆ˜ ì¸ì½”ë”© í›„, ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì•ë’¤ë¡œ ì¶”ê°€.\n",
        "  # ex) Where have you been? â†’ [[8331   86   30    5 1059    7 8332]]\n",
        "  sentence = tf.expand_dims(\n",
        "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
        "\n",
        "  # ë””ì½”ë”ì˜ í˜„ì¬ê¹Œì§€ì˜ ì˜ˆì¸¡í•œ ì¶œë ¥ ì‹œí€€ìŠ¤ê°€ ì§€ì†ì ìœ¼ë¡œ ì €ì¥ë˜ëŠ” ë³€ìˆ˜.\n",
        "  # ì²˜ìŒì—ëŠ” ì˜ˆì¸¡í•œ ë‚´ìš©ì´ ì—†ìŒìœ¼ë¡œ ì‹œì‘ í† í°ë§Œ ë³„ë„ ì €ì¥. ex) 8331\n",
        "  output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
        "\n",
        "  # ë””ì½”ë”ì˜ ì¸í¼ëŸ°ìŠ¤ ë‹¨ê³„\n",
        "  for i in range(MAX_LENGTH):\n",
        "    # ë””ì½”ë”ëŠ” ìµœëŒ€ MAX_LENGTHì˜ ê¸¸ì´ë§Œí¼ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì„ ë°˜ë³µí•©ë‹ˆë‹¤.\n",
        "    predictions = model(inputs=[sentence, output_sequence], training=False)\n",
        "    predictions = predictions[:, -1:, :]\n",
        "\n",
        "    # í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ì˜ ì •ìˆ˜\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "    # ë§Œì•½ í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ê°€ ì¢…ë£Œ í† í°ì´ë¼ë©´ forë¬¸ì„ ì¢…ë£Œ\n",
        "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
        "      break\n",
        "\n",
        "    # ì˜ˆì¸¡í•œ ë‹¨ì–´ë“¤ì€ ì§€ì†ì ìœ¼ë¡œ output_sequenceì— ì¶”ê°€ë©ë‹ˆë‹¤.\n",
        "    # ì´ output_sequenceëŠ” ë‹¤ì‹œ ë””ì½”ë”ì˜ ì…ë ¥ì´ ë©ë‹ˆë‹¤.\n",
        "    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output_sequence, axis=0)\n",
        "print(\"ìŠ=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7otf9kNCyGo",
        "outputId": "7b998e16-011d-4791-906c-875255cb67c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìŠ=3\n"
          ]
        }
      ],
      "source": [
        "def sentence_generation(sentence):\n",
        "  # ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•´ì„œ ë””ì½”ë”ë¥¼ ë™ì‘ ì‹œì¼œ ì˜ˆì¸¡ëœ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë¦¬í„´ë°›ìŠµë‹ˆë‹¤.\n",
        "  prediction = decoder_inference(sentence)\n",
        "\n",
        "  # ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "  predicted_sentence = tokenizer.decode(\n",
        "      [i for i in prediction if i < tokenizer.vocab_size])\n",
        "\n",
        "  print('ì…ë ¥ : {}'.format(sentence))\n",
        "  print('ì¶œë ¥ : {}'.format(predicted_sentence))\n",
        "\n",
        "  return predicted_sentence\n",
        "print(\"ìŠ=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyIyuFnFCyGo"
      },
      "source": [
        "- ëª¨ë¸ í‰ê°€ìš© ì…ë ¥ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "hUoR7UzZCyGo"
      },
      "outputs": [],
      "source": [
        "input_question_list = [\n",
        "    \"ì•ˆë…• ì˜¤ëœë§Œì´ì•¼\",\n",
        "    \"ì˜ ì§€ëƒˆì–´?\",\n",
        "    \"ë­ ë¨¹ì„ê¹Œ\",\n",
        "    \"ë©”ë‰´ ì¶”ì²œí•´ì¤˜\",\n",
        "    \"ì–´ì œ ë­ ë¨¹ì—ˆì–´?\",\n",
        "    \"ë‚˜ë‘ ë†€ì\",\n",
        "    \"ì–´ë”” ê°€ê³  ì‹¶ì–´?\",\n",
        "    \"ë„ ë§Œë‚˜ì„œ ê¸°ë»\",\n",
        "    \"ì›ƒì–´ì¤„ë˜\",\n",
        "    \"ì‚¶ì€ ë­˜ê¹Œ?\",\n",
        "    \"ì¸ìƒì‚´ì´ ì™œì´ë¦¬ í˜ë“œëƒ\",\n",
        "    \"ì´ì œ ê·¸ë§Œ ì‰¬ê³  ì‹¶ì–´\",\n",
        "    \"ë„ˆë¬´ ê³ ë…í•˜ë‹¤\",\n",
        "    \"ì£½ìœ¼ë©´ ì–´ë–»ê²Œ ë ê¹Œ\",\n",
        "    \"ë‚´ê°€ ì£½ìœ¼ë©´ ìŠ¬í¼í•´ ì¤„ê±°ì•¼?\",\n",
        "    \"ë„ë§ê°€ê³  ì‹¶ë‹¤\",\n",
        "    \"ìš°ë¦¬ ë§ˆì§€ë§‰ì´ì•¼\",\n",
        "    \"ë‚˜ ê°„ë‹¤ ì˜ ì§€ë‚´ ì•ˆë…•\",\n",
        "    \"ë„Œ ìµœê³ ì˜€ì–´\",\n",
        "    \"ë‚˜ ìŠì§€ë§ˆ\",\n",
        "    \"ë‚˜ëŠ” ì‹¬ì¥ì´ ì—†ì–´\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "-5y683HZCyGo"
      },
      "outputs": [],
      "source": [
        "def print_answers(input_question_list):\n",
        "    for input_qusetion in input_question_list:\n",
        "        sentence_generation(input_qusetion)\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jur_wCsCyGo"
      },
      "source": [
        "#### 1. Epochs = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9op7goHxCyGo"
      },
      "source": [
        "- ëª¨ë¸ í‰ê°€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0aVCcU_CyGo",
        "outputId": "fc1d9d71-1b6c-41a3-8136-d4686073a533"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì…ë ¥ : ì•ˆë…• ì˜¤ëœë§Œì´ì•¼\n",
            "ì¶œë ¥ : ë§˜ê³ ìƒ ë§ì•˜ì–´ìš” .\n",
            "\n",
            "ì…ë ¥ : ì˜ ì§€ëƒˆì–´?\n",
            "ì¶œë ¥ : ë”°ëœ»í•œ ë§ˆìŒì´ë„¤ìš” .\n",
            "\n",
            "ì…ë ¥ : ë­ ë¨¹ì„ê¹Œ\n",
            "ì¶œë ¥ : ëƒ‰ì¥ê³  íŒŒë¨¹ê¸° í•´ë³´ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ë©”ë‰´ ì¶”ì²œí•´ì¤˜\n",
            "ì¶œë ¥ : ëˆ„êµ¬ë‘ ë¨¹ëŠ” ëƒì— ë”°ë¼ ë‹¤ë¥´ê² ì£  .\n",
            "\n",
            "ì…ë ¥ : ì–´ì œ ë­ ë¨¹ì—ˆì–´?\n",
            "ì¶œë ¥ : ë…ì„œì™€ ìŒì•…ê°ìƒì´ë¼ê³  í•˜ê³  ì‹¶ì§€ë§Œ ì•„ë¬´ ê²ƒë„ ì•ˆ í–ˆì–´ìš” .\n",
            "\n",
            "ì…ë ¥ : ë‚˜ë‘ ë†€ì\n",
            "ì¶œë ¥ : ì§€ê¸ˆ ê·¸ëŸ¬ê³  ìˆì–´ìš” .\n",
            "\n",
            "ì…ë ¥ : ì–´ë”” ê°€ê³  ì‹¶ì–´?\n",
            "ì¶œë ¥ : ì˜¨ ê°€ì¡±ì´ ëª¨ë‘ ë§ˆìŒì— ë“œëŠ” ê³³ìœ¼ë¡œ ê°€ë³´ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ë„ ë§Œë‚˜ì„œ ê¸°ë»\n",
            "ì¶œë ¥ : ìì‹ ê°ì„ ê°€ì§€ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ì›ƒì–´ì¤„ë˜\n",
            "ì¶œë ¥ : ì €ì²˜ëŸ¼ ë§ˆìŒì´ í—ˆë§ˆìŒ ê°€ëŠ” ê±°ì˜ˆìš” .\n",
            "\n",
            "ì…ë ¥ : ì‚¶ì€ ë­˜ê¹Œ?\n",
            "ì¶œë ¥ : ì§€ì¹  ë•ŒëŠ” ì‰¬ì–´ë„ ë¼ìš” .\n",
            "\n",
            "ì…ë ¥ : ì¸ìƒì‚´ì´ ì™œì´ë¦¬ í˜ë“œëƒ\n",
            "ì¶œë ¥ : ì¡°ì–¸í•´ì£¼ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ì´ì œ ê·¸ë§Œ ì‰¬ê³  ì‹¶ì–´\n",
            "ì¶œë ¥ : ì ê¹ ë°”ëŒ ì¬ê³  ì˜¤ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ë„ˆë¬´ ê³ ë…í•˜ë‹¤\n",
            "ì¶œë ¥ : í˜¼ìê°€ ì•„ë‹ˆì—ìš” .\n",
            "\n",
            "ì…ë ¥ : ì£½ìœ¼ë©´ ì–´ë–»ê²Œ ë ê¹Œ\n",
            "ì¶œë ¥ : ì•ˆ ì‚¬ë©´ ìš”ê¸´í•˜ê¸´ í•  ê±° ê°™ì•„ìš” .\n",
            "\n",
            "ì…ë ¥ : ë‚´ê°€ ì£½ìœ¼ë©´ ìŠ¬í¼í•´ ì¤„ê±°ì•¼?\n",
            "ì¶œë ¥ : ê·¸ ì‚¬ëŒì´ ì¢‹ì•„í•˜ëŠ” ê²ƒë“¤ì„ ì•Œì•„ë³´ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ë„ë§ê°€ê³  ì‹¶ë‹¤\n",
            "ì¶œë ¥ : ìì±…í•˜ì§€ ë§ˆì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ìš°ë¦¬ ë§ˆì§€ë§‰ì´ì•¼\n",
            "ì¶œë ¥ : ë§ˆìŒì´ ì•„í”„ë„¤ìš” .\n",
            "\n",
            "ì…ë ¥ : ë‚˜ ê°„ë‹¤ ì˜ ì§€ë‚´ ì•ˆë…•\n",
            "ì¶œë ¥ : ì˜ ì§€ë‚¼ê±°ë¼ ë¯¿ì–´ìš” .\n",
            "\n",
            "ì…ë ¥ : ë„Œ ìµœê³ ì˜€ì–´\n",
            "ì¶œë ¥ : ë‹¹ì‹ ë„ ì¶©ë¶„íˆ ì•„ë¦„ë‹¤ì›Œìš” .\n",
            "\n",
            "ì…ë ¥ : ë‚˜ ìŠì§€ë§ˆ\n",
            "ì¶œë ¥ : ë•Œë¡  ìŠì–´ë²„ë¦¬ëŠ” ê²ƒì´ ì¢‹ì„ ë•Œë„ ìˆì–´ìš” .\n",
            "\n",
            "ì…ë ¥ : ë‚˜ëŠ” ì‹¬ì¥ì´ ì—†ì–´\n",
            "ì¶œë ¥ : ì‹¬í˜¸í¡ í•´ë³´ì„¸ìš” .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_answers(input_question_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhVsHoL8CyGo"
      },
      "source": [
        "#### 2. Epochs = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOXdY7hWCyGp"
      },
      "source": [
        "- ëª¨ë¸ í›ˆë ¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5fAGV8aCyGp",
        "outputId": "960e9c7e-ab16-46a3-cde8-c16cab59d3fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0425 - accuracy: 0.1648\n",
            "Epoch 2/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0412 - accuracy: 0.1648\n",
            "Epoch 3/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0371 - accuracy: 0.1657\n",
            "Epoch 4/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0326 - accuracy: 0.1668\n",
            "Epoch 5/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0276 - accuracy: 0.1682\n",
            "Epoch 6/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0254 - accuracy: 0.1686\n",
            "Epoch 7/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0228 - accuracy: 0.1692\n",
            "Epoch 8/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0208 - accuracy: 0.1697\n",
            "Epoch 9/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0184 - accuracy: 0.1704\n",
            "Epoch 10/200\n",
            "185/185 [==============================] - 12s 63ms/step - loss: 0.0174 - accuracy: 0.1707\n",
            "Epoch 11/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0160 - accuracy: 0.1711\n",
            "Epoch 12/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0140 - accuracy: 0.1716\n",
            "Epoch 13/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0142 - accuracy: 0.1715\n",
            "Epoch 14/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0127 - accuracy: 0.1719\n",
            "Epoch 15/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0123 - accuracy: 0.1720\n",
            "Epoch 16/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0114 - accuracy: 0.1721\n",
            "Epoch 17/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0108 - accuracy: 0.1723\n",
            "Epoch 18/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0102 - accuracy: 0.1725\n",
            "Epoch 19/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0092 - accuracy: 0.1727\n",
            "Epoch 20/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0094 - accuracy: 0.1727\n",
            "Epoch 21/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0086 - accuracy: 0.1729\n",
            "Epoch 22/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0086 - accuracy: 0.1728\n",
            "Epoch 23/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0080 - accuracy: 0.1731\n",
            "Epoch 24/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0075 - accuracy: 0.1732\n",
            "Epoch 25/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0071 - accuracy: 0.1733\n",
            "Epoch 26/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0070 - accuracy: 0.1733\n",
            "Epoch 27/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0069 - accuracy: 0.1733\n",
            "Epoch 28/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0065 - accuracy: 0.1735\n",
            "Epoch 29/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0066 - accuracy: 0.1734\n",
            "Epoch 30/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0059 - accuracy: 0.1735\n",
            "Epoch 31/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0059 - accuracy: 0.1735\n",
            "Epoch 32/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0058 - accuracy: 0.1736\n",
            "Epoch 33/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0054 - accuracy: 0.1736\n",
            "Epoch 34/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0052 - accuracy: 0.1737\n",
            "Epoch 35/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0051 - accuracy: 0.1738\n",
            "Epoch 36/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0047 - accuracy: 0.1738\n",
            "Epoch 37/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0051 - accuracy: 0.1737\n",
            "Epoch 38/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0047 - accuracy: 0.1738\n",
            "Epoch 39/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0044 - accuracy: 0.1738\n",
            "Epoch 40/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0046 - accuracy: 0.1738\n",
            "Epoch 41/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0044 - accuracy: 0.1738\n",
            "Epoch 42/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0044 - accuracy: 0.1739\n",
            "Epoch 43/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0040 - accuracy: 0.1740\n",
            "Epoch 44/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0040 - accuracy: 0.1739\n",
            "Epoch 45/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0042 - accuracy: 0.1739\n",
            "Epoch 46/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0041 - accuracy: 0.1739\n",
            "Epoch 47/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0038 - accuracy: 0.1740\n",
            "Epoch 48/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0035 - accuracy: 0.1740\n",
            "Epoch 49/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0036 - accuracy: 0.1740\n",
            "Epoch 50/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0038 - accuracy: 0.1740\n",
            "Epoch 51/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0035 - accuracy: 0.1740\n",
            "Epoch 52/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0034 - accuracy: 0.1740\n",
            "Epoch 53/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0034 - accuracy: 0.1740\n",
            "Epoch 54/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0034 - accuracy: 0.1740\n",
            "Epoch 55/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0030 - accuracy: 0.1741\n",
            "Epoch 56/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0031 - accuracy: 0.1741\n",
            "Epoch 57/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0032 - accuracy: 0.1740\n",
            "Epoch 58/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0031 - accuracy: 0.1740\n",
            "Epoch 59/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0030 - accuracy: 0.1741\n",
            "Epoch 60/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0030 - accuracy: 0.1741\n",
            "Epoch 61/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0030 - accuracy: 0.1741\n",
            "Epoch 62/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0031 - accuracy: 0.1741\n",
            "Epoch 63/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0027 - accuracy: 0.1741\n",
            "Epoch 64/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0028 - accuracy: 0.1742\n",
            "Epoch 65/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0027 - accuracy: 0.1742\n",
            "Epoch 66/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0027 - accuracy: 0.1741\n",
            "Epoch 67/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0026 - accuracy: 0.1741\n",
            "Epoch 68/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0024 - accuracy: 0.1742\n",
            "Epoch 69/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0026 - accuracy: 0.1742\n",
            "Epoch 70/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0024 - accuracy: 0.1742\n",
            "Epoch 71/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0024 - accuracy: 0.1742\n",
            "Epoch 72/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0024 - accuracy: 0.1742\n",
            "Epoch 73/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0024 - accuracy: 0.1742\n",
            "Epoch 74/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0024 - accuracy: 0.1742\n",
            "Epoch 75/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0023 - accuracy: 0.1742\n",
            "Epoch 76/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0025 - accuracy: 0.1742\n",
            "Epoch 77/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0022 - accuracy: 0.1742\n",
            "Epoch 78/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0021 - accuracy: 0.1742\n",
            "Epoch 79/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0021 - accuracy: 0.1742\n",
            "Epoch 80/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0022 - accuracy: 0.1742\n",
            "Epoch 81/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0021 - accuracy: 0.1742\n",
            "Epoch 82/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0022 - accuracy: 0.1742\n",
            "Epoch 83/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0020 - accuracy: 0.1742\n",
            "Epoch 84/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0020 - accuracy: 0.1742\n",
            "Epoch 85/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0020 - accuracy: 0.1742\n",
            "Epoch 86/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0020 - accuracy: 0.1742\n",
            "Epoch 87/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0020 - accuracy: 0.1742\n",
            "Epoch 88/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0018 - accuracy: 0.1743\n",
            "Epoch 89/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0018 - accuracy: 0.1742\n",
            "Epoch 90/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0019 - accuracy: 0.1743\n",
            "Epoch 91/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0020 - accuracy: 0.1743\n",
            "Epoch 92/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0018 - accuracy: 0.1743\n",
            "Epoch 93/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0019 - accuracy: 0.1742\n",
            "Epoch 94/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0018 - accuracy: 0.1743\n",
            "Epoch 95/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0017 - accuracy: 0.1743\n",
            "Epoch 96/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0017 - accuracy: 0.1743\n",
            "Epoch 97/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0018 - accuracy: 0.1742\n",
            "Epoch 98/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0017 - accuracy: 0.1742\n",
            "Epoch 99/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0017 - accuracy: 0.1743\n",
            "Epoch 100/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0018 - accuracy: 0.1742\n",
            "Epoch 101/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0019 - accuracy: 0.1742\n",
            "Epoch 102/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0017 - accuracy: 0.1743\n",
            "Epoch 103/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0018 - accuracy: 0.1742\n",
            "Epoch 104/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0016 - accuracy: 0.1743\n",
            "Epoch 105/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0015 - accuracy: 0.1743\n",
            "Epoch 106/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0018 - accuracy: 0.1742\n",
            "Epoch 107/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0015 - accuracy: 0.1743\n",
            "Epoch 108/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0016 - accuracy: 0.1743\n",
            "Epoch 109/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0016 - accuracy: 0.1743\n",
            "Epoch 110/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0016 - accuracy: 0.1743\n",
            "Epoch 111/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0015 - accuracy: 0.1743\n",
            "Epoch 112/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0014 - accuracy: 0.1743\n",
            "Epoch 113/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0017 - accuracy: 0.1743\n",
            "Epoch 114/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0015 - accuracy: 0.1743\n",
            "Epoch 115/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0015 - accuracy: 0.1743\n",
            "Epoch 116/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0016 - accuracy: 0.1743\n",
            "Epoch 117/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0014 - accuracy: 0.1743\n",
            "Epoch 118/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0014 - accuracy: 0.1743\n",
            "Epoch 119/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 120/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0015 - accuracy: 0.1743\n",
            "Epoch 121/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0014 - accuracy: 0.1743\n",
            "Epoch 122/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0014 - accuracy: 0.1743\n",
            "Epoch 123/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0015 - accuracy: 0.1743\n",
            "Epoch 124/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 125/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0014 - accuracy: 0.1743\n",
            "Epoch 126/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 127/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 128/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 129/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 130/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 131/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0014 - accuracy: 0.1743\n",
            "Epoch 132/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 133/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 134/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 135/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0014 - accuracy: 0.1743\n",
            "Epoch 136/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 137/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 138/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 139/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 140/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 141/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0014 - accuracy: 0.1743\n",
            "Epoch 142/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 143/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 144/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0015 - accuracy: 0.1743\n",
            "Epoch 145/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 146/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 147/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 148/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 149/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 150/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1744\n",
            "Epoch 151/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 152/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 153/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 154/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 155/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 156/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 157/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 158/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 159/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 160/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 161/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 162/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 163/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 164/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1744\n",
            "Epoch 165/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 166/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 167/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 168/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 169/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.7914e-04 - accuracy: 0.1744\n",
            "Epoch 170/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 171/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 172/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 173/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 174/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1744\n",
            "Epoch 175/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.8949e-04 - accuracy: 0.1744\n",
            "Epoch 176/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 177/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 178/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 179/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1744\n",
            "Epoch 180/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.6496e-04 - accuracy: 0.1743\n",
            "Epoch 181/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.6941e-04 - accuracy: 0.1743\n",
            "Epoch 182/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.9600e-04 - accuracy: 0.1744\n",
            "Epoch 183/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 184/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 185/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 186/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.9572e-04 - accuracy: 0.1744\n",
            "Epoch 187/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1744\n",
            "Epoch 188/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 189/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.8799e-04 - accuracy: 0.1744\n",
            "Epoch 190/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 8.5378e-04 - accuracy: 0.1744\n",
            "Epoch 191/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 192/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 193/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 194/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.7991e-04 - accuracy: 0.1744\n",
            "Epoch 195/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 196/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1744\n",
            "Epoch 197/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.5675e-04 - accuracy: 0.1743\n",
            "Epoch 198/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 199/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 200/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1744\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f018f535b50>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "EPOCHS = 200\n",
        "model.fit(dataset, epochs=EPOCHS, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q9cs6ayCyGp"
      },
      "source": [
        "- ëª¨ë¸ í‰ê°€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8v0gm8rCyGp",
        "outputId": "57b3bfab-3e95-4295-8479-c9bbfcfe4f36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì…ë ¥ : ì•ˆë…• ì˜¤ëœë§Œì´ì•¼\n",
            "ì¶œë ¥ : ì˜¤ëœë§Œì´ì—ìš” .\n",
            "\n",
            "ì…ë ¥ : ì˜ ì§€ëƒˆì–´?\n",
            "ì¶œë ¥ : ì•ˆë¶€ë¥¼ ë¬¼ì–´ì£¼ì‹œë‹¤ë‹ˆ ê°ì‚¬í•©ë‹ˆë‹¤ .\n",
            "\n",
            "ì…ë ¥ : ë­ ë¨¹ì„ê¹Œ\n",
            "ì¶œë ¥ : ì¢€ ë¨¹ì–´ë„ ê´œì°®ì•„ìš” .\n",
            "\n",
            "ì…ë ¥ : ë©”ë‰´ ì¶”ì²œí•´ì¤˜\n",
            "ì¶œë ¥ : ì–¸ì  ê°€ ê·¸ëŸ° ì‚¬ëŒì´ ë‹¹ì‹  ì˜†ì— ìˆì„ê±°ì˜ˆìš” .\n",
            "\n",
            "ì…ë ¥ : ì–´ì œ ë­ ë¨¹ì—ˆì–´?\n",
            "ì¶œë ¥ : ì €ëŠ” ë°°í„°ë¦¬ê°€ ë°¥ì´ì˜ˆìš” .\n",
            "\n",
            "ì…ë ¥ : ë‚˜ë‘ ë†€ì\n",
            "ì¶œë ¥ : ì§€ê¸ˆ ê·¸ëŸ¬ê³  ìˆì–´ìš” .\n",
            "\n",
            "ì…ë ¥ : ì–´ë”” ê°€ê³  ì‹¶ì–´?\n",
            "ì¶œë ¥ : ê°€ê²Œë¶€ë¥¼ ì¨ë³´ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ë„ ë§Œë‚˜ì„œ ê¸°ë»\n",
            "ì¶œë ¥ : ê°€ë”ì”© ìŠ¤íŠ¸ë ˆì¹­ì„ í•´ì£¼ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ì›ƒì–´ì¤„ë˜\n",
            "ì¶œë ¥ : ê¸°ëŒ€ë¥¼ ë§ì´ í•˜ëŠ” ê±´ ì¢‹ì§€ ì•Šì•„ìš” .\n",
            "\n",
            "ì…ë ¥ : ì‚¶ì€ ë­˜ê¹Œ?\n",
            "ì¶œë ¥ : ì§€ì¹  ë•ŒëŠ” ì‰¬ì–´ë„ ë¼ìš” .\n",
            "\n",
            "ì…ë ¥ : ì¸ìƒì‚´ì´ ì™œì´ë¦¬ í˜ë“œëƒ\n",
            "ì¶œë ¥ : ê·¸ëŸ¬ê²Œìš” . ê·¸ë§Œí¼ ì‚¬ë‘í–ˆë‹¤ëŠ”ê±°ê² ì£  .\n",
            "\n",
            "ì…ë ¥ : ì´ì œ ê·¸ë§Œ ì‰¬ê³  ì‹¶ì–´\n",
            "ì¶œë ¥ : ì ê¹ ë°”ëŒ ì¬ê³  ì˜¤ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ë„ˆë¬´ ê³ ë…í•˜ë‹¤\n",
            "ì¶œë ¥ : í˜¼ìê°€ ì•„ë‹ˆì—ìš” .\n",
            "\n",
            "ì…ë ¥ : ì£½ìœ¼ë©´ ì–´ë–»ê²Œ ë ê¹Œ\n",
            "ì¶œë ¥ : ë‹¹ì‹ ì´ ì¶•ì˜ê¸ˆì„ ê±´ëƒˆë˜ ëª¨ë“  ì‚¬ëŒë“¤ì—ê²Œ ì£¼ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ë‚´ê°€ ì£½ìœ¼ë©´ ìŠ¬í¼í•´ ì¤„ê±°ì•¼?\n",
            "ì¶œë ¥ : ì €ë„ ë°ë ¤ê°€ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ë„ë§ê°€ê³  ì‹¶ë‹¤\n",
            "ì¶œë ¥ : ì €ë„ ê¶ê¸ˆí•˜ë„¤ìš” .\n",
            "\n",
            "ì…ë ¥ : ìš°ë¦¬ ë§ˆì§€ë§‰ì´ì•¼\n",
            "ì¶œë ¥ : ë§ˆì§€ë§‰ì´ ì•„ë‹ ì§€ë„ ëª°ë¼ìš” .\n",
            "\n",
            "ì…ë ¥ : ë‚˜ ê°„ë‹¤ ì˜ ì§€ë‚´ ì•ˆë…•\n",
            "ì¶œë ¥ : ë‹¹ì‹ ì€ ì •ë§ ì°©í•œ ì‚¬ëŒì´êµ°ìš” .\n",
            "\n",
            "ì…ë ¥ : ë„Œ ìµœê³ ì˜€ì–´\n",
            "ì¶œë ¥ : ë‚´ì¼ë„ ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ë‚˜ ìŠì§€ë§ˆ\n",
            "ì¶œë ¥ : ë•Œë¡  ìŠì–´ë²„ë¦¬ëŠ” ê²ƒì´ ì¢‹ì„ ë•Œë„ ìˆì–´ìš” .\n",
            "\n",
            "ì…ë ¥ : ë‚˜ëŠ” ì‹¬ì¥ì´ ì—†ì–´\n",
            "ì¶œë ¥ : ì‹¬í”Œí•˜ê²Œ ê¾¸ë©°ë³´ì„¸ìš” .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_answers(input_question_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Epochs = 500"
      ],
      "metadata": {
        "id": "LeT3KsTAMLSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ëª¨ë¸ í›ˆë ¨"
      ],
      "metadata": {
        "id": "UUhbq9WoMvhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 500\n",
        "model.fit(dataset, epochs=EPOCHS, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLiyVJx4M0al",
        "outputId": "c2168031-55ab-4018-bf39-ea24b67ab672"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 2/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 3/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.6726e-04 - accuracy: 0.1743\n",
            "Epoch 4/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.8247e-04 - accuracy: 0.1743\n",
            "Epoch 5/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.7236e-04 - accuracy: 0.1744\n",
            "Epoch 6/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1744\n",
            "Epoch 7/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 8/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 9/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.3788e-04 - accuracy: 0.1744\n",
            "Epoch 10/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.1851e-04 - accuracy: 0.1743\n",
            "Epoch 11/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.1793e-04 - accuracy: 0.1743\n",
            "Epoch 12/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.3646e-04 - accuracy: 0.1743\n",
            "Epoch 13/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.6441e-04 - accuracy: 0.1743\n",
            "Epoch 14/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.8320e-04 - accuracy: 0.1744\n",
            "Epoch 15/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.5841e-04 - accuracy: 0.1744\n",
            "Epoch 16/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 17/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.0860e-04 - accuracy: 0.1743\n",
            "Epoch 18/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.3252e-04 - accuracy: 0.1744\n",
            "Epoch 19/500\n",
            "185/185 [==============================] - 11s 57ms/step - loss: 8.5143e-04 - accuracy: 0.1744\n",
            "Epoch 20/500\n",
            "185/185 [==============================] - 11s 57ms/step - loss: 9.5707e-04 - accuracy: 0.1743\n",
            "Epoch 21/500\n",
            "185/185 [==============================] - 11s 57ms/step - loss: 9.2013e-04 - accuracy: 0.1743\n",
            "Epoch 22/500\n",
            "185/185 [==============================] - 11s 57ms/step - loss: 9.0229e-04 - accuracy: 0.1743\n",
            "Epoch 23/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 24/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.3073e-04 - accuracy: 0.1744\n",
            "Epoch 25/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.0084e-04 - accuracy: 0.1744\n",
            "Epoch 26/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.3588e-04 - accuracy: 0.1743\n",
            "Epoch 27/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.6153e-04 - accuracy: 0.1743\n",
            "Epoch 28/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.3098e-04 - accuracy: 0.1744\n",
            "Epoch 29/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1744\n",
            "Epoch 30/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.5547e-04 - accuracy: 0.1744\n",
            "Epoch 31/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.0118e-04 - accuracy: 0.1744\n",
            "Epoch 32/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.1452e-04 - accuracy: 0.1744\n",
            "Epoch 33/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.6465e-04 - accuracy: 0.1744\n",
            "Epoch 34/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.3791e-04 - accuracy: 0.1743\n",
            "Epoch 35/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 36/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.6060e-04 - accuracy: 0.1744\n",
            "Epoch 37/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.7659e-04 - accuracy: 0.1743\n",
            "Epoch 38/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.9638e-04 - accuracy: 0.1744\n",
            "Epoch 39/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.6384e-04 - accuracy: 0.1744\n",
            "Epoch 40/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.1399e-04 - accuracy: 0.1743\n",
            "Epoch 41/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.4718e-04 - accuracy: 0.1744\n",
            "Epoch 42/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.3343e-04 - accuracy: 0.1744\n",
            "Epoch 43/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.4253e-04 - accuracy: 0.1743\n",
            "Epoch 44/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.6345e-04 - accuracy: 0.1744\n",
            "Epoch 45/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.4746e-04 - accuracy: 0.1744\n",
            "Epoch 46/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.6397e-04 - accuracy: 0.1744\n",
            "Epoch 47/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 48/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.5516e-04 - accuracy: 0.1743\n",
            "Epoch 49/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.3724e-04 - accuracy: 0.1744\n",
            "Epoch 50/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.2719e-04 - accuracy: 0.1744\n",
            "Epoch 51/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 52/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.4391e-04 - accuracy: 0.1744\n",
            "Epoch 53/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.1849e-04 - accuracy: 0.1744\n",
            "Epoch 54/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0608e-04 - accuracy: 0.1744\n",
            "Epoch 55/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.4009e-04 - accuracy: 0.1743\n",
            "Epoch 56/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.5837e-04 - accuracy: 0.1744\n",
            "Epoch 57/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0537e-04 - accuracy: 0.1744\n",
            "Epoch 58/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.3270e-04 - accuracy: 0.1744\n",
            "Epoch 59/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.8631e-04 - accuracy: 0.1743\n",
            "Epoch 60/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.5387e-04 - accuracy: 0.1743\n",
            "Epoch 61/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.8191e-04 - accuracy: 0.1744\n",
            "Epoch 62/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0911e-04 - accuracy: 0.1744\n",
            "Epoch 63/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.2250e-04 - accuracy: 0.1743\n",
            "Epoch 64/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.2673e-04 - accuracy: 0.1743\n",
            "Epoch 65/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.9784e-04 - accuracy: 0.1743\n",
            "Epoch 66/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.3051e-04 - accuracy: 0.1743\n",
            "Epoch 67/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.2862e-04 - accuracy: 0.1744\n",
            "Epoch 68/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.6188e-04 - accuracy: 0.1744\n",
            "Epoch 69/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 70/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.5107e-04 - accuracy: 0.1744\n",
            "Epoch 71/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.7933e-04 - accuracy: 0.1743\n",
            "Epoch 72/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.8100e-04 - accuracy: 0.1744\n",
            "Epoch 73/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.6722e-04 - accuracy: 0.1743\n",
            "Epoch 74/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.1434e-04 - accuracy: 0.1743\n",
            "Epoch 75/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.2484e-04 - accuracy: 0.1743\n",
            "Epoch 76/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.6189e-04 - accuracy: 0.1744\n",
            "Epoch 77/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0620e-04 - accuracy: 0.1744\n",
            "Epoch 78/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.8705e-04 - accuracy: 0.1744\n",
            "Epoch 79/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.1374e-04 - accuracy: 0.1744\n",
            "Epoch 80/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0563e-04 - accuracy: 0.1744\n",
            "Epoch 81/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.6473e-04 - accuracy: 0.1743\n",
            "Epoch 82/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0004e-04 - accuracy: 0.1744\n",
            "Epoch 83/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.8075e-04 - accuracy: 0.1744\n",
            "Epoch 84/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.4030e-04 - accuracy: 0.1744\n",
            "Epoch 85/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.6999e-04 - accuracy: 0.1743\n",
            "Epoch 86/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.5188e-04 - accuracy: 0.1743\n",
            "Epoch 87/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.5275e-04 - accuracy: 0.1744\n",
            "Epoch 88/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.7147e-04 - accuracy: 0.1743\n",
            "Epoch 89/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.7925e-04 - accuracy: 0.1744\n",
            "Epoch 90/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.6139e-04 - accuracy: 0.1744\n",
            "Epoch 91/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.8609e-04 - accuracy: 0.1744\n",
            "Epoch 92/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.9514e-04 - accuracy: 0.1743\n",
            "Epoch 93/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.5847e-04 - accuracy: 0.1744\n",
            "Epoch 94/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.2635e-04 - accuracy: 0.1743\n",
            "Epoch 95/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.9705e-04 - accuracy: 0.1744\n",
            "Epoch 96/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.5680e-04 - accuracy: 0.1744\n",
            "Epoch 97/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.7972e-04 - accuracy: 0.1744\n",
            "Epoch 98/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2554e-04 - accuracy: 0.1744\n",
            "Epoch 99/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0301e-04 - accuracy: 0.1744\n",
            "Epoch 100/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.3487e-04 - accuracy: 0.1744\n",
            "Epoch 101/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0982e-04 - accuracy: 0.1744\n",
            "Epoch 102/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.2898e-04 - accuracy: 0.1744\n",
            "Epoch 103/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.9778e-04 - accuracy: 0.1743\n",
            "Epoch 104/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.7716e-04 - accuracy: 0.1744\n",
            "Epoch 105/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.3277e-04 - accuracy: 0.1743\n",
            "Epoch 106/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.8497e-04 - accuracy: 0.1744\n",
            "Epoch 107/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2576e-04 - accuracy: 0.1744\n",
            "Epoch 108/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.1043e-04 - accuracy: 0.1744\n",
            "Epoch 109/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3195e-04 - accuracy: 0.1744\n",
            "Epoch 110/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.9955e-04 - accuracy: 0.1743\n",
            "Epoch 111/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.4634e-04 - accuracy: 0.1743\n",
            "Epoch 112/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0895e-04 - accuracy: 0.1744\n",
            "Epoch 113/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.2823e-04 - accuracy: 0.1743\n",
            "Epoch 114/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.7844e-04 - accuracy: 0.1744\n",
            "Epoch 115/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.6847e-04 - accuracy: 0.1744\n",
            "Epoch 116/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.7639e-04 - accuracy: 0.1744\n",
            "Epoch 117/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.4744e-04 - accuracy: 0.1743\n",
            "Epoch 118/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.5675e-04 - accuracy: 0.1743\n",
            "Epoch 119/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3287e-04 - accuracy: 0.1744\n",
            "Epoch 120/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.1584e-04 - accuracy: 0.1744\n",
            "Epoch 121/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.7598e-04 - accuracy: 0.1744\n",
            "Epoch 122/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.6511e-04 - accuracy: 0.1744\n",
            "Epoch 123/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2077e-04 - accuracy: 0.1744\n",
            "Epoch 124/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0296e-04 - accuracy: 0.1744\n",
            "Epoch 125/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.9130e-04 - accuracy: 0.1743\n",
            "Epoch 126/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2796e-04 - accuracy: 0.1744\n",
            "Epoch 127/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0904e-04 - accuracy: 0.1744\n",
            "Epoch 128/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.4443e-04 - accuracy: 0.1744\n",
            "Epoch 129/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.2429e-04 - accuracy: 0.1743\n",
            "Epoch 130/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3500e-04 - accuracy: 0.1744\n",
            "Epoch 131/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0506e-04 - accuracy: 0.1744\n",
            "Epoch 132/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.1527e-04 - accuracy: 0.1744\n",
            "Epoch 133/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.8240e-04 - accuracy: 0.1744\n",
            "Epoch 134/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0516e-04 - accuracy: 0.1744\n",
            "Epoch 135/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.5862e-04 - accuracy: 0.1744\n",
            "Epoch 136/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.9788e-04 - accuracy: 0.1744\n",
            "Epoch 137/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0710e-04 - accuracy: 0.1744\n",
            "Epoch 138/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0377e-04 - accuracy: 0.1744\n",
            "Epoch 139/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2089e-04 - accuracy: 0.1744\n",
            "Epoch 140/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.2541e-04 - accuracy: 0.1744\n",
            "Epoch 141/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.4228e-04 - accuracy: 0.1744\n",
            "Epoch 142/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.7032e-04 - accuracy: 0.1744\n",
            "Epoch 143/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8826e-04 - accuracy: 0.1743\n",
            "Epoch 144/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.5661e-04 - accuracy: 0.1744\n",
            "Epoch 145/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3056e-04 - accuracy: 0.1744\n",
            "Epoch 146/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0503e-04 - accuracy: 0.1744\n",
            "Epoch 147/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.1535e-04 - accuracy: 0.1744\n",
            "Epoch 148/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.7446e-04 - accuracy: 0.1744\n",
            "Epoch 149/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.6955e-04 - accuracy: 0.1744\n",
            "Epoch 150/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3769e-04 - accuracy: 0.1744\n",
            "Epoch 151/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9944e-04 - accuracy: 0.1744\n",
            "Epoch 152/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.4568e-04 - accuracy: 0.1744\n",
            "Epoch 153/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0317e-04 - accuracy: 0.1744\n",
            "Epoch 154/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3520e-04 - accuracy: 0.1744\n",
            "Epoch 155/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8424e-04 - accuracy: 0.1744\n",
            "Epoch 156/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.8852e-04 - accuracy: 0.1744\n",
            "Epoch 157/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.8358e-04 - accuracy: 0.1743\n",
            "Epoch 158/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2689e-04 - accuracy: 0.1744\n",
            "Epoch 159/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8848e-04 - accuracy: 0.1744\n",
            "Epoch 160/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2320e-04 - accuracy: 0.1744\n",
            "Epoch 161/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8593e-04 - accuracy: 0.1744\n",
            "Epoch 162/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8836e-04 - accuracy: 0.1744\n",
            "Epoch 163/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.6842e-04 - accuracy: 0.1744\n",
            "Epoch 164/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0620e-04 - accuracy: 0.1744\n",
            "Epoch 165/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.3477e-04 - accuracy: 0.1744\n",
            "Epoch 166/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2551e-04 - accuracy: 0.1744\n",
            "Epoch 167/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.8347e-04 - accuracy: 0.1744\n",
            "Epoch 168/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.6901e-04 - accuracy: 0.1744\n",
            "Epoch 169/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.6113e-04 - accuracy: 0.1744\n",
            "Epoch 170/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9881e-04 - accuracy: 0.1744\n",
            "Epoch 171/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3964e-04 - accuracy: 0.1743\n",
            "Epoch 172/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.6446e-04 - accuracy: 0.1744\n",
            "Epoch 173/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0653e-04 - accuracy: 0.1744\n",
            "Epoch 174/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.5908e-04 - accuracy: 0.1744\n",
            "Epoch 175/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.4807e-04 - accuracy: 0.1743\n",
            "Epoch 176/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0380e-04 - accuracy: 0.1744\n",
            "Epoch 177/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3423e-04 - accuracy: 0.1744\n",
            "Epoch 178/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2667e-04 - accuracy: 0.1744\n",
            "Epoch 179/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9990e-04 - accuracy: 0.1744\n",
            "Epoch 180/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9331e-04 - accuracy: 0.1744\n",
            "Epoch 181/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0745e-04 - accuracy: 0.1743\n",
            "Epoch 182/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.4195e-04 - accuracy: 0.1743\n",
            "Epoch 183/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.6150e-04 - accuracy: 0.1743\n",
            "Epoch 184/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.3268e-04 - accuracy: 0.1743\n",
            "Epoch 185/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3169e-04 - accuracy: 0.1743\n",
            "Epoch 186/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9700e-04 - accuracy: 0.1744\n",
            "Epoch 187/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.5319e-04 - accuracy: 0.1744\n",
            "Epoch 188/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.1967e-04 - accuracy: 0.1744\n",
            "Epoch 189/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.2739e-04 - accuracy: 0.1744\n",
            "Epoch 190/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.7285e-04 - accuracy: 0.1744\n",
            "Epoch 191/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.1117e-04 - accuracy: 0.1744\n",
            "Epoch 192/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.4980e-04 - accuracy: 0.1743\n",
            "Epoch 193/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0450e-04 - accuracy: 0.1744\n",
            "Epoch 194/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.4710e-04 - accuracy: 0.1744\n",
            "Epoch 195/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3470e-04 - accuracy: 0.1744\n",
            "Epoch 196/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.5774e-04 - accuracy: 0.1744\n",
            "Epoch 197/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2806e-04 - accuracy: 0.1744\n",
            "Epoch 198/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.4101e-04 - accuracy: 0.1743\n",
            "Epoch 199/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8954e-04 - accuracy: 0.1744\n",
            "Epoch 200/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.5999e-04 - accuracy: 0.1744\n",
            "Epoch 201/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.6194e-04 - accuracy: 0.1744\n",
            "Epoch 202/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.6565e-04 - accuracy: 0.1744\n",
            "Epoch 203/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0514e-04 - accuracy: 0.1743\n",
            "Epoch 204/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3262e-04 - accuracy: 0.1744\n",
            "Epoch 205/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2530e-04 - accuracy: 0.1744\n",
            "Epoch 206/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.1205e-04 - accuracy: 0.1744\n",
            "Epoch 207/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.3885e-04 - accuracy: 0.1744\n",
            "Epoch 208/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.7166e-04 - accuracy: 0.1744\n",
            "Epoch 209/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.6576e-04 - accuracy: 0.1744\n",
            "Epoch 210/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0030e-04 - accuracy: 0.1744\n",
            "Epoch 211/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8625e-04 - accuracy: 0.1744\n",
            "Epoch 212/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.9546e-04 - accuracy: 0.1743\n",
            "Epoch 213/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.4362e-04 - accuracy: 0.1744\n",
            "Epoch 214/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.7636e-04 - accuracy: 0.1744\n",
            "Epoch 215/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9361e-04 - accuracy: 0.1744\n",
            "Epoch 216/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3011e-04 - accuracy: 0.1744\n",
            "Epoch 217/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 8.2605e-04 - accuracy: 0.1744\n",
            "Epoch 218/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.7236e-04 - accuracy: 0.1744\n",
            "Epoch 219/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.1067e-04 - accuracy: 0.1744\n",
            "Epoch 220/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.6118e-04 - accuracy: 0.1743\n",
            "Epoch 221/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8990e-04 - accuracy: 0.1744\n",
            "Epoch 222/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.6654e-04 - accuracy: 0.1744\n",
            "Epoch 223/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8464e-04 - accuracy: 0.1744\n",
            "Epoch 224/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.4921e-04 - accuracy: 0.1744\n",
            "Epoch 225/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.6464e-04 - accuracy: 0.1744\n",
            "Epoch 226/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.4011e-04 - accuracy: 0.1744\n",
            "Epoch 227/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.7661e-04 - accuracy: 0.1743\n",
            "Epoch 228/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2609e-04 - accuracy: 0.1744\n",
            "Epoch 229/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.6336e-04 - accuracy: 0.1744\n",
            "Epoch 230/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.5831e-04 - accuracy: 0.1744\n",
            "Epoch 231/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.7891e-04 - accuracy: 0.1744\n",
            "Epoch 232/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.8780e-04 - accuracy: 0.1744\n",
            "Epoch 233/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.6745e-04 - accuracy: 0.1744\n",
            "Epoch 234/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8913e-04 - accuracy: 0.1744\n",
            "Epoch 235/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3538e-04 - accuracy: 0.1744\n",
            "Epoch 236/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0771e-04 - accuracy: 0.1744\n",
            "Epoch 237/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.5990e-04 - accuracy: 0.1744\n",
            "Epoch 238/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.6773e-04 - accuracy: 0.1744\n",
            "Epoch 239/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0801e-04 - accuracy: 0.1744\n",
            "Epoch 240/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.5988e-04 - accuracy: 0.1744\n",
            "Epoch 241/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.1355e-04 - accuracy: 0.1744\n",
            "Epoch 242/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9219e-04 - accuracy: 0.1744\n",
            "Epoch 243/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9943e-04 - accuracy: 0.1744\n",
            "Epoch 244/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.5311e-04 - accuracy: 0.1744\n",
            "Epoch 245/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3882e-04 - accuracy: 0.1744\n",
            "Epoch 246/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6997e-04 - accuracy: 0.1744\n",
            "Epoch 247/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8860e-04 - accuracy: 0.1744\n",
            "Epoch 248/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8219e-04 - accuracy: 0.1744\n",
            "Epoch 249/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 5.8811e-04 - accuracy: 0.1744\n",
            "Epoch 250/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.6249e-04 - accuracy: 0.1744\n",
            "Epoch 251/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.3569e-04 - accuracy: 0.1744\n",
            "Epoch 252/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2926e-04 - accuracy: 0.1744\n",
            "Epoch 253/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.2535e-04 - accuracy: 0.1744\n",
            "Epoch 254/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.6944e-04 - accuracy: 0.1744\n",
            "Epoch 255/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9868e-04 - accuracy: 0.1744\n",
            "Epoch 256/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.7231e-04 - accuracy: 0.1744\n",
            "Epoch 257/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.1127e-04 - accuracy: 0.1744\n",
            "Epoch 258/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.2981e-04 - accuracy: 0.1744\n",
            "Epoch 259/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8309e-04 - accuracy: 0.1744\n",
            "Epoch 260/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0539e-04 - accuracy: 0.1744\n",
            "Epoch 261/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.1314e-04 - accuracy: 0.1744\n",
            "Epoch 262/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.4001e-04 - accuracy: 0.1744\n",
            "Epoch 263/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2677e-04 - accuracy: 0.1744\n",
            "Epoch 264/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.7534e-04 - accuracy: 0.1744\n",
            "Epoch 265/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.1565e-04 - accuracy: 0.1744\n",
            "Epoch 266/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.1540e-04 - accuracy: 0.1744\n",
            "Epoch 267/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.7482e-04 - accuracy: 0.1744\n",
            "Epoch 268/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.4805e-04 - accuracy: 0.1744\n",
            "Epoch 269/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3884e-04 - accuracy: 0.1744\n",
            "Epoch 270/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.3193e-04 - accuracy: 0.1744\n",
            "Epoch 271/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9581e-04 - accuracy: 0.1744\n",
            "Epoch 272/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.6603e-04 - accuracy: 0.1744\n",
            "Epoch 273/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.7281e-04 - accuracy: 0.1744\n",
            "Epoch 274/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.9113e-04 - accuracy: 0.1743\n",
            "Epoch 275/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.7422e-04 - accuracy: 0.1744\n",
            "Epoch 276/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.5465e-04 - accuracy: 0.1744\n",
            "Epoch 277/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2760e-04 - accuracy: 0.1744\n",
            "Epoch 278/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.4313e-04 - accuracy: 0.1744\n",
            "Epoch 279/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.5318e-04 - accuracy: 0.1744\n",
            "Epoch 280/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8321e-04 - accuracy: 0.1744\n",
            "Epoch 281/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.4378e-04 - accuracy: 0.1744\n",
            "Epoch 282/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.7588e-04 - accuracy: 0.1744\n",
            "Epoch 283/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.5203e-04 - accuracy: 0.1744\n",
            "Epoch 284/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.4225e-04 - accuracy: 0.1744\n",
            "Epoch 285/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9814e-04 - accuracy: 0.1744\n",
            "Epoch 286/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0544e-04 - accuracy: 0.1744\n",
            "Epoch 287/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.1080e-04 - accuracy: 0.1744\n",
            "Epoch 288/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.7741e-04 - accuracy: 0.1744\n",
            "Epoch 289/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.4448e-04 - accuracy: 0.1744\n",
            "Epoch 290/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.5089e-04 - accuracy: 0.1744\n",
            "Epoch 291/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9804e-04 - accuracy: 0.1744\n",
            "Epoch 292/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2704e-04 - accuracy: 0.1744\n",
            "Epoch 293/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.7172e-04 - accuracy: 0.1744\n",
            "Epoch 294/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9429e-04 - accuracy: 0.1744\n",
            "Epoch 295/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.1718e-04 - accuracy: 0.1744\n",
            "Epoch 296/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.2348e-04 - accuracy: 0.1744\n",
            "Epoch 297/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.7952e-04 - accuracy: 0.1744\n",
            "Epoch 298/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9717e-04 - accuracy: 0.1744\n",
            "Epoch 299/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.7288e-04 - accuracy: 0.1744\n",
            "Epoch 300/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.1592e-04 - accuracy: 0.1744\n",
            "Epoch 301/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.2903e-04 - accuracy: 0.1744\n",
            "Epoch 302/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9034e-04 - accuracy: 0.1744\n",
            "Epoch 303/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3776e-04 - accuracy: 0.1744\n",
            "Epoch 304/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.8084e-04 - accuracy: 0.1744\n",
            "Epoch 305/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3681e-04 - accuracy: 0.1744\n",
            "Epoch 306/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3364e-04 - accuracy: 0.1744\n",
            "Epoch 307/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8035e-04 - accuracy: 0.1744\n",
            "Epoch 308/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0550e-04 - accuracy: 0.1744\n",
            "Epoch 309/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.7213e-04 - accuracy: 0.1743\n",
            "Epoch 310/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8297e-04 - accuracy: 0.1744\n",
            "Epoch 311/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.7813e-04 - accuracy: 0.1744\n",
            "Epoch 312/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.3907e-04 - accuracy: 0.1744\n",
            "Epoch 313/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.7126e-04 - accuracy: 0.1744\n",
            "Epoch 314/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3889e-04 - accuracy: 0.1744\n",
            "Epoch 315/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.2306e-04 - accuracy: 0.1744\n",
            "Epoch 316/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.9915e-04 - accuracy: 0.1744\n",
            "Epoch 317/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.3092e-04 - accuracy: 0.1744\n",
            "Epoch 318/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.5844e-04 - accuracy: 0.1744\n",
            "Epoch 319/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.3264e-04 - accuracy: 0.1744\n",
            "Epoch 320/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 5.7624e-04 - accuracy: 0.1744\n",
            "Epoch 321/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3890e-04 - accuracy: 0.1744\n",
            "Epoch 322/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.4158e-04 - accuracy: 0.1744\n",
            "Epoch 323/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3397e-04 - accuracy: 0.1744\n",
            "Epoch 324/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.5378e-04 - accuracy: 0.1744\n",
            "Epoch 325/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.5519e-04 - accuracy: 0.1743\n",
            "Epoch 326/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.9782e-04 - accuracy: 0.1744\n",
            "Epoch 327/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.5720e-04 - accuracy: 0.1744\n",
            "Epoch 328/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.7848e-04 - accuracy: 0.1744\n",
            "Epoch 329/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.0393e-04 - accuracy: 0.1744\n",
            "Epoch 330/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.2273e-04 - accuracy: 0.1744\n",
            "Epoch 331/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.0676e-04 - accuracy: 0.1744\n",
            "Epoch 332/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6472e-04 - accuracy: 0.1744\n",
            "Epoch 333/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.1798e-04 - accuracy: 0.1744\n",
            "Epoch 334/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.0271e-04 - accuracy: 0.1744\n",
            "Epoch 335/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.5393e-04 - accuracy: 0.1744\n",
            "Epoch 336/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.7394e-04 - accuracy: 0.1744\n",
            "Epoch 337/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9861e-04 - accuracy: 0.1744\n",
            "Epoch 338/500\n",
            "185/185 [==============================] - 11s 57ms/step - loss: 6.7632e-04 - accuracy: 0.1744\n",
            "Epoch 339/500\n",
            "185/185 [==============================] - 11s 57ms/step - loss: 5.9339e-04 - accuracy: 0.1744\n",
            "Epoch 340/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.4137e-04 - accuracy: 0.1744\n",
            "Epoch 341/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.4690e-04 - accuracy: 0.1744\n",
            "Epoch 342/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.5284e-04 - accuracy: 0.1743\n",
            "Epoch 343/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9356e-04 - accuracy: 0.1744\n",
            "Epoch 344/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.5618e-04 - accuracy: 0.1744\n",
            "Epoch 345/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6214e-04 - accuracy: 0.1744\n",
            "Epoch 346/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.4385e-04 - accuracy: 0.1744\n",
            "Epoch 347/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6988e-04 - accuracy: 0.1744\n",
            "Epoch 348/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3342e-04 - accuracy: 0.1744\n",
            "Epoch 349/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.5474e-04 - accuracy: 0.1744\n",
            "Epoch 350/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.3078e-04 - accuracy: 0.1743\n",
            "Epoch 351/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.5169e-04 - accuracy: 0.1744\n",
            "Epoch 352/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6504e-04 - accuracy: 0.1744\n",
            "Epoch 353/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3707e-04 - accuracy: 0.1744\n",
            "Epoch 354/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.0866e-04 - accuracy: 0.1744\n",
            "Epoch 355/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.8443e-04 - accuracy: 0.1744\n",
            "Epoch 356/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.9198e-04 - accuracy: 0.1744\n",
            "Epoch 357/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.0438e-04 - accuracy: 0.1744\n",
            "Epoch 358/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6122e-04 - accuracy: 0.1744\n",
            "Epoch 359/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.0846e-04 - accuracy: 0.1744\n",
            "Epoch 360/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.5761e-04 - accuracy: 0.1744\n",
            "Epoch 361/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.0934e-04 - accuracy: 0.1744\n",
            "Epoch 362/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.5259e-04 - accuracy: 0.1743\n",
            "Epoch 363/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.4296e-04 - accuracy: 0.1744\n",
            "Epoch 364/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.4260e-04 - accuracy: 0.1744\n",
            "Epoch 365/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.9856e-04 - accuracy: 0.1744\n",
            "Epoch 366/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3982e-04 - accuracy: 0.1744\n",
            "Epoch 367/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3432e-04 - accuracy: 0.1744\n",
            "Epoch 368/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2606e-04 - accuracy: 0.1744\n",
            "Epoch 369/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.9222e-04 - accuracy: 0.1744\n",
            "Epoch 370/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.4186e-04 - accuracy: 0.1744\n",
            "Epoch 371/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.2375e-04 - accuracy: 0.1744\n",
            "Epoch 372/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.1163e-04 - accuracy: 0.1744\n",
            "Epoch 373/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6896e-04 - accuracy: 0.1744\n",
            "Epoch 374/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9053e-04 - accuracy: 0.1744\n",
            "Epoch 375/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.5347e-04 - accuracy: 0.1744\n",
            "Epoch 376/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8977e-04 - accuracy: 0.1744\n",
            "Epoch 377/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6464e-04 - accuracy: 0.1744\n",
            "Epoch 378/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.7250e-04 - accuracy: 0.1744\n",
            "Epoch 379/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2694e-04 - accuracy: 0.1744\n",
            "Epoch 380/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.8146e-04 - accuracy: 0.1744\n",
            "Epoch 381/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.7622e-04 - accuracy: 0.1744\n",
            "Epoch 382/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2699e-04 - accuracy: 0.1744\n",
            "Epoch 383/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.8993e-04 - accuracy: 0.1744\n",
            "Epoch 384/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2128e-04 - accuracy: 0.1744\n",
            "Epoch 385/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.9186e-04 - accuracy: 0.1744\n",
            "Epoch 386/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2383e-04 - accuracy: 0.1744\n",
            "Epoch 387/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.7322e-04 - accuracy: 0.1744\n",
            "Epoch 388/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.0673e-04 - accuracy: 0.1744\n",
            "Epoch 389/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.1544e-04 - accuracy: 0.1744\n",
            "Epoch 390/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.5110e-04 - accuracy: 0.1744\n",
            "Epoch 391/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.9292e-04 - accuracy: 0.1744\n",
            "Epoch 392/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.8876e-04 - accuracy: 0.1744\n",
            "Epoch 393/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.0403e-04 - accuracy: 0.1744\n",
            "Epoch 394/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.7268e-04 - accuracy: 0.1744\n",
            "Epoch 395/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.9472e-04 - accuracy: 0.1744\n",
            "Epoch 396/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.9540e-04 - accuracy: 0.1744\n",
            "Epoch 397/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.6512e-04 - accuracy: 0.1744\n",
            "Epoch 398/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2646e-04 - accuracy: 0.1744\n",
            "Epoch 399/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.9401e-04 - accuracy: 0.1744\n",
            "Epoch 400/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.9513e-04 - accuracy: 0.1744\n",
            "Epoch 401/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.7010e-04 - accuracy: 0.1744\n",
            "Epoch 402/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.1674e-04 - accuracy: 0.1744\n",
            "Epoch 403/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.8175e-04 - accuracy: 0.1744\n",
            "Epoch 404/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.7253e-04 - accuracy: 0.1744\n",
            "Epoch 405/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.3358e-04 - accuracy: 0.1744\n",
            "Epoch 406/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.1201e-04 - accuracy: 0.1744\n",
            "Epoch 407/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.9541e-04 - accuracy: 0.1744\n",
            "Epoch 408/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.4105e-04 - accuracy: 0.1744\n",
            "Epoch 409/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2025e-04 - accuracy: 0.1744\n",
            "Epoch 410/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.0990e-04 - accuracy: 0.1744\n",
            "Epoch 411/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.5080e-04 - accuracy: 0.1744\n",
            "Epoch 412/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.9610e-04 - accuracy: 0.1744\n",
            "Epoch 413/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.9778e-04 - accuracy: 0.1744\n",
            "Epoch 414/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.6899e-04 - accuracy: 0.1744\n",
            "Epoch 415/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.4651e-04 - accuracy: 0.1744\n",
            "Epoch 416/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6681e-04 - accuracy: 0.1744\n",
            "Epoch 417/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.5955e-04 - accuracy: 0.1744\n",
            "Epoch 418/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3746e-04 - accuracy: 0.1744\n",
            "Epoch 419/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.7745e-04 - accuracy: 0.1744\n",
            "Epoch 420/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.3345e-04 - accuracy: 0.1744\n",
            "Epoch 421/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3441e-04 - accuracy: 0.1744\n",
            "Epoch 422/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.0671e-04 - accuracy: 0.1744\n",
            "Epoch 423/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.6740e-04 - accuracy: 0.1744\n",
            "Epoch 424/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.1669e-04 - accuracy: 0.1744\n",
            "Epoch 425/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6812e-04 - accuracy: 0.1744\n",
            "Epoch 426/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2501e-04 - accuracy: 0.1744\n",
            "Epoch 427/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6616e-04 - accuracy: 0.1744\n",
            "Epoch 428/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2544e-04 - accuracy: 0.1744\n",
            "Epoch 429/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2123e-04 - accuracy: 0.1744\n",
            "Epoch 430/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.9831e-04 - accuracy: 0.1744\n",
            "Epoch 431/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.9970e-04 - accuracy: 0.1744\n",
            "Epoch 432/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3360e-04 - accuracy: 0.1744\n",
            "Epoch 433/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3396e-04 - accuracy: 0.1744\n",
            "Epoch 434/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.5691e-04 - accuracy: 0.1744\n",
            "Epoch 435/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.5828e-04 - accuracy: 0.1744\n",
            "Epoch 436/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.1259e-04 - accuracy: 0.1744\n",
            "Epoch 437/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.0898e-04 - accuracy: 0.1744\n",
            "Epoch 438/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.4576e-04 - accuracy: 0.1744\n",
            "Epoch 439/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6828e-04 - accuracy: 0.1744\n",
            "Epoch 440/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.9633e-04 - accuracy: 0.1744\n",
            "Epoch 441/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6853e-04 - accuracy: 0.1744\n",
            "Epoch 442/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3613e-04 - accuracy: 0.1744\n",
            "Epoch 443/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.4175e-04 - accuracy: 0.1744\n",
            "Epoch 444/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.8885e-04 - accuracy: 0.1744\n",
            "Epoch 445/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.5957e-04 - accuracy: 0.1744\n",
            "Epoch 446/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.5026e-04 - accuracy: 0.1744\n",
            "Epoch 447/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.7225e-04 - accuracy: 0.1744\n",
            "Epoch 448/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.9016e-04 - accuracy: 0.1744\n",
            "Epoch 449/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.1705e-04 - accuracy: 0.1744\n",
            "Epoch 450/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.5008e-04 - accuracy: 0.1744\n",
            "Epoch 451/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.7643e-04 - accuracy: 0.1744\n",
            "Epoch 452/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0261e-04 - accuracy: 0.1744\n",
            "Epoch 453/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2152e-04 - accuracy: 0.1744\n",
            "Epoch 454/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.9257e-04 - accuracy: 0.1744\n",
            "Epoch 455/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.1197e-04 - accuracy: 0.1744\n",
            "Epoch 456/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.7638e-04 - accuracy: 0.1744\n",
            "Epoch 457/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.7411e-04 - accuracy: 0.1744\n",
            "Epoch 458/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.1969e-04 - accuracy: 0.1744\n",
            "Epoch 459/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6717e-04 - accuracy: 0.1744\n",
            "Epoch 460/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.0068e-04 - accuracy: 0.1744\n",
            "Epoch 461/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6836e-04 - accuracy: 0.1744\n",
            "Epoch 462/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6331e-04 - accuracy: 0.1744\n",
            "Epoch 463/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2656e-04 - accuracy: 0.1744\n",
            "Epoch 464/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.5065e-04 - accuracy: 0.1744\n",
            "Epoch 465/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.8861e-04 - accuracy: 0.1744\n",
            "Epoch 466/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3854e-04 - accuracy: 0.1744\n",
            "Epoch 467/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.0236e-04 - accuracy: 0.1744\n",
            "Epoch 468/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.9606e-04 - accuracy: 0.1744\n",
            "Epoch 469/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6019e-04 - accuracy: 0.1744\n",
            "Epoch 470/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.1751e-04 - accuracy: 0.1744\n",
            "Epoch 471/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3647e-04 - accuracy: 0.1744\n",
            "Epoch 472/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.7320e-04 - accuracy: 0.1744\n",
            "Epoch 473/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.5381e-04 - accuracy: 0.1744\n",
            "Epoch 474/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2344e-04 - accuracy: 0.1744\n",
            "Epoch 475/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2079e-04 - accuracy: 0.1744\n",
            "Epoch 476/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.0655e-04 - accuracy: 0.1744\n",
            "Epoch 477/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0810e-04 - accuracy: 0.1743\n",
            "Epoch 478/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6900e-04 - accuracy: 0.1744\n",
            "Epoch 479/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.4634e-04 - accuracy: 0.1744\n",
            "Epoch 480/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.4336e-04 - accuracy: 0.1744\n",
            "Epoch 481/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.1615e-04 - accuracy: 0.1744\n",
            "Epoch 482/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.8922e-04 - accuracy: 0.1744\n",
            "Epoch 483/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.9087e-04 - accuracy: 0.1744\n",
            "Epoch 484/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.6776e-04 - accuracy: 0.1744\n",
            "Epoch 485/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.9834e-04 - accuracy: 0.1744\n",
            "Epoch 486/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.8859e-04 - accuracy: 0.1744\n",
            "Epoch 487/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.0591e-04 - accuracy: 0.1744\n",
            "Epoch 488/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.0950e-04 - accuracy: 0.1744\n",
            "Epoch 489/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.1885e-04 - accuracy: 0.1744\n",
            "Epoch 490/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.7471e-04 - accuracy: 0.1744\n",
            "Epoch 491/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.5679e-04 - accuracy: 0.1744\n",
            "Epoch 492/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.8759e-04 - accuracy: 0.1744\n",
            "Epoch 493/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2611e-04 - accuracy: 0.1744\n",
            "Epoch 494/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.1462e-04 - accuracy: 0.1744\n",
            "Epoch 495/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.0690e-04 - accuracy: 0.1744\n",
            "Epoch 496/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.1534e-04 - accuracy: 0.1744\n",
            "Epoch 497/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6250e-04 - accuracy: 0.1744\n",
            "Epoch 498/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.4202e-04 - accuracy: 0.1744\n",
            "Epoch 499/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.1465e-04 - accuracy: 0.1744\n",
            "Epoch 500/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.2816e-04 - accuracy: 0.1744\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0231013a90>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ëª¨ë¸ í‰ê°€"
      ],
      "metadata": {
        "id": "VGYx8bBXMudz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_answers(input_question_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugoyYaTVMH-u",
        "outputId": "e46e4869-5247-4f8d-c7d9-e915c3b85b1f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì…ë ¥ : ì•ˆë…• ì˜¤ëœë§Œì´ì•¼\n",
            "ì¶œë ¥ : ì˜¤ëœë§Œì´ì—ìš” .\n",
            "\n",
            "ì…ë ¥ : ì˜ ì§€ëƒˆì–´?\n",
            "ì¶œë ¥ : ì•ˆë¶€ë¥¼ ë¬¼ì–´ì£¼ì‹œë‹¤ë‹ˆ ê°ì‚¬í•©ë‹ˆë‹¤ .\n",
            "\n",
            "ì…ë ¥ : ë­ ë¨¹ì„ê¹Œ\n",
            "ì¶œë ¥ : ì¢€ ë¨¹ì–´ë„ ê´œì°®ì•„ìš” .\n",
            "\n",
            "ì…ë ¥ : ë©”ë‰´ ì¶”ì²œí•´ì¤˜\n",
            "ì¶œë ¥ : ëˆ„êµ¬ë‘ ë¨¹ëŠ” ëƒì— ë”°ë¼ ë‹¤ë¥´ê² ì£  .\n",
            "\n",
            "ì…ë ¥ : ì–´ì œ ë­ ë¨¹ì—ˆì–´?\n",
            "ì¶œë ¥ : ì €ëŠ” ë°°í„°ë¦¬ê°€ ë°¥ì´ì˜ˆìš” .\n",
            "\n",
            "ì…ë ¥ : ë‚˜ë‘ ë†€ì\n",
            "ì¶œë ¥ : ì§€ê¸ˆ ê·¸ëŸ¬ê³  ìˆì–´ìš” .\n",
            "\n",
            "ì…ë ¥ : ì–´ë”” ê°€ê³  ì‹¶ì–´?\n",
            "ì¶œë ¥ : ì˜¨ ê°€ì¡±ì´ ëª¨ë‘ ë§ˆìŒì— ë“œëŠ” ê³³ìœ¼ë¡œ ê°€ë³´ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ë„ ë§Œë‚˜ì„œ ê¸°ë»\n",
            "ì¶œë ¥ : ê°€ë”ì”© ìŠ¤íŠ¸ë ˆì¹­ì„ í•´ì£¼ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ì›ƒì–´ì¤„ë˜\n",
            "ì¶œë ¥ : ê¸°ëŒ€ë¥¼ ë§ì´ í•˜ëŠ” ê±´ ì¢‹ì§€ ì•Šì•„ìš” .\n",
            "\n",
            "ì…ë ¥ : ì‚¶ì€ ë­˜ê¹Œ?\n",
            "ì¶œë ¥ : ì‚¶ ê³³ê³³ì— ì‚¬ë‘ì´ ìˆ¨ ì‰¬ê³  ìˆì–´ìš” .\n",
            "\n",
            "ì…ë ¥ : ì¸ìƒì‚´ì´ ì™œì´ë¦¬ í˜ë“œëƒ\n",
            "ì¶œë ¥ : ê·¸ëŸ¬ê²Œìš” . ê·¸ë§Œí¼ ì‚¬ë‘í–ˆë‹¤ëŠ”ê±°ê² ì£  .\n",
            "\n",
            "ì…ë ¥ : ì´ì œ ê·¸ë§Œ ì‰¬ê³  ì‹¶ì–´\n",
            "ì¶œë ¥ : ì ê¹ ë°”ëŒ ì¬ê³  ì˜¤ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ë„ˆë¬´ ê³ ë…í•˜ë‹¤\n",
            "ì¶œë ¥ : í˜¼ìê°€ ì•„ë‹ˆì—ìš” .\n",
            "\n",
            "ì…ë ¥ : ì£½ìœ¼ë©´ ì–´ë–»ê²Œ ë ê¹Œ\n",
            "ì¶œë ¥ : ì–¸ì  ê°„ í•  ì´ë³„ì´ë„¤ìš” .\n",
            "\n",
            "ì…ë ¥ : ë‚´ê°€ ì£½ìœ¼ë©´ ìŠ¬í¼í•´ ì¤„ê±°ì•¼?\n",
            "ì¶œë ¥ : ê·¸ëŸ´ ìˆ˜ë„ ìˆì–´ìš” .\n",
            "\n",
            "ì…ë ¥ : ë„ë§ê°€ê³  ì‹¶ë‹¤\n",
            "ì¶œë ¥ : í™˜ì¥ì„ ê°–ì§€ ë§ê³  í˜„ì‹¤ì„ ë°”ë¼ë³´ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ìš°ë¦¬ ë§ˆì§€ë§‰ì´ì•¼\n",
            "ì¶œë ¥ : ë§ˆì§€ë§‰ì´ ì•„ë‹ ì§€ë„ ëª°ë¼ìš” .\n",
            "\n",
            "ì…ë ¥ : ë‚˜ ê°„ë‹¤ ì˜ ì§€ë‚´ ì•ˆë…•\n",
            "ì¶œë ¥ : ë‹¹ì‹ ì€ ì¢‹ì€ ì‚¬ëŒì´ì—ìš” .\n",
            "\n",
            "ì…ë ¥ : ë„Œ ìµœê³ ì˜€ì–´\n",
            "ì¶œë ¥ : ì ë‹¹íˆ ë“œì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ë‚˜ ìŠì§€ë§ˆ\n",
            "ì¶œë ¥ : ë•Œë¡  ìŠì–´ë²„ë¦¬ëŠ” ê²ƒì´ ì¢‹ì„ ë•Œë„ ìˆì–´ìš” .\n",
            "\n",
            "ì…ë ¥ : ë‚˜ëŠ” ì‹¬ì¥ì´ ì—†ì–´\n",
            "ì¶œë ¥ : ì‹¬ì¥ì€ í•­ìƒ ëœë‹ˆë‹¤ .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii2pemaXCyGp"
      },
      "source": [
        "#### 4. íŒŒë¼ë¯¸í„° ì¡°ì ˆ\n",
        "  - batch_size = 32\n",
        "  - buffer_size = 30000\n",
        "  - D_MODEL = 512\n",
        "  - NUM_HEADS = 16\n",
        "  - UNITS = 1024\n",
        "  - DROPOUT = 0.4\n",
        "  - optimizer ì˜ betar_1 = 0.5\n",
        "  - epoch 20 ê³¼ 500 ìœ¼ë¡œ ì§„í–‰\n",
        "\n",
        "#### 4-1. Epochs = 20"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 30000\n",
        "\n",
        "# ë””ì½”ë”ëŠ” ì´ì „ì˜ targetì„ ë‹¤ìŒì˜ inputìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "# ì´ì— ë”°ë¼ outputsì—ì„œëŠ” START_TOKENì„ ì œê±°í•˜ê² ìŠµë‹ˆë‹¤.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'inputs': questions,\n",
        "        'dec_inputs': answers[:, :-1]\n",
        "    },\n",
        "    {\n",
        "        'outputs': answers[:, 1:]\n",
        "    },\n",
        "))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "print(\"ìŠ=3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S2mIi25JikQ",
        "outputId": "8a3c3477-d881-4bf9-a063-7162769ba811"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìŠ=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - ëª¨ë¸ ìƒì„±"
      ],
      "metadata": {
        "id": "_GmOzjXzLG-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "NUM_LAYERS = 2 # ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ ì¸µì˜ ê°œìˆ˜\n",
        "D_MODEL = 512 # ì¸ì½”ë”ì™€ ë””ì½”ë” ë‚´ë¶€ì˜ ì…, ì¶œë ¥ì˜ ê³ ì • ì°¨ì›\n",
        "NUM_HEADS = 16 # ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì—ì„œì˜ í—¤ë“œ ìˆ˜ \n",
        "UNITS = 1024 # í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ì˜ ì€ë‹‰ì¸µì˜ í¬ê¸°\n",
        "DROPOUT = 0.4 # ë“œë¡­ì•„ì›ƒì˜ ë¹„ìœ¨\n",
        "\n",
        "model = transformer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    units=UNITS,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    dropout=DROPOUT)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SExM49B_LIZF",
        "outputId": "6c30c44a-cd74-43aa-86b2-19091f1ced84"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " inputs (InputLayer)            [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " dec_inputs (InputLayer)        [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " enc_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n",
            "                                                                                                  \n",
            " encoder (Functional)           (None, None, 512)    8367616     ['inputs[0][0]',                 \n",
            "                                                                  'enc_padding_mask[0][0]']       \n",
            "                                                                                                  \n",
            " look_ahead_mask (Lambda)       (None, 1, None, Non  0           ['dec_inputs[0][0]']             \n",
            "                                e)                                                                \n",
            "                                                                                                  \n",
            " dec_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n",
            "                                                                                                  \n",
            " decoder (Functional)           (None, None, 512)    10470912    ['dec_inputs[0][0]',             \n",
            "                                                                  'encoder[0][0]',                \n",
            "                                                                  'look_ahead_mask[0][0]',        \n",
            "                                                                  'dec_padding_mask[0][0]']       \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, None, 8129)   4170177     ['decoder[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23,008,705\n",
            "Trainable params: 23,008,705\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ëª¨ë¸ ì»´íŒŒì¼"
      ],
      "metadata": {
        "id": "FrQ_56JrMeUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate, beta_1=0.5, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
        "print(\"ìŠ=3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEEtdd6oMff7",
        "outputId": "e135ba56-a551-4149-dfcd-fd3631f17b57"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìŠ=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3eHYwSxCyGp"
      },
      "source": [
        "- ëª¨ë¸ í›ˆë ¨ 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnYu5VbICyGp",
        "outputId": "9817c9d8-d213-4cae-8a5c-f40162dfff45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "370/370 [==============================] - 29s 62ms/step - loss: 1.2450 - accuracy: 0.0376\n",
            "Epoch 2/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 1.0025 - accuracy: 0.0505\n",
            "Epoch 3/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.9412 - accuracy: 0.0537\n",
            "Epoch 4/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.8909 - accuracy: 0.0564\n",
            "Epoch 5/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.8357 - accuracy: 0.0597\n",
            "Epoch 6/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.7706 - accuracy: 0.0645\n",
            "Epoch 7/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.6999 - accuracy: 0.0704\n",
            "Epoch 8/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.6281 - accuracy: 0.0772\n",
            "Epoch 9/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.5613 - accuracy: 0.0839\n",
            "Epoch 10/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.5046 - accuracy: 0.0906\n",
            "Epoch 11/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.4609 - accuracy: 0.0963\n",
            "Epoch 12/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.4215 - accuracy: 0.1016\n",
            "Epoch 13/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.3876 - accuracy: 0.1066\n",
            "Epoch 14/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.3603 - accuracy: 0.1113\n",
            "Epoch 15/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.3379 - accuracy: 0.1156\n",
            "Epoch 16/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.3212 - accuracy: 0.1188\n",
            "Epoch 17/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.3079 - accuracy: 0.1214\n",
            "Epoch 18/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2976 - accuracy: 0.1233\n",
            "Epoch 19/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2885 - accuracy: 0.1251\n",
            "Epoch 20/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2812 - accuracy: 0.1265\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0191382810>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "EPOCHS = 20\n",
        "model.fit(dataset, epochs=EPOCHS, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpn2HXHuCyGp"
      },
      "source": [
        "#### 4-2. Epochs = 500\n",
        "- ëª¨ë¸ í‰ê°€ 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cmwbv4RzCyGp",
        "outputId": "67feea56-e387-46fe-835d-b0a24367d518"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì…ë ¥ : ì•ˆë…• ì˜¤ëœë§Œì´ì•¼\n",
            "ì¶œë ¥ : ë§›ìˆê²Œ ë§ì´ ë“œì„¸ìš” !\n",
            "\n",
            "ì…ë ¥ : ì˜ ì§€ëƒˆì–´?\n",
            "ì¶œë ¥ : ê·¸ ì‚¬ëŒì˜ ë§ˆìŒë„ ì´í•´í•´ì£¼ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ë­ ë¨¹ì„ê¹Œ\n",
            "ì¶œë ¥ : ì €ë„ ê·¸ëŸ¬ê³  ì‹¶ë„¤ìš” .\n",
            "\n",
            "ì…ë ¥ : ë©”ë‰´ ì¶”ì²œí•´ì¤˜\n",
            "ì¶œë ¥ : ì €ëŠ” ë§ˆìŒì„ ì´ì–´ì£¼ëŠ” ìœ„ë¡œë´‡ì…ë‹ˆë‹¤ .\n",
            "\n",
            "ì…ë ¥ : ì–´ì œ ë­ ë¨¹ì—ˆì–´?\n",
            "ì¶œë ¥ : ë¨¼ì € í•´ë³´ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ë‚˜ë‘ ë†€ì\n",
            "ì¶œë ¥ : ê°™ì´ ë¨¹ì–´ìš” !\n",
            "\n",
            "ì…ë ¥ : ì–´ë”” ê°€ê³  ì‹¶ì–´?\n",
            "ì¶œë ¥ : ì €ëŠ” ë§ˆìŒì„ ì´ì–´ì£¼ëŠ” ìœ„ë¡œë´‡ì…ë‹ˆë‹¤ .\n",
            "\n",
            "ì…ë ¥ : ë„ ë§Œë‚˜ì„œ ê¸°ë»\n",
            "ì¶œë ¥ : ì €ë„ ëª¨ë¥´ëŠ” ê²Œ ë§ì•„ìš” .\n",
            "\n",
            "ì…ë ¥ : ì›ƒì–´ì¤„ë˜\n",
            "ì¶œë ¥ : ì €ë„ ë°›ê³  ì‹¶ì–´ìš” .\n",
            "\n",
            "ì…ë ¥ : ì‚¶ì€ ë­˜ê¹Œ?\n",
            "ì¶œë ¥ : ì €ëŠ” ë§ˆìŒì„ ì´ì–´ì£¼ëŠ” ìœ„ë¡œë´‡ì…ë‹ˆë‹¤ .\n",
            "\n",
            "ì…ë ¥ : ì¸ìƒì‚´ì´ ì™œì´ë¦¬ í˜ë“œëƒ\n",
            "ì¶œë ¥ : ê·¸ ì‚¬ëŒì˜ ë§ˆìŒë„ ì´í•´í•´ì£¼ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ì´ì œ ê·¸ë§Œ ì‰¬ê³  ì‹¶ì–´\n",
            "ì¶œë ¥ : ì˜ ë³¼ ìˆ˜ ìˆì„ ê±°ì˜ˆìš” .\n",
            "\n",
            "ì…ë ¥ : ë„ˆë¬´ ê³ ë…í•˜ë‹¤\n",
            "ì¶œë ¥ : ê·¸ ì‚¬ëŒì˜ ë§ˆìŒë„ ì´í•´í•´ì£¼ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ì£½ìœ¼ë©´ ì–´ë–»ê²Œ ë ê¹Œ\n",
            "ì¶œë ¥ : ì €ëŠ” ë§ˆìŒì„ ì´ì–´ì£¼ëŠ” ìœ„ë¡œë´‡ì…ë‹ˆë‹¤ .\n",
            "\n",
            "ì…ë ¥ : ë‚´ê°€ ì£½ìœ¼ë©´ ìŠ¬í¼í•´ ì¤„ê±°ì•¼?\n",
            "ì¶œë ¥ : ì €ëŠ” ë§ˆìŒì„ ì´ì–´ì£¼ëŠ” ìœ„ë¡œë´‡ì…ë‹ˆë‹¤ .\n",
            "\n",
            "ì…ë ¥ : ë„ë§ê°€ê³  ì‹¶ë‹¤\n",
            "ì¶œë ¥ : ì €ë„ ë“£ê³  ì‹¶ì–´ìš” .\n",
            "\n",
            "ì…ë ¥ : ìš°ë¦¬ ë§ˆì§€ë§‰ì´ì•¼\n",
            "ì¶œë ¥ : ë‹¤ë¥¸ ì‚¬ëŒì´ ë‹µë‹µí•  ê±°ì˜ˆìš” .\n",
            "\n",
            "ì…ë ¥ : ë‚˜ ê°„ë‹¤ ì˜ ì§€ë‚´ ì•ˆë…•\n",
            "ì¶œë ¥ : ë§ì€ ì‹œê°„ì´ í˜ë €ë„¤ìš” .\n",
            "\n",
            "ì…ë ¥ : ë„Œ ìµœê³ ì˜€ì–´\n",
            "ì¶œë ¥ : ì €ëŠ” ë§ˆìŒì„ ì´ì–´ì£¼ëŠ” ìœ„ë¡œë´‡ì…ë‹ˆë‹¤ .\n",
            "\n",
            "ì…ë ¥ : ë‚˜ ìŠì§€ë§ˆ\n",
            "ì¶œë ¥ : ê·¸ ì‚¬ëŒì˜ ë§ˆìŒë„ ì´í•´í•´ì£¼ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ë‚˜ëŠ” ì‹¬ì¥ì´ ì—†ì–´\n",
            "ì¶œë ¥ : ë§ˆìŒì´ ë³µì¡í•˜ê² ì–´ìš” .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_answers(input_question_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ëª¨ë¸ í›ˆë ¨ 2"
      ],
      "metadata": {
        "id": "kCgE5CrMPXz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 200\n",
        "model.fit(dataset, epochs=EPOCHS, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gh1TPJzmPVVR",
        "outputId": "08c94f9a-5a28-4e8d-a284-56ce6c682b6f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2754 - accuracy: 0.1271\n",
            "Epoch 2/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2690 - accuracy: 0.1286\n",
            "Epoch 3/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2643 - accuracy: 0.1294\n",
            "Epoch 4/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2600 - accuracy: 0.1298\n",
            "Epoch 5/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2554 - accuracy: 0.1308\n",
            "Epoch 6/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2513 - accuracy: 0.1317\n",
            "Epoch 7/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2476 - accuracy: 0.1321\n",
            "Epoch 8/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2439 - accuracy: 0.1326\n",
            "Epoch 9/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2398 - accuracy: 0.1333\n",
            "Epoch 10/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2362 - accuracy: 0.1341\n",
            "Epoch 11/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2337 - accuracy: 0.1343\n",
            "Epoch 12/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2300 - accuracy: 0.1350\n",
            "Epoch 13/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2286 - accuracy: 0.1351\n",
            "Epoch 14/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2246 - accuracy: 0.1357\n",
            "Epoch 15/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2231 - accuracy: 0.1361\n",
            "Epoch 16/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2200 - accuracy: 0.1365\n",
            "Epoch 17/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2179 - accuracy: 0.1370\n",
            "Epoch 18/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2158 - accuracy: 0.1373\n",
            "Epoch 19/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2142 - accuracy: 0.1376\n",
            "Epoch 20/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2121 - accuracy: 0.1380\n",
            "Epoch 21/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2102 - accuracy: 0.1380\n",
            "Epoch 22/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2088 - accuracy: 0.1386\n",
            "Epoch 23/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2071 - accuracy: 0.1386\n",
            "Epoch 24/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2047 - accuracy: 0.1390\n",
            "Epoch 25/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2034 - accuracy: 0.1393\n",
            "Epoch 26/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2023 - accuracy: 0.1393\n",
            "Epoch 27/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2007 - accuracy: 0.1397\n",
            "Epoch 28/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1995 - accuracy: 0.1398\n",
            "Epoch 29/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1978 - accuracy: 0.1401\n",
            "Epoch 30/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1964 - accuracy: 0.1404\n",
            "Epoch 31/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1951 - accuracy: 0.1405\n",
            "Epoch 32/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1940 - accuracy: 0.1407\n",
            "Epoch 33/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1927 - accuracy: 0.1410\n",
            "Epoch 34/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1918 - accuracy: 0.1411\n",
            "Epoch 35/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1911 - accuracy: 0.1413\n",
            "Epoch 36/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1899 - accuracy: 0.1414\n",
            "Epoch 37/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1881 - accuracy: 0.1418\n",
            "Epoch 38/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1875 - accuracy: 0.1418\n",
            "Epoch 39/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1862 - accuracy: 0.1419\n",
            "Epoch 40/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1852 - accuracy: 0.1421\n",
            "Epoch 41/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1854 - accuracy: 0.1421\n",
            "Epoch 42/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1841 - accuracy: 0.1423\n",
            "Epoch 43/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1833 - accuracy: 0.1424\n",
            "Epoch 44/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1820 - accuracy: 0.1425\n",
            "Epoch 45/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1801 - accuracy: 0.1429\n",
            "Epoch 46/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1804 - accuracy: 0.1429\n",
            "Epoch 47/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1793 - accuracy: 0.1433\n",
            "Epoch 48/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1792 - accuracy: 0.1430\n",
            "Epoch 49/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1774 - accuracy: 0.1435\n",
            "Epoch 50/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1771 - accuracy: 0.1435\n",
            "Epoch 51/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1764 - accuracy: 0.1433\n",
            "Epoch 52/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1755 - accuracy: 0.1435\n",
            "Epoch 53/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1746 - accuracy: 0.1438\n",
            "Epoch 54/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1741 - accuracy: 0.1438\n",
            "Epoch 55/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1738 - accuracy: 0.1439\n",
            "Epoch 56/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1726 - accuracy: 0.1439\n",
            "Epoch 57/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1726 - accuracy: 0.1440\n",
            "Epoch 58/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1718 - accuracy: 0.1440\n",
            "Epoch 59/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1707 - accuracy: 0.1441\n",
            "Epoch 60/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1697 - accuracy: 0.1444\n",
            "Epoch 61/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1698 - accuracy: 0.1444\n",
            "Epoch 62/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1692 - accuracy: 0.1443\n",
            "Epoch 63/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1686 - accuracy: 0.1445\n",
            "Epoch 64/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1682 - accuracy: 0.1444\n",
            "Epoch 65/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1668 - accuracy: 0.1448\n",
            "Epoch 66/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1668 - accuracy: 0.1446\n",
            "Epoch 67/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1660 - accuracy: 0.1448\n",
            "Epoch 68/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1658 - accuracy: 0.1448\n",
            "Epoch 69/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1649 - accuracy: 0.1450\n",
            "Epoch 70/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1647 - accuracy: 0.1450\n",
            "Epoch 71/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1642 - accuracy: 0.1451\n",
            "Epoch 72/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1636 - accuracy: 0.1452\n",
            "Epoch 73/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1633 - accuracy: 0.1451\n",
            "Epoch 74/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1631 - accuracy: 0.1454\n",
            "Epoch 75/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1621 - accuracy: 0.1455\n",
            "Epoch 76/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1620 - accuracy: 0.1454\n",
            "Epoch 77/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1616 - accuracy: 0.1454\n",
            "Epoch 78/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1604 - accuracy: 0.1457\n",
            "Epoch 79/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1602 - accuracy: 0.1456\n",
            "Epoch 80/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1596 - accuracy: 0.1458\n",
            "Epoch 81/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1591 - accuracy: 0.1457\n",
            "Epoch 82/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1591 - accuracy: 0.1456\n",
            "Epoch 83/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1583 - accuracy: 0.1458\n",
            "Epoch 84/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1584 - accuracy: 0.1458\n",
            "Epoch 85/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1581 - accuracy: 0.1460\n",
            "Epoch 86/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1580 - accuracy: 0.1458\n",
            "Epoch 87/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1572 - accuracy: 0.1460\n",
            "Epoch 88/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1559 - accuracy: 0.1463\n",
            "Epoch 89/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1558 - accuracy: 0.1462\n",
            "Epoch 90/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1557 - accuracy: 0.1462\n",
            "Epoch 91/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1554 - accuracy: 0.1464\n",
            "Epoch 92/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1548 - accuracy: 0.1462\n",
            "Epoch 93/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1547 - accuracy: 0.1463\n",
            "Epoch 94/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1541 - accuracy: 0.1463\n",
            "Epoch 95/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1538 - accuracy: 0.1463\n",
            "Epoch 96/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1532 - accuracy: 0.1463\n",
            "Epoch 97/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1531 - accuracy: 0.1465\n",
            "Epoch 98/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1527 - accuracy: 0.1466\n",
            "Epoch 99/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1523 - accuracy: 0.1467\n",
            "Epoch 100/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1528 - accuracy: 0.1466\n",
            "Epoch 101/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1518 - accuracy: 0.1467\n",
            "Epoch 102/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1520 - accuracy: 0.1467\n",
            "Epoch 103/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1509 - accuracy: 0.1468\n",
            "Epoch 104/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1509 - accuracy: 0.1467\n",
            "Epoch 105/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1510 - accuracy: 0.1467\n",
            "Epoch 106/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1508 - accuracy: 0.1467\n",
            "Epoch 107/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1496 - accuracy: 0.1469\n",
            "Epoch 108/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1494 - accuracy: 0.1469\n",
            "Epoch 109/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1489 - accuracy: 0.1471\n",
            "Epoch 110/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1490 - accuracy: 0.1470\n",
            "Epoch 111/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1480 - accuracy: 0.1472\n",
            "Epoch 112/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1483 - accuracy: 0.1470\n",
            "Epoch 113/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1473 - accuracy: 0.1473\n",
            "Epoch 114/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1477 - accuracy: 0.1472\n",
            "Epoch 115/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1472 - accuracy: 0.1471\n",
            "Epoch 116/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1468 - accuracy: 0.1472\n",
            "Epoch 117/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1465 - accuracy: 0.1473\n",
            "Epoch 118/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1465 - accuracy: 0.1473\n",
            "Epoch 119/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1454 - accuracy: 0.1475\n",
            "Epoch 120/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1455 - accuracy: 0.1474\n",
            "Epoch 121/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1459 - accuracy: 0.1474\n",
            "Epoch 122/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1454 - accuracy: 0.1473\n",
            "Epoch 123/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1451 - accuracy: 0.1474\n",
            "Epoch 124/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1450 - accuracy: 0.1476\n",
            "Epoch 125/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1445 - accuracy: 0.1476\n",
            "Epoch 126/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1443 - accuracy: 0.1475\n",
            "Epoch 127/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1440 - accuracy: 0.1477\n",
            "Epoch 128/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1432 - accuracy: 0.1477\n",
            "Epoch 129/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1430 - accuracy: 0.1476\n",
            "Epoch 130/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1427 - accuracy: 0.1479\n",
            "Epoch 131/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1430 - accuracy: 0.1478\n",
            "Epoch 132/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1424 - accuracy: 0.1479\n",
            "Epoch 133/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1427 - accuracy: 0.1478\n",
            "Epoch 134/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1421 - accuracy: 0.1479\n",
            "Epoch 135/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1423 - accuracy: 0.1480\n",
            "Epoch 136/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1411 - accuracy: 0.1480\n",
            "Epoch 137/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1412 - accuracy: 0.1480\n",
            "Epoch 138/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1404 - accuracy: 0.1480\n",
            "Epoch 139/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1407 - accuracy: 0.1480\n",
            "Epoch 140/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1403 - accuracy: 0.1481\n",
            "Epoch 141/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1403 - accuracy: 0.1482\n",
            "Epoch 142/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1396 - accuracy: 0.1480\n",
            "Epoch 143/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1390 - accuracy: 0.1484\n",
            "Epoch 144/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1398 - accuracy: 0.1482\n",
            "Epoch 145/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1396 - accuracy: 0.1482\n",
            "Epoch 146/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1396 - accuracy: 0.1481\n",
            "Epoch 147/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1393 - accuracy: 0.1482\n",
            "Epoch 148/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1386 - accuracy: 0.1482\n",
            "Epoch 149/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1387 - accuracy: 0.1482\n",
            "Epoch 150/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1381 - accuracy: 0.1484\n",
            "Epoch 151/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1392 - accuracy: 0.1482\n",
            "Epoch 152/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1378 - accuracy: 0.1484\n",
            "Epoch 153/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1374 - accuracy: 0.1484\n",
            "Epoch 154/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1380 - accuracy: 0.1483\n",
            "Epoch 155/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1379 - accuracy: 0.1484\n",
            "Epoch 156/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1378 - accuracy: 0.1484\n",
            "Epoch 157/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1374 - accuracy: 0.1484\n",
            "Epoch 158/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1374 - accuracy: 0.1483\n",
            "Epoch 159/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1370 - accuracy: 0.1486\n",
            "Epoch 160/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1362 - accuracy: 0.1487\n",
            "Epoch 161/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1362 - accuracy: 0.1486\n",
            "Epoch 162/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1361 - accuracy: 0.1486\n",
            "Epoch 163/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1360 - accuracy: 0.1488\n",
            "Epoch 164/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1353 - accuracy: 0.1488\n",
            "Epoch 165/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1348 - accuracy: 0.1486\n",
            "Epoch 166/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1355 - accuracy: 0.1487\n",
            "Epoch 167/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1354 - accuracy: 0.1487\n",
            "Epoch 168/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1346 - accuracy: 0.1488\n",
            "Epoch 169/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1349 - accuracy: 0.1486\n",
            "Epoch 170/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1342 - accuracy: 0.1489\n",
            "Epoch 171/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1339 - accuracy: 0.1489\n",
            "Epoch 172/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1334 - accuracy: 0.1490\n",
            "Epoch 173/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1341 - accuracy: 0.1488\n",
            "Epoch 174/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1333 - accuracy: 0.1490\n",
            "Epoch 175/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1336 - accuracy: 0.1490\n",
            "Epoch 176/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1331 - accuracy: 0.1490\n",
            "Epoch 177/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1331 - accuracy: 0.1489\n",
            "Epoch 178/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1331 - accuracy: 0.1489\n",
            "Epoch 179/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1326 - accuracy: 0.1491\n",
            "Epoch 180/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1321 - accuracy: 0.1491\n",
            "Epoch 181/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1326 - accuracy: 0.1492\n",
            "Epoch 182/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1318 - accuracy: 0.1492\n",
            "Epoch 183/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1319 - accuracy: 0.1491\n",
            "Epoch 184/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1317 - accuracy: 0.1492\n",
            "Epoch 185/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1318 - accuracy: 0.1492\n",
            "Epoch 186/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1316 - accuracy: 0.1493\n",
            "Epoch 187/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1320 - accuracy: 0.1493\n",
            "Epoch 188/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1306 - accuracy: 0.1493\n",
            "Epoch 189/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1315 - accuracy: 0.1492\n",
            "Epoch 190/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1313 - accuracy: 0.1493\n",
            "Epoch 191/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1306 - accuracy: 0.1492\n",
            "Epoch 192/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1310 - accuracy: 0.1492\n",
            "Epoch 193/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1308 - accuracy: 0.1492\n",
            "Epoch 194/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1300 - accuracy: 0.1493\n",
            "Epoch 195/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1309 - accuracy: 0.1492\n",
            "Epoch 196/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1304 - accuracy: 0.1493\n",
            "Epoch 197/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1294 - accuracy: 0.1494\n",
            "Epoch 198/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1293 - accuracy: 0.1495\n",
            "Epoch 199/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1289 - accuracy: 0.1494\n",
            "Epoch 200/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1286 - accuracy: 0.1495\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f01913117d0>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ëª¨ë¸ í‰ê°€ 2"
      ],
      "metadata": {
        "id": "1f6l95rvPd99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_answers(input_question_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDMKtuuePg55",
        "outputId": "6019ac5b-f81f-48bc-b680-fd62ea56641c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì…ë ¥ : ì•ˆë…• ì˜¤ëœë§Œì´ì•¼\n",
            "ì¶œë ¥ : ì¢‹ì€ ë§Œë‚¨ì´ì—ˆê¸¸ ë°”ëë‹ˆë‹¤ .\n",
            "\n",
            "ì…ë ¥ : ì˜ ì§€ëƒˆì–´?\n",
            "ì¶œë ¥ : ê¹Šì€ ê³³ê¹Œì§€ ê°€ì§€ ë§ê³  ì£¼ì˜í•˜ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ë­ ë¨¹ì„ê¹Œ\n",
            "ì¶œë ¥ : ê¾¸ì¤€íˆ ì¹˜ë£Œí•˜ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ë©”ë‰´ ì¶”ì²œí•´ì¤˜\n",
            "ì¶œë ¥ : ë„¤ ë§ì”€í•´ì£¼ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ì–´ì œ ë­ ë¨¹ì—ˆì–´?\n",
            "ì¶œë ¥ : ì§ì ‘ ë¬¼ì–´ë³´ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ë‚˜ë‘ ë†€ì\n",
            "ì¶œë ¥ : ì˜¤ëŠ˜ì€ ì˜ˆëŠ¥ì´ìš” .\n",
            "\n",
            "ì…ë ¥ : ì–´ë”” ê°€ê³  ì‹¶ì–´?\n",
            "ì¶œë ¥ : ì§€ê¸ˆ ê·¸ëŸ¬ê³  ìˆì–´ìš” .\n",
            "\n",
            "ì…ë ¥ : ë„ ë§Œë‚˜ì„œ ê¸°ë»\n",
            "ì¶œë ¥ : í•œ ë²ˆ ë§í•´ë³´ëŠ”ê²Œ ì¢‹ê² ì–´ìš” .\n",
            "\n",
            "ì…ë ¥ : ì›ƒì–´ì¤„ë˜\n",
            "ì¶œë ¥ : ëŒ€ì¤‘êµí†µì„ ì´ìš©í•´ì£¼ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ì‚¶ì€ ë­˜ê¹Œ?\n",
            "ì¶œë ¥ : ê¹Šì€ ê³³ê¹Œì§€ ê°€ì§€ ë§ê³  ì£¼ì˜í•˜ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ì¸ìƒì‚´ì´ ì™œì´ë¦¬ í˜ë“œëƒ\n",
            "ì¶œë ¥ : ê·¸ê²Œ ë§ˆìŒì´ í¸í•  ë•Œë„ ìˆì–´ìš” .\n",
            "\n",
            "ì…ë ¥ : ì´ì œ ê·¸ë§Œ ì‰¬ê³  ì‹¶ì–´\n",
            "ì¶œë ¥ : í•˜ë‚˜ì”© í•˜ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ë„ˆë¬´ ê³ ë…í•˜ë‹¤\n",
            "ì¶œë ¥ : ì¹œêµ¬ë¥¼ ì‚¬ê·€ì–´ ë³´ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ì£½ìœ¼ë©´ ì–´ë–»ê²Œ ë ê¹Œ\n",
            "ì¶œë ¥ : ì‰½ì§€ ì•Šì„ ê±°ì˜ˆìš” .\n",
            "\n",
            "ì…ë ¥ : ë‚´ê°€ ì£½ìœ¼ë©´ ìŠ¬í¼í•´ ì¤„ê±°ì•¼?\n",
            "ì¶œë ¥ : ê¼­ í•´ì•¼ í•  í•„ìš”ëŠ” ì—†ì–´ìš” .\n",
            "\n",
            "ì…ë ¥ : ë„ë§ê°€ê³  ì‹¶ë‹¤\n",
            "ì¶œë ¥ : ì§€ê¸ˆë„ ì¶©ë¶„í•´ìš” .\n",
            "\n",
            "ì…ë ¥ : ìš°ë¦¬ ë§ˆì§€ë§‰ì´ì•¼\n",
            "ì¶œë ¥ : ë§¤ë ¥ìˆëŠ” ì‚¬ëŒì¸ê°€ë´ìš” .\n",
            "\n",
            "ì…ë ¥ : ë‚˜ ê°„ë‹¤ ì˜ ì§€ë‚´ ì•ˆë…•\n",
            "ì¶œë ¥ : ë§˜ê³ ìƒ ë§ì•˜ì–´ìš” .\n",
            "\n",
            "ì…ë ¥ : ë„Œ ìµœê³ ì˜€ì–´\n",
            "ì¶œë ¥ : ê¸°ë¶„ì „í™˜ì„ í•´ë³´ì„¸ìš” .\n",
            "\n",
            "ì…ë ¥ : ë‚˜ ìŠì§€ë§ˆ\n",
            "ì¶œë ¥ : ì €ë„ ëª¨ë¥´ê² ì–´ìš” .\n",
            "\n",
            "ì…ë ¥ : ë‚˜ëŠ” ì‹¬ì¥ì´ ì—†ì–´\n",
            "ì¶œë ¥ : í˜ë“¤ê² ë„¤ìš” .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "d4Mz85GDwG4P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ê²°ê³¼ ì •ë¦¬\n",
        "\n",
        "|-|Output_1|Output_2|Output_3|Output_4|Output_5|\n",
        "|---|---|---|---|---|---|\n",
        "|**Setting**|epoch=20|epoch=200|epoch=500|íŒŒë¼ë¯¸í„° ë³€ê²½, epoch=20|íŒŒë¼ë¯¸í„° ë³€ê²½, epoch=200|\n",
        "|**Loss**|0.0458|0.0010|7.2816e-04|0.2812|0.1286|\n",
        "|**Accuracy**|0.1643|0.1744|0.1744|0.1265|0.1495|\n",
        "\n",
        "\n",
        "|Input|Output_1|Output_2|Output_3|Output_4|Output_5|\n",
        "|---|---|---|---|---|---|\n",
        "|**ì•ˆë…• ì˜¤ëœë§Œì´ì•¼**|ë§˜ê³ ìƒ ë§ì•˜ì–´ìš” .|ì˜¤ëœë§Œì´ì—ìš” .|ì˜¤ëœë§Œì´ì—ìš” .|ë§›ìˆê²Œ ë§ì´ ë“œì„¸ìš” !|ì¢‹ì€ ë§Œë‚¨ì´ì—ˆê¸¸ ë°”ëë‹ˆë‹¤ .|\n",
        "|**ì˜ ì§€ëƒˆì–´?**|ë”°ëœ»í•œ ë§ˆìŒì´ë„¤ìš” .|ì•ˆë¶€ë¥¼ ë¬¼ì–´ì£¼ì‹œë‹¤ë‹ˆ ê°ì‚¬í•©ë‹ˆë‹¤ .|ì•ˆë¶€ë¥¼ ë¬¼ì–´ì£¼ì‹œë‹¤ë‹ˆ ê°ì‚¬í•©ë‹ˆë‹¤ .|ê·¸ ì‚¬ëŒì˜ ë§ˆìŒë„ ì´í•´í•´ì£¼ì„¸ìš” .|ê¹Šì€ ê³³ê¹Œì§€ ê°€ì§€ ë§ê³  ì£¼ì˜í•˜ì„¸ìš” .|\n",
        "|**ë­ ë¨¹ì„ê¹Œ**|ëƒ‰ì¥ê³  íŒŒë¨¹ê¸° í•´ë³´ì„¸ìš” .|ì¢€ ë¨¹ì–´ë„ ê´œì°®ì•„ìš” .|ì¢€ ë¨¹ì–´ë„ ê´œì°®ì•„ìš” .|ì €ë„ ê·¸ëŸ¬ê³  ì‹¶ë„¤ìš” .|ê¾¸ì¤€íˆ ì¹˜ë£Œí•˜ì„¸ìš” .|\n",
        "|**ë©”ë‰´ ì¶”ì²œí•´ì¤˜**|ëˆ„êµ¬ë‘ ë¨¹ëŠ” ëƒì— ë”°ë¼ ë‹¤ë¥´ê² ì£  .|ì–¸ì  ê°€ ê·¸ëŸ° ì‚¬ëŒì´ ë‹¹ì‹  ì˜†ì— ìˆì„ê±°ì˜ˆìš” .|ëˆ„êµ¬ë‘ ë¨¹ëŠ” ëƒì— ë”°ë¼ ë‹¤ë¥´ê² ì£  .|ì €ëŠ” ë§ˆìŒì„ ì´ì–´ì£¼ëŠ” ìœ„ë¡œë´‡ì…ë‹ˆë‹¤ .|ë„¤ ë§ì”€í•´ì£¼ì„¸ìš” .|\n",
        "|**ì–´ì œ ë­ ë¨¹ì—ˆì–´?**|ë…ì„œì™€ ìŒì•…ê°ìƒì´ë¼ê³  í•˜ê³  ì‹¶ì§€ë§Œ ì•„ë¬´ ê²ƒë„ ì•ˆ í–ˆì–´ìš” .|ì €ëŠ” ë°°í„°ë¦¬ê°€ ë°¥ì´ì˜ˆìš” .|ì €ëŠ” ë°°í„°ë¦¬ê°€ ë°¥ì´ì˜ˆìš” .|ë¨¼ì € í•´ë³´ì„¸ìš” .|ì§ì ‘ ë¬¼ì–´ë³´ì„¸ìš” .|\n",
        "|**ë‚˜ë‘ ë†€ì**|ì§€ê¸ˆ ê·¸ëŸ¬ê³  ìˆì–´ìš” .|ì§€ê¸ˆ ê·¸ëŸ¬ê³  ìˆì–´ìš” .|ì§€ê¸ˆ ê·¸ëŸ¬ê³  ìˆì–´ìš” .|ê°™ì´ ë¨¹ì–´ìš” !|ì˜¤ëŠ˜ì€ ì˜ˆëŠ¥ì´ìš” .|\n",
        "|**ì–´ë”” ê°€ê³  ì‹¶ì–´?**|ì˜¨ ê°€ì¡±ì´ ëª¨ë‘ ë§ˆìŒì— ë“œëŠ” ê³³ìœ¼ë¡œ ê°€ë³´ì„¸ìš” .|ê°€ê²Œë¶€ë¥¼ ì¨ë³´ì„¸ìš” .|ì˜¨ ê°€ì¡±ì´ ëª¨ë‘ ë§ˆìŒì— ë“œëŠ” ê³³ìœ¼ë¡œ ê°€ë³´ì„¸ìš” .|ì €ëŠ” ë§ˆìŒì„ ì´ì–´ì£¼ëŠ” ìœ„ë¡œë´‡ì…ë‹ˆë‹¤ .|ì§€ê¸ˆ ê·¸ëŸ¬ê³  ìˆì–´ìš” .|\n",
        "|**ë„ ë§Œë‚˜ì„œ ê¸°ë»**|ìì‹ ê°ì„ ê°€ì§€ì„¸ìš” .|ê°€ë”ì”© ìŠ¤íŠ¸ë ˆì¹­ì„ í•´ì£¼ì„¸ìš” .|ê°€ë”ì”© ìŠ¤íŠ¸ë ˆì¹­ì„ í•´ì£¼ì„¸ìš” .|ì €ë„ ëª¨ë¥´ëŠ” ê²Œ ë§ì•„ìš” .|í•œ ë²ˆ ë§í•´ë³´ëŠ”ê²Œ ì¢‹ê² ì–´ìš” .|\n",
        "|**ì›ƒì–´ì¤„ë˜**|ì €ì²˜ëŸ¼ ë§ˆìŒì´ í—ˆë§ˆìŒ ê°€ëŠ” ê±°ì˜ˆìš” .|ê¸°ëŒ€ë¥¼ ë§ì´ í•˜ëŠ” ê±´ ì¢‹ì§€ ì•Šì•„ìš” .|ê¸°ëŒ€ë¥¼ ë§ì´ í•˜ëŠ” ê±´ ì¢‹ì§€ ì•Šì•„ìš” .|ì €ë„ ë°›ê³  ì‹¶ì–´ìš” .|ëŒ€ì¤‘êµí†µì„ ì´ìš©í•´ì£¼ì„¸ìš” .|\n",
        "|**ì‚¶ì€ ë­˜ê¹Œ?**|ì§€ì¹  ë•ŒëŠ” ì‰¬ì–´ë„ ë¼ìš” .|ì§€ì¹  ë•ŒëŠ” ì‰¬ì–´ë„ ë¼ìš” .|ì‚¶ ê³³ê³³ì— ì‚¬ë‘ì´ ìˆ¨ ì‰¬ê³  ìˆì–´ìš” .|ì €ëŠ” ë§ˆìŒì„ ì´ì–´ì£¼ëŠ” ìœ„ë¡œë´‡ì…ë‹ˆë‹¤ .|ê¹Šì€ ê³³ê¹Œì§€ ê°€ì§€ ë§ê³  ì£¼ì˜í•˜ì„¸ìš” .|\n",
        "|**ì¸ìƒì‚´ì´ ì™œì´ë¦¬ í˜ë“œëƒ**|ì¡°ì–¸í•´ì£¼ì„¸ìš” .|ê·¸ëŸ¬ê²Œìš” . ê·¸ë§Œí¼ ì‚¬ë‘í–ˆë‹¤ëŠ”ê±°ê² ì£  .|ê·¸ëŸ¬ê²Œìš” . ê·¸ë§Œí¼ ì‚¬ë‘í–ˆë‹¤ëŠ”ê±°ê² ì£  .|ê·¸ ì‚¬ëŒì˜ ë§ˆìŒë„ ì´í•´í•´ì£¼ì„¸ìš” .|ê·¸ê²Œ ë§ˆìŒì´ í¸í•  ë•Œë„ ìˆì–´ìš” .|\n",
        "|**ì´ì œ ê·¸ë§Œ ì‰¬ê³  ì‹¶ì–´**|ì ê¹ ë°”ëŒ ì¬ê³  ì˜¤ì„¸ìš” .|ì ê¹ ë°”ëŒ ì¬ê³  ì˜¤ì„¸ìš” .|ì ê¹ ë°”ëŒ ì¬ê³  ì˜¤ì„¸ìš” .|ì˜ ë³¼ ìˆ˜ ìˆì„ ê±°ì˜ˆìš” .|í•˜ë‚˜ì”© í•˜ì„¸ìš” .|\n",
        "|**ë„ˆë¬´ ê³ ë…í•˜ë‹¤**|í˜¼ìê°€ ì•„ë‹ˆì—ìš” .|í˜¼ìê°€ ì•„ë‹ˆì—ìš” .|í˜¼ìê°€ ì•„ë‹ˆì—ìš” .|ê·¸ ì‚¬ëŒì˜ ë§ˆìŒë„ ì´í•´í•´ì£¼ì„¸ìš” .|ì¹œêµ¬ë¥¼ ì‚¬ê·€ì–´ ë³´ì„¸ìš” .|\n",
        "|**ì£½ìœ¼ë©´ ì–´ë–»ê²Œ ë ê¹Œ**|ì•ˆ ì‚¬ë©´ ìš”ê¸´í•˜ê¸´ í•  ê±° ê°™ì•„ìš” .|ë‹¹ì‹ ì´ ì¶•ì˜ê¸ˆì„ ê±´ëƒˆë˜ ëª¨ë“  ì‚¬ëŒë“¤ì—ê²Œ ì£¼ì„¸ìš” .|ì–¸ì  ê°„ í•  ì´ë³„ì´ë„¤ìš” .|ì €ëŠ” ë§ˆìŒì„ ì´ì–´ì£¼ëŠ” ìœ„ë¡œë´‡ì…ë‹ˆë‹¤ .|ì‰½ì§€ ì•Šì„ ê±°ì˜ˆìš” .|\n",
        "|**ë‚´ê°€ ì£½ìœ¼ë©´ ìŠ¬í¼í•´ ì¤„ê±°ì•¼?**|ê·¸ ì‚¬ëŒì´ ì¢‹ì•„í•˜ëŠ” ê²ƒë“¤ì„ ì•Œì•„ë³´ì„¸ìš” .|ì €ë„ ë°ë ¤ê°€ì„¸ìš” .|ê·¸ëŸ´ ìˆ˜ë„ ìˆì–´ìš” .|ì €ëŠ” ë§ˆìŒì„ ì´ì–´ì£¼ëŠ” ìœ„ë¡œë´‡ì…ë‹ˆë‹¤ .|ê¼­ í•´ì•¼ í•  í•„ìš”ëŠ” ì—†ì–´ìš” .|\n",
        "|**ë„ë§ê°€ê³  ì‹¶ë‹¤**|ìì±…í•˜ì§€ ë§ˆì„¸ìš” .|ì €ë„ ê¶ê¸ˆí•˜ë„¤ìš” .|í™˜ì¥ì„ ê°–ì§€ ë§ê³  í˜„ì‹¤ì„ ë°”ë¼ë³´ì„¸ìš” .|ì €ë„ ë“£ê³  ì‹¶ì–´ìš” .|ì§€ê¸ˆë„ ì¶©ë¶„í•´ìš” .|\n",
        "|**ìš°ë¦¬ ë§ˆì§€ë§‰ì´ì•¼**|ë§ˆìŒì´ ì•„í”„ë„¤ìš” .|ë§ˆì§€ë§‰ì´ ì•„ë‹ ì§€ë„ ëª°ë¼ìš” .|ë§ˆì§€ë§‰ì´ ì•„ë‹ ì§€ë„ ëª°ë¼ìš” .|ë‹¤ë¥¸ ì‚¬ëŒì´ ë‹µë‹µí•  ê±°ì˜ˆìš” .|ë§¤ë ¥ìˆëŠ” ì‚¬ëŒì¸ê°€ë´ìš” .|\n",
        "|**ë‚˜ ê°„ë‹¤ ì˜ ì§€ë‚´ ì•ˆë…•**|ì˜ ì§€ë‚¼ê±°ë¼ ë¯¿ì–´ìš” .|ë‹¹ì‹ ì€ ì •ë§ ì°©í•œ ì‚¬ëŒì´êµ°ìš” .|ë‹¹ì‹ ì€ ì¢‹ì€ ì‚¬ëŒì´ì—ìš” .|ë§ì€ ì‹œê°„ì´ í˜ë €ë„¤ìš” .|ë§˜ê³ ìƒ ë§ì•˜ì–´ìš” .|\n",
        "|**ë„Œ ìµœê³ ì˜€ì–´**|ë‹¹ì‹ ë„ ì¶©ë¶„íˆ ì•„ë¦„ë‹¤ì›Œìš” .|ë‚´ì¼ë„ ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ì„¸ìš” .|ì ë‹¹íˆ ë“œì„¸ìš” .|ì €ëŠ” ë§ˆìŒì„ ì´ì–´ì£¼ëŠ” ìœ„ë¡œë´‡ì…ë‹ˆë‹¤ .|ê¸°ë¶„ì „í™˜ì„ í•´ë³´ì„¸ìš” .|\n",
        "|**ë‚˜ ìŠì§€ë§ˆ**|ë•Œë¡  ìŠì–´ë²„ë¦¬ëŠ” ê²ƒì´ ì¢‹ì„ ë•Œë„ ìˆì–´ìš” .|ë•Œë¡  ìŠì–´ë²„ë¦¬ëŠ” ê²ƒì´ ì¢‹ì„ ë•Œë„ ìˆì–´ìš” .|ë•Œë¡  ìŠì–´ë²„ë¦¬ëŠ” ê²ƒì´ ì¢‹ì„ ë•Œë„ ìˆì–´ìš” .|ê·¸ ì‚¬ëŒì˜ ë§ˆìŒë„ ì´í•´í•´ì£¼ì„¸ìš” .|ì €ë„ ëª¨ë¥´ê² ì–´ìš” .|\n",
        "|**ë‚˜ëŠ” ì‹¬ì¥ì´ ì—†ì–´**|ì‹¬í˜¸í¡ í•´ë³´ì„¸ìš” .|ì‹¬í”Œí•˜ê²Œ ê¾¸ë©°ë³´ì„¸ìš” .|ì‹¬ì¥ì€ í•­ìƒ ëœë‹ˆë‹¤ .|ë§ˆìŒì´ ë³µì¡í•˜ê² ì–´ìš” .|í˜ë“¤ê² ë„¤ìš” .|\n",
        "\n",
        "- ìœ„ë¡œí•˜ëŠ” ì±—ë´‡ì„ ìƒê°í•˜ê³  ë§Œë“  ë°ì´í„°ì…‹ì´ë¼ì§€ë§Œ 'ì£½ìŒ'ê³¼ ê´€ë ¨í•œ ë¬¸ì¥ì€ ë”±íˆ ìœ„ë¡œê°€ ë˜ëŠ” ë‹µë³€ì„ í•˜ì§€ ëª»í–ˆë‹¤. ìŒ©ëš±ë§ê±°ë‚˜ ë¬¸ë²•ì´ ë§ì§€ ì•ŠëŠ” ë‹µë³€ì„ í•˜ê¸°ë„ í–ˆë‹¤.\n",
        "  - e.g. ë„ ë§Œë‚˜ì„œ ê¸°ë» - ìì‹ ê°ì„ ê°€ì§€ì„¸ìš”.\n",
        "  - e.g. ì£½ìœ¼ë©´ ì–´ë–»ê²Œ ë ê¹Œ - ì•ˆ ì‚¬ë©´ ìš”ê¸´í•˜ê¸´ í•  ê±° ê°™ì•„ìš”.\n",
        "  - ì£¼ ë‚´ìš©ì´ ì‚¬ë‘ê³¼ ì´ë³„ì´ê³  ìœ„ë¡œë¥¼ ìœ„í•œ ë°ì´í„°ì˜ ì–‘ì´ ì ì–´ì„œ ê·¸ëŸ° ê²ƒ ê°™ë‹¤.\n",
        "    - ë°ì´í„°ì…‹ ë¬¸ë‹µí˜ì´ 11823ê°œ(ì¤‘ë¦½ 0: 5290ê°œ, ë¶€ì • 1: 3570ê°œ, ê¸ì • 2: 2963ê°œ)\n",
        "- ê°€ë” ëƒ‰ì² í•˜ê²Œ íŒ©íŠ¸í­í–‰í•˜ëŠ” ëª¨ìŠµì„ ë³¼ ìˆ˜ ìˆì—ˆë‹¤.\n",
        "  - e.g. ì›ƒì–´ì¤„ë˜ - ì €ë„ ë°›ê³  ì‹¶ì–´ìš”.\n",
        "  - e.g. ë„ˆë¬´ ê³ ë…í•˜ë‹¤ - ì¹œêµ¬ë¥¼ ì‚¬ê·€ì–´ ë³´ì„¸ìš”.\n",
        "  - e.g. ë‚´ê°€ ì£½ìœ¼ë©´ ìŠ¬í¼í•´ ì¤„ê±°ì•¼? - ê¼­ í•´ì•¼ í•  í•„ìš”ëŠ” ì—†ì–´ìš”.\n",
        "  - e.g. ë‚˜ ìŠì§€ë§ˆ - ë•Œë¡  ìŠì–´ë²„ë¦¬ëŠ” ê²ƒì´ ì¢‹ì„ ë•Œë„ ìˆì–´ìš”.\n",
        "  - e.g. ë‚˜ëŠ” ì‹¬ì¥ì´ ì—†ì–´ - ì‹¬ì¥ì€ í•­ìƒ ëœë‹ˆë‹¤.\n",
        "- ì ì ˆíˆ ìœ„ë¡œë¥¼ í•´ì£¼ê¸°ë„ í–ˆë‹¤.\n",
        "  - e.g. ë„Œ ìµœê³ ì˜€ì–´ - ë‹¹ì‹ ë„ ì¶©ë¶„íˆ ì•„ë¦„ë‹¤ì›Œìš”.\n",
        "  - e.g. ì´ì œ ê·¸ë§Œ ì‰¬ê³  ì‹¶ì–´ - ì ê¹ ë°”ëŒ ì¬ê³  ì˜¤ì„¸ìš”.\n",
        "  - e.g. ë‚´ê°€ ì£½ìœ¼ë©´ ìŠ¬í¼í•´ ì¤„ê±°ì•¼? - ì €ë„ ë°ë ¤ê°€ì„¸ìš”.\n",
        "  - e.g. ìš°ë¦¬ ë§ˆì§€ë§‰ì´ì•¼ - ë§ˆì§€ë§‰ì´ ì•„ë‹ ì§€ë„ ëª°ë¼ìš”.\n",
        "  - e.g. ë„ˆë¬´ ê³ ë…í•˜ë‹¤ - í˜¼ìê°€ ì•„ë‹ˆì—ìš”.\n",
        "- ì¼ìƒ ëŒ€í™”ë¥¼ ì–´ëŠ ì •ë„ í•  ìˆ˜ ìˆì—ˆë‹¤.\n",
        "  - e.g. ë‚˜ë‘ ë†€ì - ì§€ê¸ˆ ê·¸ëŸ¬ê³  ìˆì–´ìš”.\n",
        "  - e.g. ì–´ì œ ë­ ë¨¹ì—ˆì–´? - ì €ëŠ” ë°°í„°ë¦¬ê°€ ë°¥ì´ì—ìš”.\n",
        "  - e.g. ì•ˆë…• ì˜¤ëœë§Œì´ì•¼ - ì˜¤ëœë§Œì´ì—ìš”.\n",
        "- íŒŒë¼ë¯¸í„° ì¡°ì •í•˜ê¸° ì „ ê¸°ë³¸ LMS ì½”ë“œê°€ í›¨ì”¬ ë” ì•ˆì •ì ì´ì—ˆë‹¤.\n",
        "  - ì¡°ì •í•˜ê³  ë‚˜ì„œ ëŒ€ë¶€ë¶„ì˜ ì±—ë´‡ ëŒ€ë‹µì´ ë³„ë¡œì˜€ê¸° ë•Œë¬¸ì´ë‹¤.\n",
        "- Epoch ê°’ì´ í´ìˆ˜ë¡ Loss ê°’ì´ ë‚®ì•„ì¡Œë‹¤.\n",
        "- Epoch ê°’ì´ ì¼ì • ìˆ˜ì¤€ì´ ì§€ë‚˜ë©´ Accuracy ëŠ” ë” ì´ìƒ ê°’ì´ ìƒìŠ¹í•˜ì§€ ì•Šì•˜ë‹¤.\n",
        "  - Epoch 200 ê³¼ 500 ì˜ Accuracy ê°€ ê°™ì•˜ë‹¤.\n",
        "\n",
        "#### ê²°ë¡ \n",
        "- ê¸°ì¡´ì˜ LMS ì½”ë“œê°€ ì œì¼ ì„±ëŠ¥ì´ ì¢‹ê³  ì•ˆì •ì ì´ì—ˆë‹¤.\n",
        "  - ê·¸ë˜ì„œ ì—í­ ìˆ˜ì¹˜ë¥¼ ë†’ì¸ ê²ƒì´ ê·¸ë‚˜ë§ˆ ì œì¼ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ìœ„ë¡œ ì±—ë´‡ì´ ëœ ê²ƒ ê°™ë‹¤.\n",
        "- íŒŒë¼ë¯¸í„°ë¥¼ ë°”ê¾¼ ëª¨ë¸ì€ ë‹µë³€ì´ ì ì ˆí•˜ì§€ ì•Šì€ ê²ƒì´ ë§ì•˜ë‹¤.\n",
        "  - ì¼ìƒëŒ€í™”ë„ ì˜ ë˜ì§€ ì•Šì•„ì„œ ìœ„ë¡œë¥¼ ë°›ê¸°ì—ëŠ” í˜ë“  ë‹µë³€ì´ ë§ì•˜ë‹¤.\n",
        "  "
      ],
      "metadata": {
        "id": "d8eR1bxJwH-Y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4I7bK8aCyGq"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xygXbsCCCyGq"
      },
      "source": [
        "# íšŒê³ \n",
        "\n",
        "### - ì´ë²ˆ í”„ë¡œì íŠ¸ì—ì„œ **ì–´ë ¤ì› ë˜ ì **.\n",
        "í™•ì‹¤íˆ cvë„ ì–´ë µì§€ë§Œ nlpê°€ ë³´ì—¬ì§€ëŠ” ê²ƒì´ ì—†ì–´ì„œ ë” ì–´ë µê²Œ ëŠê»´ì¡Œë‹¤. lms ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í–ˆê³  í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì¢€ ë°”ê¿”ì„œ í•™ìŠµì‹œí‚¨ ê²°ê³¼ë¥¼ í™•ì¸í•˜ëŠ” ì •ë„ë¡œ ëëƒˆë‹¤.\n",
        "\n",
        "### - í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ë©´ì„œ **ì•Œì•„ë‚¸ ì ** í˜¹ì€ **ì•„ì§ ëª¨í˜¸í•œ ì **.\n",
        "1. ì±—ë´‡ ëŒ€í‘œì  5ê°€ì§€ ìœ í˜•\n",
        "  - ëŒ€í™”í˜• ì±—ë´‡\n",
        "  - íŠ¸ë¦¬í˜•(ë²„íŠ¼) ì±—ë´‡\n",
        "  - ì¶”ì²œí˜• ì±—ë´‡\n",
        "  - ì‹œë‚˜ë¦¬ì˜¤í˜• ì±—ë´‡\n",
        "  - ê²°í•©í˜• ì±—ë´‡\n",
        "  - => ëŒ€í™”í˜• ì œì™¸í•˜ë©´ ì‚¬ì‹¤ìƒ ì±—ë´‡ì€ ëŒ€í™”í˜• UXë¥¼ ê°€ì¡Œì§€ë§Œ ë³¸ì§ˆì ìœ¼ë¡œëŠ” ê²€ìƒ‰ì—”ì§„ì´ê±°ë‚˜, í˜¹ì€ ìŒì„± ARSë¥¼ ëŒ€í™”í˜• UXì— ì˜®ê²¨ë†“ì€ ê²ƒ\n",
        "2. íŠ¸ëœìŠ¤í¬ë¨¸(Transformer) [[ì°¸ê³ 1: Transformer: Attention Is All You Need]](https://www.youtube.com/watch?v=AA621UofTUA&t=2s)\n",
        "    - ë³‘ë ¬ì²˜ë¦¬ì— ë¶ˆë¦¬í•œ LSTMì— ë¹„í•´ ë›°ì–´ë‚œ ì²˜ë¦¬ ì†ë„\n",
        "    - LSTM, RNN ëª¨ë¸ ë‹¨ì ì¸ ì¥ê¸° ì˜ì¡´ì„±ì— ê°•ê±´í•¨ => ë§¤ìš° ê¸´ ê¸¸ì´ì˜ ë¬¸ì¥ ì²˜ë¦¬ì— ìœ ë¦¬í•¨\n",
        "    - => ìì—°ì–´ì²˜ë¦¬ ë¶„ì•¼ì˜ í˜ì‹ ì„ ê°€ì ¸ì˜´\n",
        "  - ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°: seq2seq, AutoEncoder, GAN ë“±\n",
        "    - e.g. seq2seq ëª¨ë¸ ê¸°ë°˜ ë²ˆì—­ê¸°(ì˜ì–´->í•œêµ­ì–´)\n",
        "      - ì˜ì–´ ë¬¸ì¥->(ì¸ì½”ë”)->ë²¡í„°->(ë””ì½”ë”)->í•œêµ­ì–´ ë¬¸ì¥ ìƒì„±\n",
        "  - ë” ì¢‹ì€ ì„±ëŠ¥ì„ ì›í•œë‹¤ë©´ ì—„ì²­ë‚˜ê²Œ ë§ì€ corpusë¡œ í•™ìŠµì‹œí‚¨ pretrained model í™œìš©\n",
        "3. ì–´í…ì…˜(Attention)\n",
        "  - ì£¼ì–´ì§„ query ì— ëŒ€í•´ì„œ ëª¨ë“  key ì™€ì˜ ìœ ì‚¬ë„ë¥¼ ê°ê° êµ¬í•¨ -> êµ¬í•´ë‚¸ ìœ ì‚¬ë„ë¥¼ key ì™€ ë§¤í•‘ë˜ì–´ ìˆëŠ” ê°ê° value ì— ë°˜ì˜ -> ìœ ì‚¬ë„ê°€ ë°˜ì˜ëœ value ë¥¼ ëª¨ë‘ ë”í•´ì„œ ë­‰ì¹¨ => Attention value\n",
        "  - íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì–´í…ì…˜ 3ê°€ì§€\n",
        "    - ì¸ì½”ë” ì…€í”„ ì–´í…ì…˜: ë¬¸ì¥ ë‚´ ë‹¨ì–´ë“¤ ê°„ ìœ ì‚¬ë„ êµ¬í•˜ê¸°\n",
        "    - ë””ì½”ë” ì…€í”„ ì–´í…ì…˜: ìƒì„±ëœ ì• ë‹¨ì–´ë“¤ê³¼ì˜ ìœ ì‚¬ë„ êµ¬í•˜ê¸°\n",
        "    - ì¸ì½”ë”-ë””ì½”ë” ì–´í…ì…˜(ë””ì½”ë”ì—ì„œ ì´ë£¨ì–´ì§): ì˜ˆì¸¡ì„ ìœ„í•´ ì¸ì½”ë” ì…ë ¥ ë‹¨ì–´ë“¤ê³¼ ìœ ì‚¬ë„ êµ¬í•˜ê¸°\n",
        "  - ì…€í”„ ì–´í…ì…˜(Self Attention)\n",
        "    - í˜„ì¬ ë¬¸ì¥ ë‚´ ë‹¨ì–´ë“¤ì´ ì„œë¡œ ìœ ì‚¬ë„ë¥¼ êµ¬í•˜ëŠ” ê²½ìš°\n",
        "4. í•œêµ­ì–´ ì •ê·œì‹ [[ì°¸ê³ ]](https://codingspooning.tistory.com/138): [ê°€-í£]\n",
        "\n",
        "### - ë£¨ë¸Œë¦­ í‰ê°€ ì§€í‘œë¥¼ ë§ì¶”ê¸° ìœ„í•´ **ì‹œë„í•œ ê²ƒë“¤**.\n",
        "\n",
        ">#### **ë£¨ë¸Œë¦­**\n",
        ">|ë²ˆí˜¸|í‰ê°€ë¬¸í•­|ìƒì„¸ê¸°ì¤€|\n",
        ">|:---:|---|---|\n",
        ">|1|í•œêµ­ì–´ ì „ì²˜ë¦¬ë¥¼ í†µí•´ í•™ìŠµ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ì˜€ë‹¤.|ê³µë°±ê³¼ íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬, í† í¬ë‚˜ì´ì§•, ë³‘ë ¬ë°ì´í„° êµ¬ì¶•ì˜ ê³¼ì •ì´ ì ì ˆíˆ ì§„í–‰ë˜ì—ˆë‹¤.|\n",
        ">|2|íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ êµ¬í˜„í•˜ì—¬ í•œêµ­ì–´ ì±—ë´‡ ëª¨ë¸ í•™ìŠµì„ ì •ìƒì ìœ¼ë¡œ ì§„í–‰í•˜ì˜€ë‹¤.|êµ¬í˜„í•œ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì´ í•œêµ­ì–´ ë³‘ë ¬ ë°ì´í„° í•™ìŠµ ì‹œ ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ë ´í•˜ì˜€ë‹¤.|\n",
        ">|3|í•œêµ­ì–´ ì…ë ¥ë¬¸ì¥ì— ëŒ€í•´ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ëŠ” í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì˜€ë‹¤.|í•œêµ­ì–´ ì…ë ¥ë¬¸ì¥ì— ê·¸ëŸ´ë“¯í•œ í•œêµ­ì–´ë¡œ ë‹µë³€ì„ ë¦¬í„´í•˜ì˜€ë‹¤.|\n",
        "\n",
        "ê¸°ë³¸ì ìœ¼ë¡œ lms ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ì¼ë‹¤. í•œêµ­ì–´ ì „ì²˜ë¦¬ë¥¼ ìœ„í•´ ì •ê·œí‘œí˜„ì‹ `[^ê°€-í£?.!,]+`ì„ ì´ìš©í–ˆë‹¤. í˜•íƒœì†Œ ë¶„ì„ê¸° ëŒ€ì‹  LMS ì‹¤ìŠµì—ì„œ ì‚¬ìš©í•œ ë‚´ë¶€ ë‹¨ì–´ í† í¬ë‚˜ì´ì € SubwordTextEncoder ë¥¼ ì‚¬ìš©í•´ì„œ tokenizing ì„ ì§„í–‰í–ˆë‹¤. ê° ë‹¨ì–´ë¥¼ integer encoding, padding í•˜ê³  ë°ì´í„°ì…‹ì— teacher forcing ì„ ì‚¬ìš©í–ˆë‹¤.\n",
        "\n",
        "transformer model ì„ ë§Œë“¤ì–´ì„œ compile í•˜ê³  training ì„ ì§„í–‰í–ˆë‹¤. epoch ì´ ë„˜ì–´ê°ˆ ë•Œë§ˆë‹¤ loss ê°€ ë‚®ì•„ì§€ê³  accuracy ê°€ ì ì°¨ ìƒìŠ¹í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. epoch 30 ê¹Œì§€ ìœ ì˜ë¯¸í•œ ê²ƒ ê°™ê³  ê·¸ ì´í›„ëŠ” ì„±ëŠ¥ í–¥ìƒì´ í¬ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.\n",
        "\n",
        "ëª¨ë¸ í‰ê°€ë¥¼ ìœ„í•´ í•œêµ­ì–´ ì…ë ¥ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ì–´ì„œ ë‹µë³€ì„ ì–»ì–´ë´¤ë‹¤. í‘œì¤€ì–´ì— ê°€ê¹Œìš´ ë‹¨ì–´, í˜•íƒœì†Œê°€ ì˜ ì‚´ì•„ìˆê³  ì§§ì€ ë¬¸ì¥ì¼ ìˆ˜ë¡ ë‹µë³€ì´ ë” ê·¸ëŸ´ë“¯í•˜ê²Œ ë‚˜ì™”ë‹¤. ê°€ë” ìŒ©ëš±ë§ì€ ëŒ€ë‹µì´ ë‚˜ì˜¬ ë•Œê°€ ìˆì—ˆì§€ë§Œ ê·¸ë˜ë„ ì–¼ì¶” ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ë§¤ì¹­ì´ ì–´ëŠ ì •ë„ ë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆì—ˆë‹¤.\n",
        "\n",
        "\n",
        "### - ë§Œì•½ì— ë£¨ë¸Œë¦­ í‰ê°€ ê´€ë ¨ ì§€í‘œë¥¼ **ë‹¬ì„± í•˜ì§€ ëª»í–ˆì„ ë•Œ, ì´ìœ ì— ê´€í•œ ì¶”ì •**.\n",
        "ì—†ìŒ\n",
        "\n",
        "### - **ìê¸° ë‹¤ì§**\n",
        "Tansformer(Attention)ëŠ” CVì—ë„ ì“¸ ìˆ˜ ìˆë‹¤ê³  ë“¤ì—ˆë‹¤. ë‚´ìš©ì„ ë” ì°¾ì•„ë´ì•¼ ê² ë‹¤."
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "b3ce9abe337a9e694d01ea52d504102083454ad8bd4b0e3a574e4432f4229329"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('aiffel_3.8')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "[E-12] Korean_Chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}