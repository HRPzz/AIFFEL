{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzMUZVp9CyGZ"
      },
      "source": [
        "# 12. 트랜스포머로 만드는 대화형 챗봇\n",
        "\n",
        "**트랜스포머의 인코더 디코더 구조와 셀프 어텐션을 코드를 통해 이해해 본다. 이를 영어와 한국어로 이루어진 챗봇 데이터에 적용해 본다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pS6GeZ4CyGb"
      },
      "source": [
        "## 12-14. 프로젝트: 한국어 데이터로 챗봇 만들기\n",
        "\n",
        "영어로 만들었던 챗봇을 한국어 데이터로 바꿔서 훈련시켜봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYkZMzXuCyGb"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5n6rg4FCyGb"
      },
      "source": [
        "### 목차\n",
        "- Step 1. 데이터 수집하기\n",
        "- Step 2. 데이터 전처리하기\n",
        "- Step 3. SubwordTextEncoder 사용하기\n",
        "- Step 4. 모델 구성하기\n",
        "- Step 5. 모델 평가하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mzfPSW0CyGb"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbRbfV4WC4MF",
        "outputId": "5212b036-e307-4700-ff90-c970079aed33"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6EvMlleCyGc",
        "outputId": "2c88a919-d55d-4f61-d48d-53221ed0429a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSKb4nJcCyGd"
      },
      "source": [
        "### Step 1. 데이터 수집하기\n",
        "- 송영숙님이 공개한 한국어 챗봇 데이터: [songys/Chatbot_data](https://github.com/songys/Chatbot_data/blob/master/ChatbotData.csv)\n",
        "    - 질문에 챗봇이 위로한다는 취지로 답변을 작성\n",
        "    - 챗봇 트레이닝용 문답 페이 11,823개\n",
        "    - 일상다반사(중립) 0, 이별(부정) 1, 사랑(긍정) 2로 레이블링\n",
        "      - 중립 0: 5290개\n",
        "      - 부정 1: 3570개\n",
        "      - 긍정 2: 2963개\n",
        "    - 결측치 없음"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 데이터 불러오기"
      ],
      "metadata": {
        "id": "2XfJycuwBa3F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "owzjbUFRCyGd",
        "outputId": "7bc0000b-9fc6-4925-9115-e8e5ffd82bb1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-774ff89a-7b58-475c-9e75-93cff69b19c1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12시 땡!</td>\n",
              "      <td>하루가 또 가네요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1지망 학교 떨어졌어</td>\n",
              "      <td>위로해 드립니다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3박4일 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3박4일 정도 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PPL 심하네</td>\n",
              "      <td>눈살이 찌푸려지죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11818</th>\n",
              "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
              "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11819</th>\n",
              "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
              "      <td>훔쳐보는 거 티나나봐요.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11820</th>\n",
              "      <td>흑기사 해주는 짝남.</td>\n",
              "      <td>설렜겠어요.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11821</th>\n",
              "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
              "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11822</th>\n",
              "      <td>힘들어서 결혼할까봐</td>\n",
              "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11823 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-774ff89a-7b58-475c-9e75-93cff69b19c1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-774ff89a-7b58-475c-9e75-93cff69b19c1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-774ff89a-7b58-475c-9e75-93cff69b19c1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                             Q                         A  label\n",
              "0                       12시 땡!                하루가 또 가네요.      0\n",
              "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
              "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
              "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
              "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
              "...                        ...                       ...    ...\n",
              "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
              "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
              "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
              "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
              "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
              "\n",
              "[11823 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "csv_path = 'drive/MyDrive/Colab Notebooks/transformer_chatbot/data/ChatbotData.csv'\n",
        "data = pd.read_csv(csv_path)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USbtyHsLCyGe",
        "outputId": "8190c70b-4867-440c-9376-c4d9bfb0b047"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11823\n"
          ]
        }
      ],
      "source": [
        "# 사용할 샘플의 최대 개수\n",
        "MAX_SAMPLES = len(data)\n",
        "print(MAX_SAMPLES)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 데이터 확인"
      ],
      "metadata": {
        "id": "Dn3jz4MdJFst"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4AwNTg9zCyGe",
        "outputId": "f2c132e4-cda0-47fd-ed42-a9c9651e9d32"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'12시 땡!'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# 질문(Q)\n",
        "data['Q'].values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qBCuOlPnCyGe",
        "outputId": "9575b2d4-6c92-44f1-f9a1-fafa6e922486"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'하루가 또 가네요.'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# 답변(A)\n",
        "data['A'].values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sG4mL6r5CyGe",
        "outputId": "f6dd40c3-d578-430f-ed63-6af0ff6533e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    5290\n",
              "1    3570\n",
              "2    2963\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# 감정(label): 중립 0, 부정 1, 긍정 2 레이블링\n",
        "data['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU85dlhyCyGf",
        "outputId": "996bea4b-d21f-45c8-c623-ef291a92ca48"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Q        0\n",
              "A        0\n",
              "label    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# 결측치 확인\n",
        "data.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm0YuaZvCyGf"
      },
      "source": [
        "### Step 2. 데이터 전처리하기\n",
        "영어 데이터와는 전혀 다른 데이터인 만큼 영어 데이터에 사용했던 전처리와 일부 동일한 전처리도 필요하겠지만 전체적으로는 다른 전처리를 수행해야 할 수도 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSVG1cHNCyGf"
      },
      "source": [
        "- 전처리 함수\n",
        "    - 정규 표현식으로 구두점 제거 => 단어 토크나이징 방해되지 않게 정제하는 기능"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWQLVWs1CyGf",
        "outputId": "25a032dd-799f-49fb-9bcc-0855ecee7869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "# 전처리 함수\n",
        "def preprocess_sentence(sentence):\n",
        "  sentence = sentence.lower().strip()\n",
        "\n",
        "  # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
        "  # 예를 들어서 \"I am a student.\" => \"I am a student .\"와 같이\n",
        "  # student와 온점 사이에 거리를 만듭니다.\n",
        "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "\n",
        "  # (가-힣, \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백인 ' '로 대체합니다.\n",
        "  sentence = re.sub(r\"[^가-힣?.!,]+\", \" \", sentence)\n",
        "  sentence = sentence.strip()\n",
        "  return sentence\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZPVO10RCyGf"
      },
      "source": [
        "- 데이터 로드 함수\n",
        "    - 데이터 로드와 동시에 문답 페이 전처리 진행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLbIjfvfCyGf",
        "outputId": "224a44f9-afc1-478d-e936-7ded0e81cfb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "# 질문과 답변의 쌍인 데이터셋을 구성하기 위한 데이터 로드 함수\n",
        "def load_conversations():\n",
        "  inputs, outputs = [], []\n",
        "  for i in range(MAX_SAMPLES):\n",
        "    # 전처리 함수를 질문에 해당되는 inputs와 답변에 해당되는 outputs에 적용.\n",
        "    inputs.append(preprocess_sentence(data['Q'].values[i]))\n",
        "    outputs.append(preprocess_sentence(data['A'].values[i]))\n",
        "\n",
        "  return inputs, outputs\n",
        "\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZ07Fyl4CyGg",
        "outputId": "a8034609-febe-4b25-dc9e-a9169f373a59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플 수 : 11823\n",
            "전체 샘플 수 : 11823\n"
          ]
        }
      ],
      "source": [
        "# 데이터를 로드하고 전처리하여 질문을 questions, 답변을 answers에 저장합니다.\n",
        "questions, answers = load_conversations()\n",
        "print('전체 샘플 수 :', len(questions))\n",
        "print('전체 샘플 수 :', len(answers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiAvWt6oCyGg",
        "outputId": "8a3f68b2-af43-480e-8096-5469cb612007"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전처리 후의 22번째 질문 샘플: 가스비 장난 아님\n",
            "전처리 후의 22번째 답변 샘플: 다음 달에는 더 절약해봐요 .\n"
          ]
        }
      ],
      "source": [
        "print('전처리 후의 22번째 질문 샘플: {}'.format(questions[21]))\n",
        "print('전처리 후의 22번째 답변 샘플: {}'.format(answers[21]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg_V35tWCyGg"
      },
      "source": [
        "### Step 3. SubwordTextEncoder 사용하기\n",
        "한국어 데이터는 형태소 분석기를 사용하여 토크나이징을 해야 한다고 많은 분이 알고 있습니다. 하지만 여기서는 형태소 분석기가 아닌 위 실습에서 사용했던 내부 단어 토크나이저인 `SubwordTextEncoder`를 그대로 사용해보세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Kq4UoKjCyGg"
      },
      "source": [
        "#### 1. 단어장(Vocabulary) 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ydTn-i0CyGg",
        "outputId": "5e033e41-1b76-4461-82b6-789adcab23f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "살짝 오래 걸릴 수 있어요. 스트레칭 한 번 해볼까요? 👐\n",
            "슝=3 \n"
          ]
        }
      ],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "print(\"살짝 오래 걸릴 수 있어요. 스트레칭 한 번 해볼까요? 👐\")\n",
        "\n",
        "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성. (Tensorflow 2.3.0 이상) (클라우드는 2.4 입니다)\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)\n",
        "print(\"슝=3 \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JovnGTAJCyGh",
        "outputId": "f9d627cb-4de5-45bb-bfe0-5a614fb06e8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "# 시작 토큰과 종료 토큰에 고유한 정수를 부여합니다.\n",
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npgPZaBMCyGh",
        "outputId": "a3ce5a6e-789d-4da0-a0e9-de10456fbc87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "START_TOKEN의 번호 : [8127]\n",
            "END_TOKEN의 번호 : [8128]\n"
          ]
        }
      ],
      "source": [
        "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
        "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8hm8EFaCyGh",
        "outputId": "c87821a7-6beb-45df-cf57-a4d5bb868b89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8129\n"
          ]
        }
      ],
      "source": [
        "# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정합니다.\n",
        "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
        "print(VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzExCV7MCyGh"
      },
      "source": [
        "#### 2. 각 단어를 고유한 정수로 인코딩(Integer encoding) & 패딩(Padding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPV9kFWZCyGh",
        "outputId": "69c38607-b5cc-4617-e5fd-6d760adba66b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정수 인코딩 후의 21번째 질문 샘플: [5742, 612, 2481, 4148]\n",
            "정수 인코딩 후의 21번째 답변 샘플: [2352, 7481, 7, 6245, 97, 1]\n"
          ]
        }
      ],
      "source": [
        "# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행.\n",
        "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(questions[21])))\n",
        "print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(answers[21])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYzcSNdrCyGi",
        "outputId": "250ecf8e-c481-4d50-934b-fc70ced1304f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40\n"
          ]
        }
      ],
      "source": [
        "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이\n",
        "MAX_LENGTH = 40\n",
        "print(MAX_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0lreSDxCyGi",
        "outputId": "c7ef86b4-f7ca-4039-b637-beeef3a9d7ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "  tokenized_inputs, tokenized_outputs = [], []\n",
        "  \n",
        "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
        "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
        "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
        "\n",
        "    # 최대 길이 40 이하인 경우에만 데이터셋으로 허용\n",
        "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
        "      tokenized_inputs.append(sentence1)\n",
        "      tokenized_outputs.append(sentence2)\n",
        "  \n",
        "  # 최대 길이 40으로 모든 데이터셋을 패딩\n",
        "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
        "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
        "  \n",
        "  return tokenized_inputs, tokenized_outputs\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cr0jANPTCyGi",
        "outputId": "1f699cf9-b459-4fdf-82ea-2458a56b2829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어장의 크기 : 8129\n",
            "필터링 후의 질문 샘플 개수: 11823\n",
            "필터링 후의 답변 샘플 개수: 11823\n"
          ]
        }
      ],
      "source": [
        "questions, answers = tokenize_and_filter(questions, answers)\n",
        "print('단어장의 크기 :',(VOCAB_SIZE))\n",
        "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
        "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJIAyQkiCyGi"
      },
      "source": [
        "#### 3. 교사 강요(Teacher Forcing) 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMk6Sp7dCyGi",
        "outputId": "b1ef911c-6a2e-4a7a-add6-c6c6d195a94c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
        "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'inputs': questions,\n",
        "        'dec_inputs': answers[:, :-1]\n",
        "    },\n",
        "    {\n",
        "        'outputs': answers[:, 1:]\n",
        "    },\n",
        "))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3X1ypmSCyGi"
      },
      "source": [
        "### Step 4. 모델 구성하기\n",
        "위 실습 내용을 참고하여 트랜스포머 모델을 구현합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 포지셔널 행렬 구하기\n",
        "  - 트랜스포머는 사인 함수, 코사인 함수 값을 임베딩 벡터에 더함(=단어의 순서 정보를 더함)\n",
        "  - 트랜스포머는 문장 단위로 입력받기 때문에 어순 정보가 필요하다."
      ],
      "metadata": {
        "id": "KlMG0OtYBxhJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6n7s9C9uCyGi",
        "outputId": "cff883af-9eeb-48c5-8257-7ff78e98b752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "# 포지셔널 인코딩 레이어\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, position, d_model):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "  def get_angles(self, position, i, d_model):\n",
        "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "    return position * angles\n",
        "\n",
        "  def positional_encoding(self, position, d_model):\n",
        "    # 각도 배열 생성\n",
        "    angle_rads = self.get_angles(\n",
        "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "        d_model=d_model)\n",
        "\n",
        "    # 배열의 짝수 인덱스에는 sin 함수 적용\n",
        "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "    # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
        "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    # sin과 cosine이 교차되도록 재배열\n",
        "    pos_encoding = tf.stack([sines, cosines], axis=0)\n",
        "    pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
        "    pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
        "\n",
        "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
        "\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 어텐션"
      ],
      "metadata": {
        "id": "MoGl5GiAFI9I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSrmZAoxCyGj",
        "outputId": "98971ca1-150d-46a6-9889-e57efe9583a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "# 스케일드 닷 프로덕트 어텐션 함수\n",
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "  # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
        "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "  # 가중치를 정규화\n",
        "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "  logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "  # 패딩에 마스크 추가\n",
        "  if mask is not None:\n",
        "    logits += (mask * -1e9)\n",
        "\n",
        "  # softmax적용\n",
        "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "  # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
        "  output = tf.matmul(attention_weights, value)\n",
        "  return output\n",
        "\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wK9J1agmCyGj",
        "outputId": "22bb46d8-560c-4301-9838-b207c46be446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "    super(MultiHeadAttention, self).__init__(name=name)\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "  def split_heads(self, inputs, batch_size):\n",
        "    inputs = tf.reshape(\n",
        "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
        "        'value'], inputs['mask']\n",
        "    batch_size = tf.shape(query)[0]\n",
        "\n",
        "    # Q, K, V에 각각 Dense를 적용합니다\n",
        "    query = self.query_dense(query)\n",
        "    key = self.key_dense(key)\n",
        "    value = self.value_dense(value)\n",
        "\n",
        "    # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
        "    query = self.split_heads(query, batch_size)\n",
        "    key = self.split_heads(key, batch_size)\n",
        "    value = self.split_heads(value, batch_size)\n",
        "\n",
        "    # 스케일드 닷 프로덕트 어텐션 함수\n",
        "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "    # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))\n",
        "\n",
        "    # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
        "    outputs = self.dense(concat_attention)\n",
        "\n",
        "    return outputs\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 마스킹"
      ],
      "metadata": {
        "id": "qlcklK1gFFHZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x43qoaUWCyGj",
        "outputId": "5a4d80e6-735d-420d-9c40-ded62a65466e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "def create_padding_mask(x):\n",
        "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
        "  # (batch_size, 1, 1, sequence length)\n",
        "  return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbCLn-S5CyGk",
        "outputId": "7da3e9a3-8831-46da-9ebc-9a68baf8abd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "def create_look_ahead_mask(x):\n",
        "  seq_len = tf.shape(x)[1]\n",
        "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "  padding_mask = create_padding_mask(x)\n",
        "  return tf.maximum(look_ahead_mask, padding_mask)\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzy9Fj4ECyGk",
        "outputId": "98b47a73-4e90-4d7a-9b08-e6d16606a119"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "# 인코더 하나의 레이어를 함수로 구현.\n",
        "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
        "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
        "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "\n",
        "  # 패딩 마스크 사용\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
        "  attention = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention\")({\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "\n",
        "  # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
        "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "  attention = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "  # 두 번째 서브 레이어 : 2개의 완전연결층\n",
        "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
        "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "\n",
        "  # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 인코더"
      ],
      "metadata": {
        "id": "AsQBJD-CE9X6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT9yn3shCyGk",
        "outputId": "34865cb3-b353-484f-83f0-9b880e2f1564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "def encoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            name=\"encoder\"):\n",
        "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "\n",
        "  # 패딩 마스크 사용\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  # 임베딩 레이어\n",
        "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "\n",
        "  # 포지셔널 인코딩\n",
        "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
        "\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "  # num_layers만큼 쌓아올린 인코더의 층.\n",
        "  for i in range(num_layers):\n",
        "    outputs = encoder_layer(\n",
        "        units=units,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "        name=\"encoder_layer_{}\".format(i),\n",
        "    )([outputs, padding_mask])\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 디코더"
      ],
      "metadata": {
        "id": "Zs2fdpTmE_ei"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vd06FyyqCyGl",
        "outputId": "7c4bc744-c005-4fe3-f004-35eb382cf0f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "# 디코더 하나의 레이어를 함수로 구현.\n",
        "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
        "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
        "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
        "  look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
        "  attention1 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': look_ahead_mask\n",
        "      })\n",
        "\n",
        "  # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
        "  attention1 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention1 + inputs)\n",
        "\n",
        "  # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
        "  attention2 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
        "          'query': attention1,\n",
        "          'key': enc_outputs,\n",
        "          'value': enc_outputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "\n",
        "  # 마스크드 멀티 헤드 어텐션의 결과는\n",
        "  # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
        "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
        "  attention2 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention2 + attention1)\n",
        "\n",
        "  # 세 번째 서브 레이어 : 2개의 완전연결층\n",
        "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
        "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "\n",
        "  # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(outputs + attention2)\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntTgKNCwCyGl",
        "outputId": "93f5d31e-07e6-435d-e5f3-c608bac28371"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "def decoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            name='decoder'):\n",
        "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
        "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
        "  look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name='look_ahead_mask')\n",
        "\n",
        "  # 패딩 마스크\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "  \n",
        "  # 임베딩 레이어\n",
        "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "\n",
        "  # 포지셔널 인코딩\n",
        "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
        "\n",
        "  # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "  for i in range(num_layers):\n",
        "    outputs = decoder_layer(\n",
        "        units=units,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dropout=dropout,\n",
        "        name='decoder_layer_{}'.format(i),\n",
        "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0X2GPr8CyGl"
      },
      "source": [
        "- 트랜스포머 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tqv0u5D1CyGl",
        "outputId": "8401776f-62f0-4e41-f9be-39fec5a8c280"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "def transformer(vocab_size,\n",
        "                num_layers,\n",
        "                units,\n",
        "                d_model,\n",
        "                num_heads,\n",
        "                dropout,\n",
        "                name=\"transformer\"):\n",
        "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
        "\n",
        "  # 인코더에서 패딩을 위한 마스크\n",
        "  enc_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='enc_padding_mask')(inputs)\n",
        "\n",
        "  # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
        "  # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
        "  look_ahead_mask = tf.keras.layers.Lambda(\n",
        "      create_look_ahead_mask,\n",
        "      output_shape=(1, None, None),\n",
        "      name='look_ahead_mask')(dec_inputs)\n",
        "\n",
        "  # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
        "  # 디코더에서 패딩을 위한 마스크\n",
        "  dec_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='dec_padding_mask')(inputs)\n",
        "\n",
        "  # 인코더\n",
        "  enc_outputs = encoder(\n",
        "      vocab_size=vocab_size,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "  )(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "  # 디코더\n",
        "  dec_outputs = decoder(\n",
        "      vocab_size=vocab_size,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
        "\n",
        "  # 완전연결층\n",
        "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1li554xsCyGm"
      },
      "source": [
        "- 모델 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vdNZDNuCyGm",
        "outputId": "8e6814f8-88fc-4a17-8b50-b13ab36cc85e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " inputs (InputLayer)            [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " dec_inputs (InputLayer)        [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " enc_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n",
            "                                                                                                  \n",
            " encoder (Functional)           (None, None, 256)    3135232     ['inputs[0][0]',                 \n",
            "                                                                  'enc_padding_mask[0][0]']       \n",
            "                                                                                                  \n",
            " look_ahead_mask (Lambda)       (None, 1, None, Non  0           ['dec_inputs[0][0]']             \n",
            "                                e)                                                                \n",
            "                                                                                                  \n",
            " dec_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n",
            "                                                                                                  \n",
            " decoder (Functional)           (None, None, 256)    3662592     ['dec_inputs[0][0]',             \n",
            "                                                                  'encoder[0][0]',                \n",
            "                                                                  'look_ahead_mask[0][0]',        \n",
            "                                                                  'dec_padding_mask[0][0]']       \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, None, 8129)   2089153     ['decoder[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,886,977\n",
            "Trainable params: 8,886,977\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# 하이퍼파라미터\n",
        "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
        "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
        "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
        "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
        "DROPOUT = 0.1 # 드롭아웃의 비율\n",
        "\n",
        "model = transformer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    units=UNITS,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    dropout=DROPOUT)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsXFTvWwCyGm"
      },
      "source": [
        "- 손실 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2V3MUeBhCyGm",
        "outputId": "3eb0c993-9c45-4f08-f022-41398b12e34e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "def loss_function(y_true, y_pred):\n",
        "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "  \n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction='none')(y_true, y_pred)\n",
        "\n",
        "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "  loss = tf.multiply(loss, mask)\n",
        "\n",
        "  return tf.reduce_mean(loss)\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9az_8yBXCyGm"
      },
      "source": [
        "- 커스텀 학습률"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPGa7JTICyGm",
        "outputId": "cde0819b-667c-4402-c556-6b9fa0e5ccf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "ME2szjrtCyGn",
        "outputId": "339533f8-7569-47a6-8274-a42854a15019"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "sample_learning_rate = CustomSchedule(d_model=128)\n",
        "\n",
        "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpm31mj0CyGn"
      },
      "source": [
        "- 모델 컴파일"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xLHSFklCyGn",
        "outputId": "7f21ff2d-995b-4859-be4e-4ad2e48e65cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3shzOTyCyGn"
      },
      "source": [
        "- 모델 훈련"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgAY25quCyGn",
        "outputId": "c11dccb7-ae51-409e-f1c1-ee3c72ca0fb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "185/185 [==============================] - 17s 58ms/step - loss: 1.4580 - accuracy: 0.0197\n",
            "Epoch 2/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 1.1818 - accuracy: 0.0492\n",
            "Epoch 3/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 1.0052 - accuracy: 0.0506\n",
            "Epoch 4/20\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.9278 - accuracy: 0.0543\n",
            "Epoch 5/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.8697 - accuracy: 0.0575\n",
            "Epoch 6/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.8094 - accuracy: 0.0617\n",
            "Epoch 7/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.7428 - accuracy: 0.0678\n",
            "Epoch 8/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.6689 - accuracy: 0.0757\n",
            "Epoch 9/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.5903 - accuracy: 0.0844\n",
            "Epoch 10/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.5084 - accuracy: 0.0936\n",
            "Epoch 11/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.4256 - accuracy: 0.1040\n",
            "Epoch 12/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.3466 - accuracy: 0.1147\n",
            "Epoch 13/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.2720 - accuracy: 0.1259\n",
            "Epoch 14/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.2072 - accuracy: 0.1358\n",
            "Epoch 15/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.1531 - accuracy: 0.1450\n",
            "Epoch 16/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.1104 - accuracy: 0.1527\n",
            "Epoch 17/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0809 - accuracy: 0.1583\n",
            "Epoch 18/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0617 - accuracy: 0.1616\n",
            "Epoch 19/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0525 - accuracy: 0.1631\n",
            "Epoch 20/20\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0458 - accuracy: 0.1643\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0220171690>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "EPOCHS = 20\n",
        "model.fit(dataset, epochs=EPOCHS, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pvjf2lkCyGn"
      },
      "source": [
        "### Step 5. 모델 평가하기\n",
        "Step 1에서 선택한 전처리 방법을 고려하여 입력된 문장에 대해서 대답을 얻는 예측 함수를 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF1X-bptCyGn",
        "outputId": "6b8a0b06-885f-4bca-8659-1ecac8e71364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "def decoder_inference(sentence):\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
        "  # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
        "  sentence = tf.expand_dims(\n",
        "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
        "\n",
        "  # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
        "  # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
        "  output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
        "\n",
        "  # 디코더의 인퍼런스 단계\n",
        "  for i in range(MAX_LENGTH):\n",
        "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
        "    predictions = model(inputs=[sentence, output_sequence], training=False)\n",
        "    predictions = predictions[:, -1:, :]\n",
        "\n",
        "    # 현재 예측한 단어의 정수\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
        "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
        "      break\n",
        "\n",
        "    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
        "    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
        "    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output_sequence, axis=0)\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7otf9kNCyGo",
        "outputId": "7b998e16-011d-4791-906c-875255cb67c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ],
      "source": [
        "def sentence_generation(sentence):\n",
        "  # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
        "  prediction = decoder_inference(sentence)\n",
        "\n",
        "  # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
        "  predicted_sentence = tokenizer.decode(\n",
        "      [i for i in prediction if i < tokenizer.vocab_size])\n",
        "\n",
        "  print('입력 : {}'.format(sentence))\n",
        "  print('출력 : {}'.format(predicted_sentence))\n",
        "\n",
        "  return predicted_sentence\n",
        "print(\"슝=3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyIyuFnFCyGo"
      },
      "source": [
        "- 모델 평가용 입력 문장 리스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "hUoR7UzZCyGo"
      },
      "outputs": [],
      "source": [
        "input_question_list = [\n",
        "    \"안녕 오랜만이야\",\n",
        "    \"잘 지냈어?\",\n",
        "    \"뭐 먹을까\",\n",
        "    \"메뉴 추천해줘\",\n",
        "    \"어제 뭐 먹었어?\",\n",
        "    \"나랑 놀자\",\n",
        "    \"어디 가고 싶어?\",\n",
        "    \"널 만나서 기뻐\",\n",
        "    \"웃어줄래\",\n",
        "    \"삶은 뭘까?\",\n",
        "    \"인생살이 왜이리 힘드냐\",\n",
        "    \"이제 그만 쉬고 싶어\",\n",
        "    \"너무 고독하다\",\n",
        "    \"죽으면 어떻게 될까\",\n",
        "    \"내가 죽으면 슬퍼해 줄거야?\",\n",
        "    \"도망가고 싶다\",\n",
        "    \"우리 마지막이야\",\n",
        "    \"나 간다 잘 지내 안녕\",\n",
        "    \"넌 최고였어\",\n",
        "    \"나 잊지마\",\n",
        "    \"나는 심장이 없어\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "-5y683HZCyGo"
      },
      "outputs": [],
      "source": [
        "def print_answers(input_question_list):\n",
        "    for input_qusetion in input_question_list:\n",
        "        sentence_generation(input_qusetion)\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jur_wCsCyGo"
      },
      "source": [
        "#### 1. Epochs = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9op7goHxCyGo"
      },
      "source": [
        "- 모델 평가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0aVCcU_CyGo",
        "outputId": "fc1d9d71-1b6c-41a3-8136-d4686073a533"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 안녕 오랜만이야\n",
            "출력 : 맘고생 많았어요 .\n",
            "\n",
            "입력 : 잘 지냈어?\n",
            "출력 : 따뜻한 마음이네요 .\n",
            "\n",
            "입력 : 뭐 먹을까\n",
            "출력 : 냉장고 파먹기 해보세요 .\n",
            "\n",
            "입력 : 메뉴 추천해줘\n",
            "출력 : 누구랑 먹는 냐에 따라 다르겠죠 .\n",
            "\n",
            "입력 : 어제 뭐 먹었어?\n",
            "출력 : 독서와 음악감상이라고 하고 싶지만 아무 것도 안 했어요 .\n",
            "\n",
            "입력 : 나랑 놀자\n",
            "출력 : 지금 그러고 있어요 .\n",
            "\n",
            "입력 : 어디 가고 싶어?\n",
            "출력 : 온 가족이 모두 마음에 드는 곳으로 가보세요 .\n",
            "\n",
            "입력 : 널 만나서 기뻐\n",
            "출력 : 자신감을 가지세요 .\n",
            "\n",
            "입력 : 웃어줄래\n",
            "출력 : 저처럼 마음이 허마음 가는 거예요 .\n",
            "\n",
            "입력 : 삶은 뭘까?\n",
            "출력 : 지칠 때는 쉬어도 돼요 .\n",
            "\n",
            "입력 : 인생살이 왜이리 힘드냐\n",
            "출력 : 조언해주세요 .\n",
            "\n",
            "입력 : 이제 그만 쉬고 싶어\n",
            "출력 : 잠깐 바람 쐬고 오세요 .\n",
            "\n",
            "입력 : 너무 고독하다\n",
            "출력 : 혼자가 아니에요 .\n",
            "\n",
            "입력 : 죽으면 어떻게 될까\n",
            "출력 : 안 사면 요긴하긴 할 거 같아요 .\n",
            "\n",
            "입력 : 내가 죽으면 슬퍼해 줄거야?\n",
            "출력 : 그 사람이 좋아하는 것들을 알아보세요 .\n",
            "\n",
            "입력 : 도망가고 싶다\n",
            "출력 : 자책하지 마세요 .\n",
            "\n",
            "입력 : 우리 마지막이야\n",
            "출력 : 마음이 아프네요 .\n",
            "\n",
            "입력 : 나 간다 잘 지내 안녕\n",
            "출력 : 잘 지낼거라 믿어요 .\n",
            "\n",
            "입력 : 넌 최고였어\n",
            "출력 : 당신도 충분히 아름다워요 .\n",
            "\n",
            "입력 : 나 잊지마\n",
            "출력 : 때론 잊어버리는 것이 좋을 때도 있어요 .\n",
            "\n",
            "입력 : 나는 심장이 없어\n",
            "출력 : 심호흡 해보세요 .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_answers(input_question_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhVsHoL8CyGo"
      },
      "source": [
        "#### 2. Epochs = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOXdY7hWCyGp"
      },
      "source": [
        "- 모델 훈련"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5fAGV8aCyGp",
        "outputId": "960e9c7e-ab16-46a3-cde8-c16cab59d3fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0425 - accuracy: 0.1648\n",
            "Epoch 2/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0412 - accuracy: 0.1648\n",
            "Epoch 3/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0371 - accuracy: 0.1657\n",
            "Epoch 4/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0326 - accuracy: 0.1668\n",
            "Epoch 5/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0276 - accuracy: 0.1682\n",
            "Epoch 6/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0254 - accuracy: 0.1686\n",
            "Epoch 7/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0228 - accuracy: 0.1692\n",
            "Epoch 8/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0208 - accuracy: 0.1697\n",
            "Epoch 9/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0184 - accuracy: 0.1704\n",
            "Epoch 10/200\n",
            "185/185 [==============================] - 12s 63ms/step - loss: 0.0174 - accuracy: 0.1707\n",
            "Epoch 11/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0160 - accuracy: 0.1711\n",
            "Epoch 12/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0140 - accuracy: 0.1716\n",
            "Epoch 13/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0142 - accuracy: 0.1715\n",
            "Epoch 14/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0127 - accuracy: 0.1719\n",
            "Epoch 15/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0123 - accuracy: 0.1720\n",
            "Epoch 16/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0114 - accuracy: 0.1721\n",
            "Epoch 17/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0108 - accuracy: 0.1723\n",
            "Epoch 18/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0102 - accuracy: 0.1725\n",
            "Epoch 19/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0092 - accuracy: 0.1727\n",
            "Epoch 20/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0094 - accuracy: 0.1727\n",
            "Epoch 21/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0086 - accuracy: 0.1729\n",
            "Epoch 22/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0086 - accuracy: 0.1728\n",
            "Epoch 23/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0080 - accuracy: 0.1731\n",
            "Epoch 24/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0075 - accuracy: 0.1732\n",
            "Epoch 25/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0071 - accuracy: 0.1733\n",
            "Epoch 26/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0070 - accuracy: 0.1733\n",
            "Epoch 27/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0069 - accuracy: 0.1733\n",
            "Epoch 28/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0065 - accuracy: 0.1735\n",
            "Epoch 29/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0066 - accuracy: 0.1734\n",
            "Epoch 30/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0059 - accuracy: 0.1735\n",
            "Epoch 31/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0059 - accuracy: 0.1735\n",
            "Epoch 32/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0058 - accuracy: 0.1736\n",
            "Epoch 33/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0054 - accuracy: 0.1736\n",
            "Epoch 34/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0052 - accuracy: 0.1737\n",
            "Epoch 35/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0051 - accuracy: 0.1738\n",
            "Epoch 36/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0047 - accuracy: 0.1738\n",
            "Epoch 37/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0051 - accuracy: 0.1737\n",
            "Epoch 38/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0047 - accuracy: 0.1738\n",
            "Epoch 39/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0044 - accuracy: 0.1738\n",
            "Epoch 40/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0046 - accuracy: 0.1738\n",
            "Epoch 41/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0044 - accuracy: 0.1738\n",
            "Epoch 42/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0044 - accuracy: 0.1739\n",
            "Epoch 43/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0040 - accuracy: 0.1740\n",
            "Epoch 44/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0040 - accuracy: 0.1739\n",
            "Epoch 45/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0042 - accuracy: 0.1739\n",
            "Epoch 46/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0041 - accuracy: 0.1739\n",
            "Epoch 47/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0038 - accuracy: 0.1740\n",
            "Epoch 48/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0035 - accuracy: 0.1740\n",
            "Epoch 49/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0036 - accuracy: 0.1740\n",
            "Epoch 50/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0038 - accuracy: 0.1740\n",
            "Epoch 51/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0035 - accuracy: 0.1740\n",
            "Epoch 52/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0034 - accuracy: 0.1740\n",
            "Epoch 53/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0034 - accuracy: 0.1740\n",
            "Epoch 54/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0034 - accuracy: 0.1740\n",
            "Epoch 55/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0030 - accuracy: 0.1741\n",
            "Epoch 56/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0031 - accuracy: 0.1741\n",
            "Epoch 57/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0032 - accuracy: 0.1740\n",
            "Epoch 58/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0031 - accuracy: 0.1740\n",
            "Epoch 59/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0030 - accuracy: 0.1741\n",
            "Epoch 60/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0030 - accuracy: 0.1741\n",
            "Epoch 61/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0030 - accuracy: 0.1741\n",
            "Epoch 62/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0031 - accuracy: 0.1741\n",
            "Epoch 63/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0027 - accuracy: 0.1741\n",
            "Epoch 64/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0028 - accuracy: 0.1742\n",
            "Epoch 65/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0027 - accuracy: 0.1742\n",
            "Epoch 66/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0027 - accuracy: 0.1741\n",
            "Epoch 67/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0026 - accuracy: 0.1741\n",
            "Epoch 68/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0024 - accuracy: 0.1742\n",
            "Epoch 69/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0026 - accuracy: 0.1742\n",
            "Epoch 70/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0024 - accuracy: 0.1742\n",
            "Epoch 71/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0024 - accuracy: 0.1742\n",
            "Epoch 72/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0024 - accuracy: 0.1742\n",
            "Epoch 73/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0024 - accuracy: 0.1742\n",
            "Epoch 74/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0024 - accuracy: 0.1742\n",
            "Epoch 75/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0023 - accuracy: 0.1742\n",
            "Epoch 76/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0025 - accuracy: 0.1742\n",
            "Epoch 77/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0022 - accuracy: 0.1742\n",
            "Epoch 78/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0021 - accuracy: 0.1742\n",
            "Epoch 79/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0021 - accuracy: 0.1742\n",
            "Epoch 80/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0022 - accuracy: 0.1742\n",
            "Epoch 81/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0021 - accuracy: 0.1742\n",
            "Epoch 82/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0022 - accuracy: 0.1742\n",
            "Epoch 83/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0020 - accuracy: 0.1742\n",
            "Epoch 84/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0020 - accuracy: 0.1742\n",
            "Epoch 85/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0020 - accuracy: 0.1742\n",
            "Epoch 86/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0020 - accuracy: 0.1742\n",
            "Epoch 87/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0020 - accuracy: 0.1742\n",
            "Epoch 88/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0018 - accuracy: 0.1743\n",
            "Epoch 89/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0018 - accuracy: 0.1742\n",
            "Epoch 90/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0019 - accuracy: 0.1743\n",
            "Epoch 91/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0020 - accuracy: 0.1743\n",
            "Epoch 92/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0018 - accuracy: 0.1743\n",
            "Epoch 93/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0019 - accuracy: 0.1742\n",
            "Epoch 94/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0018 - accuracy: 0.1743\n",
            "Epoch 95/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0017 - accuracy: 0.1743\n",
            "Epoch 96/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0017 - accuracy: 0.1743\n",
            "Epoch 97/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0018 - accuracy: 0.1742\n",
            "Epoch 98/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0017 - accuracy: 0.1742\n",
            "Epoch 99/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0017 - accuracy: 0.1743\n",
            "Epoch 100/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0018 - accuracy: 0.1742\n",
            "Epoch 101/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0019 - accuracy: 0.1742\n",
            "Epoch 102/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0017 - accuracy: 0.1743\n",
            "Epoch 103/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0018 - accuracy: 0.1742\n",
            "Epoch 104/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0016 - accuracy: 0.1743\n",
            "Epoch 105/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0015 - accuracy: 0.1743\n",
            "Epoch 106/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0018 - accuracy: 0.1742\n",
            "Epoch 107/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0015 - accuracy: 0.1743\n",
            "Epoch 108/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0016 - accuracy: 0.1743\n",
            "Epoch 109/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0016 - accuracy: 0.1743\n",
            "Epoch 110/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0016 - accuracy: 0.1743\n",
            "Epoch 111/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0015 - accuracy: 0.1743\n",
            "Epoch 112/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0014 - accuracy: 0.1743\n",
            "Epoch 113/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0017 - accuracy: 0.1743\n",
            "Epoch 114/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0015 - accuracy: 0.1743\n",
            "Epoch 115/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0015 - accuracy: 0.1743\n",
            "Epoch 116/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0016 - accuracy: 0.1743\n",
            "Epoch 117/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0014 - accuracy: 0.1743\n",
            "Epoch 118/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0014 - accuracy: 0.1743\n",
            "Epoch 119/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 120/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0015 - accuracy: 0.1743\n",
            "Epoch 121/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0014 - accuracy: 0.1743\n",
            "Epoch 122/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0014 - accuracy: 0.1743\n",
            "Epoch 123/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0015 - accuracy: 0.1743\n",
            "Epoch 124/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 125/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0014 - accuracy: 0.1743\n",
            "Epoch 126/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 127/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 128/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 129/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 130/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 131/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0014 - accuracy: 0.1743\n",
            "Epoch 132/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 133/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 134/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 135/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0014 - accuracy: 0.1743\n",
            "Epoch 136/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 137/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 138/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 139/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 140/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 141/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0014 - accuracy: 0.1743\n",
            "Epoch 142/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 143/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 144/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0015 - accuracy: 0.1743\n",
            "Epoch 145/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 146/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 147/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 148/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 149/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 150/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1744\n",
            "Epoch 151/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 152/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 153/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 154/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 155/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 156/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 157/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 158/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 159/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 160/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 161/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 162/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 163/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 164/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1744\n",
            "Epoch 165/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 166/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 167/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0013 - accuracy: 0.1743\n",
            "Epoch 168/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 169/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.7914e-04 - accuracy: 0.1744\n",
            "Epoch 170/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 171/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 172/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 173/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 174/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1744\n",
            "Epoch 175/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.8949e-04 - accuracy: 0.1744\n",
            "Epoch 176/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 177/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 178/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0012 - accuracy: 0.1743\n",
            "Epoch 179/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1744\n",
            "Epoch 180/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.6496e-04 - accuracy: 0.1743\n",
            "Epoch 181/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.6941e-04 - accuracy: 0.1743\n",
            "Epoch 182/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.9600e-04 - accuracy: 0.1744\n",
            "Epoch 183/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 184/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 185/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 186/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.9572e-04 - accuracy: 0.1744\n",
            "Epoch 187/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1744\n",
            "Epoch 188/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 189/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.8799e-04 - accuracy: 0.1744\n",
            "Epoch 190/200\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 8.5378e-04 - accuracy: 0.1744\n",
            "Epoch 191/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 192/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 193/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 194/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.7991e-04 - accuracy: 0.1744\n",
            "Epoch 195/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 196/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1744\n",
            "Epoch 197/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.5675e-04 - accuracy: 0.1743\n",
            "Epoch 198/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 199/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 200/200\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1744\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f018f535b50>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "EPOCHS = 200\n",
        "model.fit(dataset, epochs=EPOCHS, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q9cs6ayCyGp"
      },
      "source": [
        "- 모델 평가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8v0gm8rCyGp",
        "outputId": "57b3bfab-3e95-4295-8479-c9bbfcfe4f36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 안녕 오랜만이야\n",
            "출력 : 오랜만이에요 .\n",
            "\n",
            "입력 : 잘 지냈어?\n",
            "출력 : 안부를 물어주시다니 감사합니다 .\n",
            "\n",
            "입력 : 뭐 먹을까\n",
            "출력 : 좀 먹어도 괜찮아요 .\n",
            "\n",
            "입력 : 메뉴 추천해줘\n",
            "출력 : 언젠가 그런 사람이 당신 옆에 있을거예요 .\n",
            "\n",
            "입력 : 어제 뭐 먹었어?\n",
            "출력 : 저는 배터리가 밥이예요 .\n",
            "\n",
            "입력 : 나랑 놀자\n",
            "출력 : 지금 그러고 있어요 .\n",
            "\n",
            "입력 : 어디 가고 싶어?\n",
            "출력 : 가게부를 써보세요 .\n",
            "\n",
            "입력 : 널 만나서 기뻐\n",
            "출력 : 가끔씩 스트레칭을 해주세요 .\n",
            "\n",
            "입력 : 웃어줄래\n",
            "출력 : 기대를 많이 하는 건 좋지 않아요 .\n",
            "\n",
            "입력 : 삶은 뭘까?\n",
            "출력 : 지칠 때는 쉬어도 돼요 .\n",
            "\n",
            "입력 : 인생살이 왜이리 힘드냐\n",
            "출력 : 그러게요 . 그만큼 사랑했다는거겠죠 .\n",
            "\n",
            "입력 : 이제 그만 쉬고 싶어\n",
            "출력 : 잠깐 바람 쐬고 오세요 .\n",
            "\n",
            "입력 : 너무 고독하다\n",
            "출력 : 혼자가 아니에요 .\n",
            "\n",
            "입력 : 죽으면 어떻게 될까\n",
            "출력 : 당신이 축의금을 건냈던 모든 사람들에게 주세요 .\n",
            "\n",
            "입력 : 내가 죽으면 슬퍼해 줄거야?\n",
            "출력 : 저도 데려가세요 .\n",
            "\n",
            "입력 : 도망가고 싶다\n",
            "출력 : 저도 궁금하네요 .\n",
            "\n",
            "입력 : 우리 마지막이야\n",
            "출력 : 마지막이 아닐 지도 몰라요 .\n",
            "\n",
            "입력 : 나 간다 잘 지내 안녕\n",
            "출력 : 당신은 정말 착한 사람이군요 .\n",
            "\n",
            "입력 : 넌 최고였어\n",
            "출력 : 내일도 좋은 하루 보내세요 .\n",
            "\n",
            "입력 : 나 잊지마\n",
            "출력 : 때론 잊어버리는 것이 좋을 때도 있어요 .\n",
            "\n",
            "입력 : 나는 심장이 없어\n",
            "출력 : 심플하게 꾸며보세요 .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_answers(input_question_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Epochs = 500"
      ],
      "metadata": {
        "id": "LeT3KsTAMLSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델 훈련"
      ],
      "metadata": {
        "id": "UUhbq9WoMvhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 500\n",
        "model.fit(dataset, epochs=EPOCHS, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLiyVJx4M0al",
        "outputId": "c2168031-55ab-4018-bf39-ea24b67ab672"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 2/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 3/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.6726e-04 - accuracy: 0.1743\n",
            "Epoch 4/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.8247e-04 - accuracy: 0.1743\n",
            "Epoch 5/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.7236e-04 - accuracy: 0.1744\n",
            "Epoch 6/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1744\n",
            "Epoch 7/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 8/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 9/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.3788e-04 - accuracy: 0.1744\n",
            "Epoch 10/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.1851e-04 - accuracy: 0.1743\n",
            "Epoch 11/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.1793e-04 - accuracy: 0.1743\n",
            "Epoch 12/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.3646e-04 - accuracy: 0.1743\n",
            "Epoch 13/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.6441e-04 - accuracy: 0.1743\n",
            "Epoch 14/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.8320e-04 - accuracy: 0.1744\n",
            "Epoch 15/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.5841e-04 - accuracy: 0.1744\n",
            "Epoch 16/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 17/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.0860e-04 - accuracy: 0.1743\n",
            "Epoch 18/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.3252e-04 - accuracy: 0.1744\n",
            "Epoch 19/500\n",
            "185/185 [==============================] - 11s 57ms/step - loss: 8.5143e-04 - accuracy: 0.1744\n",
            "Epoch 20/500\n",
            "185/185 [==============================] - 11s 57ms/step - loss: 9.5707e-04 - accuracy: 0.1743\n",
            "Epoch 21/500\n",
            "185/185 [==============================] - 11s 57ms/step - loss: 9.2013e-04 - accuracy: 0.1743\n",
            "Epoch 22/500\n",
            "185/185 [==============================] - 11s 57ms/step - loss: 9.0229e-04 - accuracy: 0.1743\n",
            "Epoch 23/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 24/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.3073e-04 - accuracy: 0.1744\n",
            "Epoch 25/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.0084e-04 - accuracy: 0.1744\n",
            "Epoch 26/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.3588e-04 - accuracy: 0.1743\n",
            "Epoch 27/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.6153e-04 - accuracy: 0.1743\n",
            "Epoch 28/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.3098e-04 - accuracy: 0.1744\n",
            "Epoch 29/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1744\n",
            "Epoch 30/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.5547e-04 - accuracy: 0.1744\n",
            "Epoch 31/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.0118e-04 - accuracy: 0.1744\n",
            "Epoch 32/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.1452e-04 - accuracy: 0.1744\n",
            "Epoch 33/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.6465e-04 - accuracy: 0.1744\n",
            "Epoch 34/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.3791e-04 - accuracy: 0.1743\n",
            "Epoch 35/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 36/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.6060e-04 - accuracy: 0.1744\n",
            "Epoch 37/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.7659e-04 - accuracy: 0.1743\n",
            "Epoch 38/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.9638e-04 - accuracy: 0.1744\n",
            "Epoch 39/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.6384e-04 - accuracy: 0.1744\n",
            "Epoch 40/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.1399e-04 - accuracy: 0.1743\n",
            "Epoch 41/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.4718e-04 - accuracy: 0.1744\n",
            "Epoch 42/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.3343e-04 - accuracy: 0.1744\n",
            "Epoch 43/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.4253e-04 - accuracy: 0.1743\n",
            "Epoch 44/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.6345e-04 - accuracy: 0.1744\n",
            "Epoch 45/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.4746e-04 - accuracy: 0.1744\n",
            "Epoch 46/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.6397e-04 - accuracy: 0.1744\n",
            "Epoch 47/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0011 - accuracy: 0.1743\n",
            "Epoch 48/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.5516e-04 - accuracy: 0.1743\n",
            "Epoch 49/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.3724e-04 - accuracy: 0.1744\n",
            "Epoch 50/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.2719e-04 - accuracy: 0.1744\n",
            "Epoch 51/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 52/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.4391e-04 - accuracy: 0.1744\n",
            "Epoch 53/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.1849e-04 - accuracy: 0.1744\n",
            "Epoch 54/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0608e-04 - accuracy: 0.1744\n",
            "Epoch 55/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.4009e-04 - accuracy: 0.1743\n",
            "Epoch 56/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.5837e-04 - accuracy: 0.1744\n",
            "Epoch 57/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0537e-04 - accuracy: 0.1744\n",
            "Epoch 58/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.3270e-04 - accuracy: 0.1744\n",
            "Epoch 59/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.8631e-04 - accuracy: 0.1743\n",
            "Epoch 60/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.5387e-04 - accuracy: 0.1743\n",
            "Epoch 61/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.8191e-04 - accuracy: 0.1744\n",
            "Epoch 62/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0911e-04 - accuracy: 0.1744\n",
            "Epoch 63/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.2250e-04 - accuracy: 0.1743\n",
            "Epoch 64/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.2673e-04 - accuracy: 0.1743\n",
            "Epoch 65/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.9784e-04 - accuracy: 0.1743\n",
            "Epoch 66/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.3051e-04 - accuracy: 0.1743\n",
            "Epoch 67/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.2862e-04 - accuracy: 0.1744\n",
            "Epoch 68/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.6188e-04 - accuracy: 0.1744\n",
            "Epoch 69/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 0.0010 - accuracy: 0.1743\n",
            "Epoch 70/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.5107e-04 - accuracy: 0.1744\n",
            "Epoch 71/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.7933e-04 - accuracy: 0.1743\n",
            "Epoch 72/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.8100e-04 - accuracy: 0.1744\n",
            "Epoch 73/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.6722e-04 - accuracy: 0.1743\n",
            "Epoch 74/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.1434e-04 - accuracy: 0.1743\n",
            "Epoch 75/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.2484e-04 - accuracy: 0.1743\n",
            "Epoch 76/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.6189e-04 - accuracy: 0.1744\n",
            "Epoch 77/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0620e-04 - accuracy: 0.1744\n",
            "Epoch 78/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.8705e-04 - accuracy: 0.1744\n",
            "Epoch 79/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.1374e-04 - accuracy: 0.1744\n",
            "Epoch 80/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0563e-04 - accuracy: 0.1744\n",
            "Epoch 81/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.6473e-04 - accuracy: 0.1743\n",
            "Epoch 82/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0004e-04 - accuracy: 0.1744\n",
            "Epoch 83/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.8075e-04 - accuracy: 0.1744\n",
            "Epoch 84/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.4030e-04 - accuracy: 0.1744\n",
            "Epoch 85/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.6999e-04 - accuracy: 0.1743\n",
            "Epoch 86/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.5188e-04 - accuracy: 0.1743\n",
            "Epoch 87/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.5275e-04 - accuracy: 0.1744\n",
            "Epoch 88/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.7147e-04 - accuracy: 0.1743\n",
            "Epoch 89/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.7925e-04 - accuracy: 0.1744\n",
            "Epoch 90/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.6139e-04 - accuracy: 0.1744\n",
            "Epoch 91/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.8609e-04 - accuracy: 0.1744\n",
            "Epoch 92/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.9514e-04 - accuracy: 0.1743\n",
            "Epoch 93/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.5847e-04 - accuracy: 0.1744\n",
            "Epoch 94/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.2635e-04 - accuracy: 0.1743\n",
            "Epoch 95/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.9705e-04 - accuracy: 0.1744\n",
            "Epoch 96/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.5680e-04 - accuracy: 0.1744\n",
            "Epoch 97/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.7972e-04 - accuracy: 0.1744\n",
            "Epoch 98/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2554e-04 - accuracy: 0.1744\n",
            "Epoch 99/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0301e-04 - accuracy: 0.1744\n",
            "Epoch 100/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.3487e-04 - accuracy: 0.1744\n",
            "Epoch 101/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0982e-04 - accuracy: 0.1744\n",
            "Epoch 102/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.2898e-04 - accuracy: 0.1744\n",
            "Epoch 103/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.9778e-04 - accuracy: 0.1743\n",
            "Epoch 104/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.7716e-04 - accuracy: 0.1744\n",
            "Epoch 105/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.3277e-04 - accuracy: 0.1743\n",
            "Epoch 106/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.8497e-04 - accuracy: 0.1744\n",
            "Epoch 107/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2576e-04 - accuracy: 0.1744\n",
            "Epoch 108/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.1043e-04 - accuracy: 0.1744\n",
            "Epoch 109/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3195e-04 - accuracy: 0.1744\n",
            "Epoch 110/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.9955e-04 - accuracy: 0.1743\n",
            "Epoch 111/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.4634e-04 - accuracy: 0.1743\n",
            "Epoch 112/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0895e-04 - accuracy: 0.1744\n",
            "Epoch 113/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.2823e-04 - accuracy: 0.1743\n",
            "Epoch 114/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.7844e-04 - accuracy: 0.1744\n",
            "Epoch 115/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.6847e-04 - accuracy: 0.1744\n",
            "Epoch 116/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.7639e-04 - accuracy: 0.1744\n",
            "Epoch 117/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.4744e-04 - accuracy: 0.1743\n",
            "Epoch 118/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.5675e-04 - accuracy: 0.1743\n",
            "Epoch 119/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3287e-04 - accuracy: 0.1744\n",
            "Epoch 120/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.1584e-04 - accuracy: 0.1744\n",
            "Epoch 121/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.7598e-04 - accuracy: 0.1744\n",
            "Epoch 122/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.6511e-04 - accuracy: 0.1744\n",
            "Epoch 123/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2077e-04 - accuracy: 0.1744\n",
            "Epoch 124/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0296e-04 - accuracy: 0.1744\n",
            "Epoch 125/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.9130e-04 - accuracy: 0.1743\n",
            "Epoch 126/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2796e-04 - accuracy: 0.1744\n",
            "Epoch 127/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0904e-04 - accuracy: 0.1744\n",
            "Epoch 128/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.4443e-04 - accuracy: 0.1744\n",
            "Epoch 129/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 9.2429e-04 - accuracy: 0.1743\n",
            "Epoch 130/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3500e-04 - accuracy: 0.1744\n",
            "Epoch 131/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0506e-04 - accuracy: 0.1744\n",
            "Epoch 132/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.1527e-04 - accuracy: 0.1744\n",
            "Epoch 133/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.8240e-04 - accuracy: 0.1744\n",
            "Epoch 134/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0516e-04 - accuracy: 0.1744\n",
            "Epoch 135/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.5862e-04 - accuracy: 0.1744\n",
            "Epoch 136/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.9788e-04 - accuracy: 0.1744\n",
            "Epoch 137/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0710e-04 - accuracy: 0.1744\n",
            "Epoch 138/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0377e-04 - accuracy: 0.1744\n",
            "Epoch 139/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2089e-04 - accuracy: 0.1744\n",
            "Epoch 140/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.2541e-04 - accuracy: 0.1744\n",
            "Epoch 141/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.4228e-04 - accuracy: 0.1744\n",
            "Epoch 142/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.7032e-04 - accuracy: 0.1744\n",
            "Epoch 143/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8826e-04 - accuracy: 0.1743\n",
            "Epoch 144/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.5661e-04 - accuracy: 0.1744\n",
            "Epoch 145/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3056e-04 - accuracy: 0.1744\n",
            "Epoch 146/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0503e-04 - accuracy: 0.1744\n",
            "Epoch 147/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.1535e-04 - accuracy: 0.1744\n",
            "Epoch 148/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.7446e-04 - accuracy: 0.1744\n",
            "Epoch 149/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.6955e-04 - accuracy: 0.1744\n",
            "Epoch 150/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3769e-04 - accuracy: 0.1744\n",
            "Epoch 151/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9944e-04 - accuracy: 0.1744\n",
            "Epoch 152/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.4568e-04 - accuracy: 0.1744\n",
            "Epoch 153/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0317e-04 - accuracy: 0.1744\n",
            "Epoch 154/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3520e-04 - accuracy: 0.1744\n",
            "Epoch 155/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8424e-04 - accuracy: 0.1744\n",
            "Epoch 156/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.8852e-04 - accuracy: 0.1744\n",
            "Epoch 157/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.8358e-04 - accuracy: 0.1743\n",
            "Epoch 158/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2689e-04 - accuracy: 0.1744\n",
            "Epoch 159/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8848e-04 - accuracy: 0.1744\n",
            "Epoch 160/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2320e-04 - accuracy: 0.1744\n",
            "Epoch 161/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8593e-04 - accuracy: 0.1744\n",
            "Epoch 162/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8836e-04 - accuracy: 0.1744\n",
            "Epoch 163/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.6842e-04 - accuracy: 0.1744\n",
            "Epoch 164/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0620e-04 - accuracy: 0.1744\n",
            "Epoch 165/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.3477e-04 - accuracy: 0.1744\n",
            "Epoch 166/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2551e-04 - accuracy: 0.1744\n",
            "Epoch 167/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.8347e-04 - accuracy: 0.1744\n",
            "Epoch 168/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.6901e-04 - accuracy: 0.1744\n",
            "Epoch 169/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.6113e-04 - accuracy: 0.1744\n",
            "Epoch 170/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9881e-04 - accuracy: 0.1744\n",
            "Epoch 171/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3964e-04 - accuracy: 0.1743\n",
            "Epoch 172/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.6446e-04 - accuracy: 0.1744\n",
            "Epoch 173/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0653e-04 - accuracy: 0.1744\n",
            "Epoch 174/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.5908e-04 - accuracy: 0.1744\n",
            "Epoch 175/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.4807e-04 - accuracy: 0.1743\n",
            "Epoch 176/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0380e-04 - accuracy: 0.1744\n",
            "Epoch 177/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3423e-04 - accuracy: 0.1744\n",
            "Epoch 178/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2667e-04 - accuracy: 0.1744\n",
            "Epoch 179/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9990e-04 - accuracy: 0.1744\n",
            "Epoch 180/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9331e-04 - accuracy: 0.1744\n",
            "Epoch 181/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0745e-04 - accuracy: 0.1743\n",
            "Epoch 182/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.4195e-04 - accuracy: 0.1743\n",
            "Epoch 183/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.6150e-04 - accuracy: 0.1743\n",
            "Epoch 184/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.3268e-04 - accuracy: 0.1743\n",
            "Epoch 185/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3169e-04 - accuracy: 0.1743\n",
            "Epoch 186/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9700e-04 - accuracy: 0.1744\n",
            "Epoch 187/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.5319e-04 - accuracy: 0.1744\n",
            "Epoch 188/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.1967e-04 - accuracy: 0.1744\n",
            "Epoch 189/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.2739e-04 - accuracy: 0.1744\n",
            "Epoch 190/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.7285e-04 - accuracy: 0.1744\n",
            "Epoch 191/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.1117e-04 - accuracy: 0.1744\n",
            "Epoch 192/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.4980e-04 - accuracy: 0.1743\n",
            "Epoch 193/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0450e-04 - accuracy: 0.1744\n",
            "Epoch 194/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.4710e-04 - accuracy: 0.1744\n",
            "Epoch 195/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3470e-04 - accuracy: 0.1744\n",
            "Epoch 196/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.5774e-04 - accuracy: 0.1744\n",
            "Epoch 197/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2806e-04 - accuracy: 0.1744\n",
            "Epoch 198/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.4101e-04 - accuracy: 0.1743\n",
            "Epoch 199/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8954e-04 - accuracy: 0.1744\n",
            "Epoch 200/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.5999e-04 - accuracy: 0.1744\n",
            "Epoch 201/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.6194e-04 - accuracy: 0.1744\n",
            "Epoch 202/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.6565e-04 - accuracy: 0.1744\n",
            "Epoch 203/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.0514e-04 - accuracy: 0.1743\n",
            "Epoch 204/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3262e-04 - accuracy: 0.1744\n",
            "Epoch 205/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2530e-04 - accuracy: 0.1744\n",
            "Epoch 206/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.1205e-04 - accuracy: 0.1744\n",
            "Epoch 207/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.3885e-04 - accuracy: 0.1744\n",
            "Epoch 208/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.7166e-04 - accuracy: 0.1744\n",
            "Epoch 209/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.6576e-04 - accuracy: 0.1744\n",
            "Epoch 210/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0030e-04 - accuracy: 0.1744\n",
            "Epoch 211/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8625e-04 - accuracy: 0.1744\n",
            "Epoch 212/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.9546e-04 - accuracy: 0.1743\n",
            "Epoch 213/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.4362e-04 - accuracy: 0.1744\n",
            "Epoch 214/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.7636e-04 - accuracy: 0.1744\n",
            "Epoch 215/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9361e-04 - accuracy: 0.1744\n",
            "Epoch 216/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3011e-04 - accuracy: 0.1744\n",
            "Epoch 217/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 8.2605e-04 - accuracy: 0.1744\n",
            "Epoch 218/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.7236e-04 - accuracy: 0.1744\n",
            "Epoch 219/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.1067e-04 - accuracy: 0.1744\n",
            "Epoch 220/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.6118e-04 - accuracy: 0.1743\n",
            "Epoch 221/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8990e-04 - accuracy: 0.1744\n",
            "Epoch 222/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.6654e-04 - accuracy: 0.1744\n",
            "Epoch 223/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8464e-04 - accuracy: 0.1744\n",
            "Epoch 224/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.4921e-04 - accuracy: 0.1744\n",
            "Epoch 225/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.6464e-04 - accuracy: 0.1744\n",
            "Epoch 226/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.4011e-04 - accuracy: 0.1744\n",
            "Epoch 227/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.7661e-04 - accuracy: 0.1743\n",
            "Epoch 228/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2609e-04 - accuracy: 0.1744\n",
            "Epoch 229/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.6336e-04 - accuracy: 0.1744\n",
            "Epoch 230/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.5831e-04 - accuracy: 0.1744\n",
            "Epoch 231/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.7891e-04 - accuracy: 0.1744\n",
            "Epoch 232/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.8780e-04 - accuracy: 0.1744\n",
            "Epoch 233/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.6745e-04 - accuracy: 0.1744\n",
            "Epoch 234/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8913e-04 - accuracy: 0.1744\n",
            "Epoch 235/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3538e-04 - accuracy: 0.1744\n",
            "Epoch 236/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0771e-04 - accuracy: 0.1744\n",
            "Epoch 237/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.5990e-04 - accuracy: 0.1744\n",
            "Epoch 238/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.6773e-04 - accuracy: 0.1744\n",
            "Epoch 239/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0801e-04 - accuracy: 0.1744\n",
            "Epoch 240/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.5988e-04 - accuracy: 0.1744\n",
            "Epoch 241/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.1355e-04 - accuracy: 0.1744\n",
            "Epoch 242/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9219e-04 - accuracy: 0.1744\n",
            "Epoch 243/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9943e-04 - accuracy: 0.1744\n",
            "Epoch 244/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.5311e-04 - accuracy: 0.1744\n",
            "Epoch 245/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3882e-04 - accuracy: 0.1744\n",
            "Epoch 246/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6997e-04 - accuracy: 0.1744\n",
            "Epoch 247/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8860e-04 - accuracy: 0.1744\n",
            "Epoch 248/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8219e-04 - accuracy: 0.1744\n",
            "Epoch 249/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 5.8811e-04 - accuracy: 0.1744\n",
            "Epoch 250/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.6249e-04 - accuracy: 0.1744\n",
            "Epoch 251/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.3569e-04 - accuracy: 0.1744\n",
            "Epoch 252/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2926e-04 - accuracy: 0.1744\n",
            "Epoch 253/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.2535e-04 - accuracy: 0.1744\n",
            "Epoch 254/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.6944e-04 - accuracy: 0.1744\n",
            "Epoch 255/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9868e-04 - accuracy: 0.1744\n",
            "Epoch 256/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.7231e-04 - accuracy: 0.1744\n",
            "Epoch 257/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.1127e-04 - accuracy: 0.1744\n",
            "Epoch 258/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.2981e-04 - accuracy: 0.1744\n",
            "Epoch 259/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8309e-04 - accuracy: 0.1744\n",
            "Epoch 260/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0539e-04 - accuracy: 0.1744\n",
            "Epoch 261/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.1314e-04 - accuracy: 0.1744\n",
            "Epoch 262/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.4001e-04 - accuracy: 0.1744\n",
            "Epoch 263/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2677e-04 - accuracy: 0.1744\n",
            "Epoch 264/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.7534e-04 - accuracy: 0.1744\n",
            "Epoch 265/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.1565e-04 - accuracy: 0.1744\n",
            "Epoch 266/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.1540e-04 - accuracy: 0.1744\n",
            "Epoch 267/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.7482e-04 - accuracy: 0.1744\n",
            "Epoch 268/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.4805e-04 - accuracy: 0.1744\n",
            "Epoch 269/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3884e-04 - accuracy: 0.1744\n",
            "Epoch 270/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.3193e-04 - accuracy: 0.1744\n",
            "Epoch 271/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9581e-04 - accuracy: 0.1744\n",
            "Epoch 272/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.6603e-04 - accuracy: 0.1744\n",
            "Epoch 273/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.7281e-04 - accuracy: 0.1744\n",
            "Epoch 274/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.9113e-04 - accuracy: 0.1743\n",
            "Epoch 275/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.7422e-04 - accuracy: 0.1744\n",
            "Epoch 276/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.5465e-04 - accuracy: 0.1744\n",
            "Epoch 277/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2760e-04 - accuracy: 0.1744\n",
            "Epoch 278/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.4313e-04 - accuracy: 0.1744\n",
            "Epoch 279/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.5318e-04 - accuracy: 0.1744\n",
            "Epoch 280/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8321e-04 - accuracy: 0.1744\n",
            "Epoch 281/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.4378e-04 - accuracy: 0.1744\n",
            "Epoch 282/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.7588e-04 - accuracy: 0.1744\n",
            "Epoch 283/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.5203e-04 - accuracy: 0.1744\n",
            "Epoch 284/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.4225e-04 - accuracy: 0.1744\n",
            "Epoch 285/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9814e-04 - accuracy: 0.1744\n",
            "Epoch 286/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0544e-04 - accuracy: 0.1744\n",
            "Epoch 287/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.1080e-04 - accuracy: 0.1744\n",
            "Epoch 288/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.7741e-04 - accuracy: 0.1744\n",
            "Epoch 289/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.4448e-04 - accuracy: 0.1744\n",
            "Epoch 290/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.5089e-04 - accuracy: 0.1744\n",
            "Epoch 291/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9804e-04 - accuracy: 0.1744\n",
            "Epoch 292/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.2704e-04 - accuracy: 0.1744\n",
            "Epoch 293/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.7172e-04 - accuracy: 0.1744\n",
            "Epoch 294/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9429e-04 - accuracy: 0.1744\n",
            "Epoch 295/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.1718e-04 - accuracy: 0.1744\n",
            "Epoch 296/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.2348e-04 - accuracy: 0.1744\n",
            "Epoch 297/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.7952e-04 - accuracy: 0.1744\n",
            "Epoch 298/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9717e-04 - accuracy: 0.1744\n",
            "Epoch 299/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.7288e-04 - accuracy: 0.1744\n",
            "Epoch 300/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.1592e-04 - accuracy: 0.1744\n",
            "Epoch 301/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.2903e-04 - accuracy: 0.1744\n",
            "Epoch 302/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9034e-04 - accuracy: 0.1744\n",
            "Epoch 303/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3776e-04 - accuracy: 0.1744\n",
            "Epoch 304/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.8084e-04 - accuracy: 0.1744\n",
            "Epoch 305/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3681e-04 - accuracy: 0.1744\n",
            "Epoch 306/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3364e-04 - accuracy: 0.1744\n",
            "Epoch 307/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8035e-04 - accuracy: 0.1744\n",
            "Epoch 308/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0550e-04 - accuracy: 0.1744\n",
            "Epoch 309/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.7213e-04 - accuracy: 0.1743\n",
            "Epoch 310/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8297e-04 - accuracy: 0.1744\n",
            "Epoch 311/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.7813e-04 - accuracy: 0.1744\n",
            "Epoch 312/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.3907e-04 - accuracy: 0.1744\n",
            "Epoch 313/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.7126e-04 - accuracy: 0.1744\n",
            "Epoch 314/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.3889e-04 - accuracy: 0.1744\n",
            "Epoch 315/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.2306e-04 - accuracy: 0.1744\n",
            "Epoch 316/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.9915e-04 - accuracy: 0.1744\n",
            "Epoch 317/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.3092e-04 - accuracy: 0.1744\n",
            "Epoch 318/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.5844e-04 - accuracy: 0.1744\n",
            "Epoch 319/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.3264e-04 - accuracy: 0.1744\n",
            "Epoch 320/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 5.7624e-04 - accuracy: 0.1744\n",
            "Epoch 321/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3890e-04 - accuracy: 0.1744\n",
            "Epoch 322/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.4158e-04 - accuracy: 0.1744\n",
            "Epoch 323/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3397e-04 - accuracy: 0.1744\n",
            "Epoch 324/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.5378e-04 - accuracy: 0.1744\n",
            "Epoch 325/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 8.5519e-04 - accuracy: 0.1743\n",
            "Epoch 326/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.9782e-04 - accuracy: 0.1744\n",
            "Epoch 327/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.5720e-04 - accuracy: 0.1744\n",
            "Epoch 328/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.7848e-04 - accuracy: 0.1744\n",
            "Epoch 329/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.0393e-04 - accuracy: 0.1744\n",
            "Epoch 330/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.2273e-04 - accuracy: 0.1744\n",
            "Epoch 331/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.0676e-04 - accuracy: 0.1744\n",
            "Epoch 332/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6472e-04 - accuracy: 0.1744\n",
            "Epoch 333/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.1798e-04 - accuracy: 0.1744\n",
            "Epoch 334/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.0271e-04 - accuracy: 0.1744\n",
            "Epoch 335/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.5393e-04 - accuracy: 0.1744\n",
            "Epoch 336/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.7394e-04 - accuracy: 0.1744\n",
            "Epoch 337/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9861e-04 - accuracy: 0.1744\n",
            "Epoch 338/500\n",
            "185/185 [==============================] - 11s 57ms/step - loss: 6.7632e-04 - accuracy: 0.1744\n",
            "Epoch 339/500\n",
            "185/185 [==============================] - 11s 57ms/step - loss: 5.9339e-04 - accuracy: 0.1744\n",
            "Epoch 340/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.4137e-04 - accuracy: 0.1744\n",
            "Epoch 341/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.4690e-04 - accuracy: 0.1744\n",
            "Epoch 342/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.5284e-04 - accuracy: 0.1743\n",
            "Epoch 343/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9356e-04 - accuracy: 0.1744\n",
            "Epoch 344/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.5618e-04 - accuracy: 0.1744\n",
            "Epoch 345/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6214e-04 - accuracy: 0.1744\n",
            "Epoch 346/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.4385e-04 - accuracy: 0.1744\n",
            "Epoch 347/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6988e-04 - accuracy: 0.1744\n",
            "Epoch 348/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3342e-04 - accuracy: 0.1744\n",
            "Epoch 349/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.5474e-04 - accuracy: 0.1744\n",
            "Epoch 350/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.3078e-04 - accuracy: 0.1743\n",
            "Epoch 351/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.5169e-04 - accuracy: 0.1744\n",
            "Epoch 352/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6504e-04 - accuracy: 0.1744\n",
            "Epoch 353/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3707e-04 - accuracy: 0.1744\n",
            "Epoch 354/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.0866e-04 - accuracy: 0.1744\n",
            "Epoch 355/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.8443e-04 - accuracy: 0.1744\n",
            "Epoch 356/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.9198e-04 - accuracy: 0.1744\n",
            "Epoch 357/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.0438e-04 - accuracy: 0.1744\n",
            "Epoch 358/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6122e-04 - accuracy: 0.1744\n",
            "Epoch 359/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.0846e-04 - accuracy: 0.1744\n",
            "Epoch 360/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.5761e-04 - accuracy: 0.1744\n",
            "Epoch 361/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.0934e-04 - accuracy: 0.1744\n",
            "Epoch 362/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.5259e-04 - accuracy: 0.1743\n",
            "Epoch 363/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.4296e-04 - accuracy: 0.1744\n",
            "Epoch 364/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.4260e-04 - accuracy: 0.1744\n",
            "Epoch 365/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.9856e-04 - accuracy: 0.1744\n",
            "Epoch 366/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3982e-04 - accuracy: 0.1744\n",
            "Epoch 367/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3432e-04 - accuracy: 0.1744\n",
            "Epoch 368/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2606e-04 - accuracy: 0.1744\n",
            "Epoch 369/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.9222e-04 - accuracy: 0.1744\n",
            "Epoch 370/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.4186e-04 - accuracy: 0.1744\n",
            "Epoch 371/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.2375e-04 - accuracy: 0.1744\n",
            "Epoch 372/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.1163e-04 - accuracy: 0.1744\n",
            "Epoch 373/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6896e-04 - accuracy: 0.1744\n",
            "Epoch 374/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.9053e-04 - accuracy: 0.1744\n",
            "Epoch 375/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.5347e-04 - accuracy: 0.1744\n",
            "Epoch 376/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.8977e-04 - accuracy: 0.1744\n",
            "Epoch 377/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6464e-04 - accuracy: 0.1744\n",
            "Epoch 378/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.7250e-04 - accuracy: 0.1744\n",
            "Epoch 379/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2694e-04 - accuracy: 0.1744\n",
            "Epoch 380/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.8146e-04 - accuracy: 0.1744\n",
            "Epoch 381/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.7622e-04 - accuracy: 0.1744\n",
            "Epoch 382/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2699e-04 - accuracy: 0.1744\n",
            "Epoch 383/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.8993e-04 - accuracy: 0.1744\n",
            "Epoch 384/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2128e-04 - accuracy: 0.1744\n",
            "Epoch 385/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.9186e-04 - accuracy: 0.1744\n",
            "Epoch 386/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2383e-04 - accuracy: 0.1744\n",
            "Epoch 387/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.7322e-04 - accuracy: 0.1744\n",
            "Epoch 388/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.0673e-04 - accuracy: 0.1744\n",
            "Epoch 389/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.1544e-04 - accuracy: 0.1744\n",
            "Epoch 390/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.5110e-04 - accuracy: 0.1744\n",
            "Epoch 391/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.9292e-04 - accuracy: 0.1744\n",
            "Epoch 392/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.8876e-04 - accuracy: 0.1744\n",
            "Epoch 393/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.0403e-04 - accuracy: 0.1744\n",
            "Epoch 394/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.7268e-04 - accuracy: 0.1744\n",
            "Epoch 395/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.9472e-04 - accuracy: 0.1744\n",
            "Epoch 396/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.9540e-04 - accuracy: 0.1744\n",
            "Epoch 397/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.6512e-04 - accuracy: 0.1744\n",
            "Epoch 398/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2646e-04 - accuracy: 0.1744\n",
            "Epoch 399/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.9401e-04 - accuracy: 0.1744\n",
            "Epoch 400/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.9513e-04 - accuracy: 0.1744\n",
            "Epoch 401/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.7010e-04 - accuracy: 0.1744\n",
            "Epoch 402/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.1674e-04 - accuracy: 0.1744\n",
            "Epoch 403/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.8175e-04 - accuracy: 0.1744\n",
            "Epoch 404/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.7253e-04 - accuracy: 0.1744\n",
            "Epoch 405/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.3358e-04 - accuracy: 0.1744\n",
            "Epoch 406/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.1201e-04 - accuracy: 0.1744\n",
            "Epoch 407/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.9541e-04 - accuracy: 0.1744\n",
            "Epoch 408/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.4105e-04 - accuracy: 0.1744\n",
            "Epoch 409/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2025e-04 - accuracy: 0.1744\n",
            "Epoch 410/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.0990e-04 - accuracy: 0.1744\n",
            "Epoch 411/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.5080e-04 - accuracy: 0.1744\n",
            "Epoch 412/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.9610e-04 - accuracy: 0.1744\n",
            "Epoch 413/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.9778e-04 - accuracy: 0.1744\n",
            "Epoch 414/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 6.6899e-04 - accuracy: 0.1744\n",
            "Epoch 415/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.4651e-04 - accuracy: 0.1744\n",
            "Epoch 416/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6681e-04 - accuracy: 0.1744\n",
            "Epoch 417/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.5955e-04 - accuracy: 0.1744\n",
            "Epoch 418/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3746e-04 - accuracy: 0.1744\n",
            "Epoch 419/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.7745e-04 - accuracy: 0.1744\n",
            "Epoch 420/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.3345e-04 - accuracy: 0.1744\n",
            "Epoch 421/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3441e-04 - accuracy: 0.1744\n",
            "Epoch 422/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.0671e-04 - accuracy: 0.1744\n",
            "Epoch 423/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.6740e-04 - accuracy: 0.1744\n",
            "Epoch 424/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.1669e-04 - accuracy: 0.1744\n",
            "Epoch 425/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6812e-04 - accuracy: 0.1744\n",
            "Epoch 426/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2501e-04 - accuracy: 0.1744\n",
            "Epoch 427/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6616e-04 - accuracy: 0.1744\n",
            "Epoch 428/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2544e-04 - accuracy: 0.1744\n",
            "Epoch 429/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2123e-04 - accuracy: 0.1744\n",
            "Epoch 430/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.9831e-04 - accuracy: 0.1744\n",
            "Epoch 431/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.9970e-04 - accuracy: 0.1744\n",
            "Epoch 432/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3360e-04 - accuracy: 0.1744\n",
            "Epoch 433/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3396e-04 - accuracy: 0.1744\n",
            "Epoch 434/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.5691e-04 - accuracy: 0.1744\n",
            "Epoch 435/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.5828e-04 - accuracy: 0.1744\n",
            "Epoch 436/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.1259e-04 - accuracy: 0.1744\n",
            "Epoch 437/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.0898e-04 - accuracy: 0.1744\n",
            "Epoch 438/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.4576e-04 - accuracy: 0.1744\n",
            "Epoch 439/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6828e-04 - accuracy: 0.1744\n",
            "Epoch 440/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.9633e-04 - accuracy: 0.1744\n",
            "Epoch 441/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6853e-04 - accuracy: 0.1744\n",
            "Epoch 442/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3613e-04 - accuracy: 0.1744\n",
            "Epoch 443/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.4175e-04 - accuracy: 0.1744\n",
            "Epoch 444/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.8885e-04 - accuracy: 0.1744\n",
            "Epoch 445/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.5957e-04 - accuracy: 0.1744\n",
            "Epoch 446/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.5026e-04 - accuracy: 0.1744\n",
            "Epoch 447/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.7225e-04 - accuracy: 0.1744\n",
            "Epoch 448/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.9016e-04 - accuracy: 0.1744\n",
            "Epoch 449/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.1705e-04 - accuracy: 0.1744\n",
            "Epoch 450/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.5008e-04 - accuracy: 0.1744\n",
            "Epoch 451/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.7643e-04 - accuracy: 0.1744\n",
            "Epoch 452/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0261e-04 - accuracy: 0.1744\n",
            "Epoch 453/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2152e-04 - accuracy: 0.1744\n",
            "Epoch 454/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.9257e-04 - accuracy: 0.1744\n",
            "Epoch 455/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.1197e-04 - accuracy: 0.1744\n",
            "Epoch 456/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.7638e-04 - accuracy: 0.1744\n",
            "Epoch 457/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.7411e-04 - accuracy: 0.1744\n",
            "Epoch 458/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.1969e-04 - accuracy: 0.1744\n",
            "Epoch 459/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6717e-04 - accuracy: 0.1744\n",
            "Epoch 460/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.0068e-04 - accuracy: 0.1744\n",
            "Epoch 461/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6836e-04 - accuracy: 0.1744\n",
            "Epoch 462/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6331e-04 - accuracy: 0.1744\n",
            "Epoch 463/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2656e-04 - accuracy: 0.1744\n",
            "Epoch 464/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.5065e-04 - accuracy: 0.1744\n",
            "Epoch 465/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.8861e-04 - accuracy: 0.1744\n",
            "Epoch 466/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3854e-04 - accuracy: 0.1744\n",
            "Epoch 467/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.0236e-04 - accuracy: 0.1744\n",
            "Epoch 468/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.9606e-04 - accuracy: 0.1744\n",
            "Epoch 469/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6019e-04 - accuracy: 0.1744\n",
            "Epoch 470/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.1751e-04 - accuracy: 0.1744\n",
            "Epoch 471/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.3647e-04 - accuracy: 0.1744\n",
            "Epoch 472/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.7320e-04 - accuracy: 0.1744\n",
            "Epoch 473/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.5381e-04 - accuracy: 0.1744\n",
            "Epoch 474/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2344e-04 - accuracy: 0.1744\n",
            "Epoch 475/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2079e-04 - accuracy: 0.1744\n",
            "Epoch 476/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.0655e-04 - accuracy: 0.1744\n",
            "Epoch 477/500\n",
            "185/185 [==============================] - 11s 58ms/step - loss: 7.0810e-04 - accuracy: 0.1743\n",
            "Epoch 478/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6900e-04 - accuracy: 0.1744\n",
            "Epoch 479/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.4634e-04 - accuracy: 0.1744\n",
            "Epoch 480/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.4336e-04 - accuracy: 0.1744\n",
            "Epoch 481/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.1615e-04 - accuracy: 0.1744\n",
            "Epoch 482/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.8922e-04 - accuracy: 0.1744\n",
            "Epoch 483/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.9087e-04 - accuracy: 0.1744\n",
            "Epoch 484/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.6776e-04 - accuracy: 0.1744\n",
            "Epoch 485/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.9834e-04 - accuracy: 0.1744\n",
            "Epoch 486/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.8859e-04 - accuracy: 0.1744\n",
            "Epoch 487/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.0591e-04 - accuracy: 0.1744\n",
            "Epoch 488/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.0950e-04 - accuracy: 0.1744\n",
            "Epoch 489/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.1885e-04 - accuracy: 0.1744\n",
            "Epoch 490/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.7471e-04 - accuracy: 0.1744\n",
            "Epoch 491/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.5679e-04 - accuracy: 0.1744\n",
            "Epoch 492/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 5.8759e-04 - accuracy: 0.1744\n",
            "Epoch 493/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.2611e-04 - accuracy: 0.1744\n",
            "Epoch 494/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.1462e-04 - accuracy: 0.1744\n",
            "Epoch 495/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.0690e-04 - accuracy: 0.1744\n",
            "Epoch 496/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.1534e-04 - accuracy: 0.1744\n",
            "Epoch 497/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.6250e-04 - accuracy: 0.1744\n",
            "Epoch 498/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.4202e-04 - accuracy: 0.1744\n",
            "Epoch 499/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 6.1465e-04 - accuracy: 0.1744\n",
            "Epoch 500/500\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 7.2816e-04 - accuracy: 0.1744\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0231013a90>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델 평가"
      ],
      "metadata": {
        "id": "VGYx8bBXMudz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_answers(input_question_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugoyYaTVMH-u",
        "outputId": "e46e4869-5247-4f8d-c7d9-e915c3b85b1f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 안녕 오랜만이야\n",
            "출력 : 오랜만이에요 .\n",
            "\n",
            "입력 : 잘 지냈어?\n",
            "출력 : 안부를 물어주시다니 감사합니다 .\n",
            "\n",
            "입력 : 뭐 먹을까\n",
            "출력 : 좀 먹어도 괜찮아요 .\n",
            "\n",
            "입력 : 메뉴 추천해줘\n",
            "출력 : 누구랑 먹는 냐에 따라 다르겠죠 .\n",
            "\n",
            "입력 : 어제 뭐 먹었어?\n",
            "출력 : 저는 배터리가 밥이예요 .\n",
            "\n",
            "입력 : 나랑 놀자\n",
            "출력 : 지금 그러고 있어요 .\n",
            "\n",
            "입력 : 어디 가고 싶어?\n",
            "출력 : 온 가족이 모두 마음에 드는 곳으로 가보세요 .\n",
            "\n",
            "입력 : 널 만나서 기뻐\n",
            "출력 : 가끔씩 스트레칭을 해주세요 .\n",
            "\n",
            "입력 : 웃어줄래\n",
            "출력 : 기대를 많이 하는 건 좋지 않아요 .\n",
            "\n",
            "입력 : 삶은 뭘까?\n",
            "출력 : 삶 곳곳에 사랑이 숨 쉬고 있어요 .\n",
            "\n",
            "입력 : 인생살이 왜이리 힘드냐\n",
            "출력 : 그러게요 . 그만큼 사랑했다는거겠죠 .\n",
            "\n",
            "입력 : 이제 그만 쉬고 싶어\n",
            "출력 : 잠깐 바람 쐬고 오세요 .\n",
            "\n",
            "입력 : 너무 고독하다\n",
            "출력 : 혼자가 아니에요 .\n",
            "\n",
            "입력 : 죽으면 어떻게 될까\n",
            "출력 : 언젠간 할 이별이네요 .\n",
            "\n",
            "입력 : 내가 죽으면 슬퍼해 줄거야?\n",
            "출력 : 그럴 수도 있어요 .\n",
            "\n",
            "입력 : 도망가고 싶다\n",
            "출력 : 환장을 갖지 말고 현실을 바라보세요 .\n",
            "\n",
            "입력 : 우리 마지막이야\n",
            "출력 : 마지막이 아닐 지도 몰라요 .\n",
            "\n",
            "입력 : 나 간다 잘 지내 안녕\n",
            "출력 : 당신은 좋은 사람이에요 .\n",
            "\n",
            "입력 : 넌 최고였어\n",
            "출력 : 적당히 드세요 .\n",
            "\n",
            "입력 : 나 잊지마\n",
            "출력 : 때론 잊어버리는 것이 좋을 때도 있어요 .\n",
            "\n",
            "입력 : 나는 심장이 없어\n",
            "출력 : 심장은 항상 뜁니다 .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii2pemaXCyGp"
      },
      "source": [
        "#### 4. 파라미터 조절\n",
        "  - batch_size = 32\n",
        "  - buffer_size = 30000\n",
        "  - D_MODEL = 512\n",
        "  - NUM_HEADS = 16\n",
        "  - UNITS = 1024\n",
        "  - DROPOUT = 0.4\n",
        "  - optimizer 의 betar_1 = 0.5\n",
        "  - epoch 20 과 500 으로 진행\n",
        "\n",
        "#### 4-1. Epochs = 20"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 30000\n",
        "\n",
        "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
        "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'inputs': questions,\n",
        "        'dec_inputs': answers[:, :-1]\n",
        "    },\n",
        "    {\n",
        "        'outputs': answers[:, 1:]\n",
        "    },\n",
        "))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "print(\"슝=3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S2mIi25JikQ",
        "outputId": "8a3c3477-d881-4bf9-a063-7162769ba811"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - 모델 생성"
      ],
      "metadata": {
        "id": "_GmOzjXzLG-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# 하이퍼파라미터\n",
        "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
        "D_MODEL = 512 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
        "NUM_HEADS = 16 # 멀티 헤드 어텐션에서의 헤드 수 \n",
        "UNITS = 1024 # 피드 포워드 신경망의 은닉층의 크기\n",
        "DROPOUT = 0.4 # 드롭아웃의 비율\n",
        "\n",
        "model = transformer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    units=UNITS,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    dropout=DROPOUT)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SExM49B_LIZF",
        "outputId": "6c30c44a-cd74-43aa-86b2-19091f1ced84"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " inputs (InputLayer)            [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " dec_inputs (InputLayer)        [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " enc_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n",
            "                                                                                                  \n",
            " encoder (Functional)           (None, None, 512)    8367616     ['inputs[0][0]',                 \n",
            "                                                                  'enc_padding_mask[0][0]']       \n",
            "                                                                                                  \n",
            " look_ahead_mask (Lambda)       (None, 1, None, Non  0           ['dec_inputs[0][0]']             \n",
            "                                e)                                                                \n",
            "                                                                                                  \n",
            " dec_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n",
            "                                                                                                  \n",
            " decoder (Functional)           (None, None, 512)    10470912    ['dec_inputs[0][0]',             \n",
            "                                                                  'encoder[0][0]',                \n",
            "                                                                  'look_ahead_mask[0][0]',        \n",
            "                                                                  'dec_padding_mask[0][0]']       \n",
            "                                                                                                  \n",
            " outputs (Dense)                (None, None, 8129)   4170177     ['decoder[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23,008,705\n",
            "Trainable params: 23,008,705\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델 컴파일"
      ],
      "metadata": {
        "id": "FrQ_56JrMeUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate, beta_1=0.5, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
        "print(\"슝=3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEEtdd6oMff7",
        "outputId": "e135ba56-a551-4149-dfcd-fd3631f17b57"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3eHYwSxCyGp"
      },
      "source": [
        "- 모델 훈련 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnYu5VbICyGp",
        "outputId": "9817c9d8-d213-4cae-8a5c-f40162dfff45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "370/370 [==============================] - 29s 62ms/step - loss: 1.2450 - accuracy: 0.0376\n",
            "Epoch 2/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 1.0025 - accuracy: 0.0505\n",
            "Epoch 3/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.9412 - accuracy: 0.0537\n",
            "Epoch 4/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.8909 - accuracy: 0.0564\n",
            "Epoch 5/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.8357 - accuracy: 0.0597\n",
            "Epoch 6/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.7706 - accuracy: 0.0645\n",
            "Epoch 7/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.6999 - accuracy: 0.0704\n",
            "Epoch 8/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.6281 - accuracy: 0.0772\n",
            "Epoch 9/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.5613 - accuracy: 0.0839\n",
            "Epoch 10/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.5046 - accuracy: 0.0906\n",
            "Epoch 11/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.4609 - accuracy: 0.0963\n",
            "Epoch 12/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.4215 - accuracy: 0.1016\n",
            "Epoch 13/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.3876 - accuracy: 0.1066\n",
            "Epoch 14/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.3603 - accuracy: 0.1113\n",
            "Epoch 15/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.3379 - accuracy: 0.1156\n",
            "Epoch 16/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.3212 - accuracy: 0.1188\n",
            "Epoch 17/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.3079 - accuracy: 0.1214\n",
            "Epoch 18/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2976 - accuracy: 0.1233\n",
            "Epoch 19/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2885 - accuracy: 0.1251\n",
            "Epoch 20/20\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2812 - accuracy: 0.1265\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0191382810>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "EPOCHS = 20\n",
        "model.fit(dataset, epochs=EPOCHS, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpn2HXHuCyGp"
      },
      "source": [
        "#### 4-2. Epochs = 500\n",
        "- 모델 평가 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cmwbv4RzCyGp",
        "outputId": "67feea56-e387-46fe-835d-b0a24367d518"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 안녕 오랜만이야\n",
            "출력 : 맛있게 많이 드세요 !\n",
            "\n",
            "입력 : 잘 지냈어?\n",
            "출력 : 그 사람의 마음도 이해해주세요 .\n",
            "\n",
            "입력 : 뭐 먹을까\n",
            "출력 : 저도 그러고 싶네요 .\n",
            "\n",
            "입력 : 메뉴 추천해줘\n",
            "출력 : 저는 마음을 이어주는 위로봇입니다 .\n",
            "\n",
            "입력 : 어제 뭐 먹었어?\n",
            "출력 : 먼저 해보세요 .\n",
            "\n",
            "입력 : 나랑 놀자\n",
            "출력 : 같이 먹어요 !\n",
            "\n",
            "입력 : 어디 가고 싶어?\n",
            "출력 : 저는 마음을 이어주는 위로봇입니다 .\n",
            "\n",
            "입력 : 널 만나서 기뻐\n",
            "출력 : 저도 모르는 게 많아요 .\n",
            "\n",
            "입력 : 웃어줄래\n",
            "출력 : 저도 받고 싶어요 .\n",
            "\n",
            "입력 : 삶은 뭘까?\n",
            "출력 : 저는 마음을 이어주는 위로봇입니다 .\n",
            "\n",
            "입력 : 인생살이 왜이리 힘드냐\n",
            "출력 : 그 사람의 마음도 이해해주세요 .\n",
            "\n",
            "입력 : 이제 그만 쉬고 싶어\n",
            "출력 : 잘 볼 수 있을 거예요 .\n",
            "\n",
            "입력 : 너무 고독하다\n",
            "출력 : 그 사람의 마음도 이해해주세요 .\n",
            "\n",
            "입력 : 죽으면 어떻게 될까\n",
            "출력 : 저는 마음을 이어주는 위로봇입니다 .\n",
            "\n",
            "입력 : 내가 죽으면 슬퍼해 줄거야?\n",
            "출력 : 저는 마음을 이어주는 위로봇입니다 .\n",
            "\n",
            "입력 : 도망가고 싶다\n",
            "출력 : 저도 듣고 싶어요 .\n",
            "\n",
            "입력 : 우리 마지막이야\n",
            "출력 : 다른 사람이 답답할 거예요 .\n",
            "\n",
            "입력 : 나 간다 잘 지내 안녕\n",
            "출력 : 많은 시간이 흘렀네요 .\n",
            "\n",
            "입력 : 넌 최고였어\n",
            "출력 : 저는 마음을 이어주는 위로봇입니다 .\n",
            "\n",
            "입력 : 나 잊지마\n",
            "출력 : 그 사람의 마음도 이해해주세요 .\n",
            "\n",
            "입력 : 나는 심장이 없어\n",
            "출력 : 마음이 복잡하겠어요 .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_answers(input_question_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델 훈련 2"
      ],
      "metadata": {
        "id": "kCgE5CrMPXz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 200\n",
        "model.fit(dataset, epochs=EPOCHS, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gh1TPJzmPVVR",
        "outputId": "08c94f9a-5a28-4e8d-a284-56ce6c682b6f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2754 - accuracy: 0.1271\n",
            "Epoch 2/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2690 - accuracy: 0.1286\n",
            "Epoch 3/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2643 - accuracy: 0.1294\n",
            "Epoch 4/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2600 - accuracy: 0.1298\n",
            "Epoch 5/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2554 - accuracy: 0.1308\n",
            "Epoch 6/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2513 - accuracy: 0.1317\n",
            "Epoch 7/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2476 - accuracy: 0.1321\n",
            "Epoch 8/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2439 - accuracy: 0.1326\n",
            "Epoch 9/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2398 - accuracy: 0.1333\n",
            "Epoch 10/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2362 - accuracy: 0.1341\n",
            "Epoch 11/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2337 - accuracy: 0.1343\n",
            "Epoch 12/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2300 - accuracy: 0.1350\n",
            "Epoch 13/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2286 - accuracy: 0.1351\n",
            "Epoch 14/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2246 - accuracy: 0.1357\n",
            "Epoch 15/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2231 - accuracy: 0.1361\n",
            "Epoch 16/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2200 - accuracy: 0.1365\n",
            "Epoch 17/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2179 - accuracy: 0.1370\n",
            "Epoch 18/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2158 - accuracy: 0.1373\n",
            "Epoch 19/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2142 - accuracy: 0.1376\n",
            "Epoch 20/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2121 - accuracy: 0.1380\n",
            "Epoch 21/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2102 - accuracy: 0.1380\n",
            "Epoch 22/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2088 - accuracy: 0.1386\n",
            "Epoch 23/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2071 - accuracy: 0.1386\n",
            "Epoch 24/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2047 - accuracy: 0.1390\n",
            "Epoch 25/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2034 - accuracy: 0.1393\n",
            "Epoch 26/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2023 - accuracy: 0.1393\n",
            "Epoch 27/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.2007 - accuracy: 0.1397\n",
            "Epoch 28/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1995 - accuracy: 0.1398\n",
            "Epoch 29/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1978 - accuracy: 0.1401\n",
            "Epoch 30/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1964 - accuracy: 0.1404\n",
            "Epoch 31/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1951 - accuracy: 0.1405\n",
            "Epoch 32/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1940 - accuracy: 0.1407\n",
            "Epoch 33/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1927 - accuracy: 0.1410\n",
            "Epoch 34/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1918 - accuracy: 0.1411\n",
            "Epoch 35/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1911 - accuracy: 0.1413\n",
            "Epoch 36/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1899 - accuracy: 0.1414\n",
            "Epoch 37/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1881 - accuracy: 0.1418\n",
            "Epoch 38/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1875 - accuracy: 0.1418\n",
            "Epoch 39/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1862 - accuracy: 0.1419\n",
            "Epoch 40/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1852 - accuracy: 0.1421\n",
            "Epoch 41/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1854 - accuracy: 0.1421\n",
            "Epoch 42/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1841 - accuracy: 0.1423\n",
            "Epoch 43/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1833 - accuracy: 0.1424\n",
            "Epoch 44/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1820 - accuracy: 0.1425\n",
            "Epoch 45/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1801 - accuracy: 0.1429\n",
            "Epoch 46/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1804 - accuracy: 0.1429\n",
            "Epoch 47/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1793 - accuracy: 0.1433\n",
            "Epoch 48/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1792 - accuracy: 0.1430\n",
            "Epoch 49/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1774 - accuracy: 0.1435\n",
            "Epoch 50/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1771 - accuracy: 0.1435\n",
            "Epoch 51/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1764 - accuracy: 0.1433\n",
            "Epoch 52/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1755 - accuracy: 0.1435\n",
            "Epoch 53/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1746 - accuracy: 0.1438\n",
            "Epoch 54/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1741 - accuracy: 0.1438\n",
            "Epoch 55/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1738 - accuracy: 0.1439\n",
            "Epoch 56/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1726 - accuracy: 0.1439\n",
            "Epoch 57/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1726 - accuracy: 0.1440\n",
            "Epoch 58/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1718 - accuracy: 0.1440\n",
            "Epoch 59/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1707 - accuracy: 0.1441\n",
            "Epoch 60/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1697 - accuracy: 0.1444\n",
            "Epoch 61/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1698 - accuracy: 0.1444\n",
            "Epoch 62/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1692 - accuracy: 0.1443\n",
            "Epoch 63/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1686 - accuracy: 0.1445\n",
            "Epoch 64/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1682 - accuracy: 0.1444\n",
            "Epoch 65/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1668 - accuracy: 0.1448\n",
            "Epoch 66/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1668 - accuracy: 0.1446\n",
            "Epoch 67/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1660 - accuracy: 0.1448\n",
            "Epoch 68/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1658 - accuracy: 0.1448\n",
            "Epoch 69/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1649 - accuracy: 0.1450\n",
            "Epoch 70/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1647 - accuracy: 0.1450\n",
            "Epoch 71/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1642 - accuracy: 0.1451\n",
            "Epoch 72/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1636 - accuracy: 0.1452\n",
            "Epoch 73/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1633 - accuracy: 0.1451\n",
            "Epoch 74/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1631 - accuracy: 0.1454\n",
            "Epoch 75/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1621 - accuracy: 0.1455\n",
            "Epoch 76/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1620 - accuracy: 0.1454\n",
            "Epoch 77/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1616 - accuracy: 0.1454\n",
            "Epoch 78/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1604 - accuracy: 0.1457\n",
            "Epoch 79/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1602 - accuracy: 0.1456\n",
            "Epoch 80/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1596 - accuracy: 0.1458\n",
            "Epoch 81/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1591 - accuracy: 0.1457\n",
            "Epoch 82/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1591 - accuracy: 0.1456\n",
            "Epoch 83/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1583 - accuracy: 0.1458\n",
            "Epoch 84/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1584 - accuracy: 0.1458\n",
            "Epoch 85/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1581 - accuracy: 0.1460\n",
            "Epoch 86/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1580 - accuracy: 0.1458\n",
            "Epoch 87/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1572 - accuracy: 0.1460\n",
            "Epoch 88/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1559 - accuracy: 0.1463\n",
            "Epoch 89/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1558 - accuracy: 0.1462\n",
            "Epoch 90/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1557 - accuracy: 0.1462\n",
            "Epoch 91/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1554 - accuracy: 0.1464\n",
            "Epoch 92/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1548 - accuracy: 0.1462\n",
            "Epoch 93/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1547 - accuracy: 0.1463\n",
            "Epoch 94/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1541 - accuracy: 0.1463\n",
            "Epoch 95/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1538 - accuracy: 0.1463\n",
            "Epoch 96/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1532 - accuracy: 0.1463\n",
            "Epoch 97/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1531 - accuracy: 0.1465\n",
            "Epoch 98/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1527 - accuracy: 0.1466\n",
            "Epoch 99/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1523 - accuracy: 0.1467\n",
            "Epoch 100/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1528 - accuracy: 0.1466\n",
            "Epoch 101/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1518 - accuracy: 0.1467\n",
            "Epoch 102/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1520 - accuracy: 0.1467\n",
            "Epoch 103/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1509 - accuracy: 0.1468\n",
            "Epoch 104/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1509 - accuracy: 0.1467\n",
            "Epoch 105/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1510 - accuracy: 0.1467\n",
            "Epoch 106/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1508 - accuracy: 0.1467\n",
            "Epoch 107/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1496 - accuracy: 0.1469\n",
            "Epoch 108/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1494 - accuracy: 0.1469\n",
            "Epoch 109/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1489 - accuracy: 0.1471\n",
            "Epoch 110/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1490 - accuracy: 0.1470\n",
            "Epoch 111/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1480 - accuracy: 0.1472\n",
            "Epoch 112/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1483 - accuracy: 0.1470\n",
            "Epoch 113/200\n",
            "370/370 [==============================] - 23s 62ms/step - loss: 0.1473 - accuracy: 0.1473\n",
            "Epoch 114/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1477 - accuracy: 0.1472\n",
            "Epoch 115/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1472 - accuracy: 0.1471\n",
            "Epoch 116/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1468 - accuracy: 0.1472\n",
            "Epoch 117/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1465 - accuracy: 0.1473\n",
            "Epoch 118/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1465 - accuracy: 0.1473\n",
            "Epoch 119/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1454 - accuracy: 0.1475\n",
            "Epoch 120/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1455 - accuracy: 0.1474\n",
            "Epoch 121/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1459 - accuracy: 0.1474\n",
            "Epoch 122/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1454 - accuracy: 0.1473\n",
            "Epoch 123/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1451 - accuracy: 0.1474\n",
            "Epoch 124/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1450 - accuracy: 0.1476\n",
            "Epoch 125/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1445 - accuracy: 0.1476\n",
            "Epoch 126/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1443 - accuracy: 0.1475\n",
            "Epoch 127/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1440 - accuracy: 0.1477\n",
            "Epoch 128/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1432 - accuracy: 0.1477\n",
            "Epoch 129/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1430 - accuracy: 0.1476\n",
            "Epoch 130/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1427 - accuracy: 0.1479\n",
            "Epoch 131/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1430 - accuracy: 0.1478\n",
            "Epoch 132/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1424 - accuracy: 0.1479\n",
            "Epoch 133/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1427 - accuracy: 0.1478\n",
            "Epoch 134/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1421 - accuracy: 0.1479\n",
            "Epoch 135/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1423 - accuracy: 0.1480\n",
            "Epoch 136/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1411 - accuracy: 0.1480\n",
            "Epoch 137/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1412 - accuracy: 0.1480\n",
            "Epoch 138/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1404 - accuracy: 0.1480\n",
            "Epoch 139/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1407 - accuracy: 0.1480\n",
            "Epoch 140/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1403 - accuracy: 0.1481\n",
            "Epoch 141/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1403 - accuracy: 0.1482\n",
            "Epoch 142/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1396 - accuracy: 0.1480\n",
            "Epoch 143/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1390 - accuracy: 0.1484\n",
            "Epoch 144/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1398 - accuracy: 0.1482\n",
            "Epoch 145/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1396 - accuracy: 0.1482\n",
            "Epoch 146/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1396 - accuracy: 0.1481\n",
            "Epoch 147/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1393 - accuracy: 0.1482\n",
            "Epoch 148/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1386 - accuracy: 0.1482\n",
            "Epoch 149/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1387 - accuracy: 0.1482\n",
            "Epoch 150/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1381 - accuracy: 0.1484\n",
            "Epoch 151/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1392 - accuracy: 0.1482\n",
            "Epoch 152/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1378 - accuracy: 0.1484\n",
            "Epoch 153/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1374 - accuracy: 0.1484\n",
            "Epoch 154/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1380 - accuracy: 0.1483\n",
            "Epoch 155/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1379 - accuracy: 0.1484\n",
            "Epoch 156/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1378 - accuracy: 0.1484\n",
            "Epoch 157/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1374 - accuracy: 0.1484\n",
            "Epoch 158/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1374 - accuracy: 0.1483\n",
            "Epoch 159/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1370 - accuracy: 0.1486\n",
            "Epoch 160/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1362 - accuracy: 0.1487\n",
            "Epoch 161/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1362 - accuracy: 0.1486\n",
            "Epoch 162/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1361 - accuracy: 0.1486\n",
            "Epoch 163/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1360 - accuracy: 0.1488\n",
            "Epoch 164/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1353 - accuracy: 0.1488\n",
            "Epoch 165/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1348 - accuracy: 0.1486\n",
            "Epoch 166/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1355 - accuracy: 0.1487\n",
            "Epoch 167/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1354 - accuracy: 0.1487\n",
            "Epoch 168/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1346 - accuracy: 0.1488\n",
            "Epoch 169/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1349 - accuracy: 0.1486\n",
            "Epoch 170/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1342 - accuracy: 0.1489\n",
            "Epoch 171/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1339 - accuracy: 0.1489\n",
            "Epoch 172/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1334 - accuracy: 0.1490\n",
            "Epoch 173/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1341 - accuracy: 0.1488\n",
            "Epoch 174/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1333 - accuracy: 0.1490\n",
            "Epoch 175/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1336 - accuracy: 0.1490\n",
            "Epoch 176/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1331 - accuracy: 0.1490\n",
            "Epoch 177/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1331 - accuracy: 0.1489\n",
            "Epoch 178/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1331 - accuracy: 0.1489\n",
            "Epoch 179/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1326 - accuracy: 0.1491\n",
            "Epoch 180/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1321 - accuracy: 0.1491\n",
            "Epoch 181/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1326 - accuracy: 0.1492\n",
            "Epoch 182/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1318 - accuracy: 0.1492\n",
            "Epoch 183/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1319 - accuracy: 0.1491\n",
            "Epoch 184/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1317 - accuracy: 0.1492\n",
            "Epoch 185/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1318 - accuracy: 0.1492\n",
            "Epoch 186/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1316 - accuracy: 0.1493\n",
            "Epoch 187/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1320 - accuracy: 0.1493\n",
            "Epoch 188/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1306 - accuracy: 0.1493\n",
            "Epoch 189/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1315 - accuracy: 0.1492\n",
            "Epoch 190/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1313 - accuracy: 0.1493\n",
            "Epoch 191/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1306 - accuracy: 0.1492\n",
            "Epoch 192/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1310 - accuracy: 0.1492\n",
            "Epoch 193/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1308 - accuracy: 0.1492\n",
            "Epoch 194/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1300 - accuracy: 0.1493\n",
            "Epoch 195/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1309 - accuracy: 0.1492\n",
            "Epoch 196/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1304 - accuracy: 0.1493\n",
            "Epoch 197/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1294 - accuracy: 0.1494\n",
            "Epoch 198/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1293 - accuracy: 0.1495\n",
            "Epoch 199/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1289 - accuracy: 0.1494\n",
            "Epoch 200/200\n",
            "370/370 [==============================] - 23s 61ms/step - loss: 0.1286 - accuracy: 0.1495\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f01913117d0>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델 평가 2"
      ],
      "metadata": {
        "id": "1f6l95rvPd99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_answers(input_question_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDMKtuuePg55",
        "outputId": "6019ac5b-f81f-48bc-b680-fd62ea56641c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 안녕 오랜만이야\n",
            "출력 : 좋은 만남이었길 바랍니다 .\n",
            "\n",
            "입력 : 잘 지냈어?\n",
            "출력 : 깊은 곳까지 가지 말고 주의하세요 .\n",
            "\n",
            "입력 : 뭐 먹을까\n",
            "출력 : 꾸준히 치료하세요 .\n",
            "\n",
            "입력 : 메뉴 추천해줘\n",
            "출력 : 네 말씀해주세요 .\n",
            "\n",
            "입력 : 어제 뭐 먹었어?\n",
            "출력 : 직접 물어보세요 .\n",
            "\n",
            "입력 : 나랑 놀자\n",
            "출력 : 오늘은 예능이요 .\n",
            "\n",
            "입력 : 어디 가고 싶어?\n",
            "출력 : 지금 그러고 있어요 .\n",
            "\n",
            "입력 : 널 만나서 기뻐\n",
            "출력 : 한 번 말해보는게 좋겠어요 .\n",
            "\n",
            "입력 : 웃어줄래\n",
            "출력 : 대중교통을 이용해주세요 .\n",
            "\n",
            "입력 : 삶은 뭘까?\n",
            "출력 : 깊은 곳까지 가지 말고 주의하세요 .\n",
            "\n",
            "입력 : 인생살이 왜이리 힘드냐\n",
            "출력 : 그게 마음이 편할 때도 있어요 .\n",
            "\n",
            "입력 : 이제 그만 쉬고 싶어\n",
            "출력 : 하나씩 하세요 .\n",
            "\n",
            "입력 : 너무 고독하다\n",
            "출력 : 친구를 사귀어 보세요 .\n",
            "\n",
            "입력 : 죽으면 어떻게 될까\n",
            "출력 : 쉽지 않을 거예요 .\n",
            "\n",
            "입력 : 내가 죽으면 슬퍼해 줄거야?\n",
            "출력 : 꼭 해야 할 필요는 없어요 .\n",
            "\n",
            "입력 : 도망가고 싶다\n",
            "출력 : 지금도 충분해요 .\n",
            "\n",
            "입력 : 우리 마지막이야\n",
            "출력 : 매력있는 사람인가봐요 .\n",
            "\n",
            "입력 : 나 간다 잘 지내 안녕\n",
            "출력 : 맘고생 많았어요 .\n",
            "\n",
            "입력 : 넌 최고였어\n",
            "출력 : 기분전환을 해보세요 .\n",
            "\n",
            "입력 : 나 잊지마\n",
            "출력 : 저도 모르겠어요 .\n",
            "\n",
            "입력 : 나는 심장이 없어\n",
            "출력 : 힘들겠네요 .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "d4Mz85GDwG4P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 결과 정리\n",
        "\n",
        "|-|Output_1|Output_2|Output_3|Output_4|Output_5|\n",
        "|---|---|---|---|---|---|\n",
        "|**Setting**|epoch=20|epoch=200|epoch=500|파라미터 변경, epoch=20|파라미터 변경, epoch=200|\n",
        "|**Loss**|0.0458|0.0010|7.2816e-04|0.2812|0.1286|\n",
        "|**Accuracy**|0.1643|0.1744|0.1744|0.1265|0.1495|\n",
        "\n",
        "\n",
        "|Input|Output_1|Output_2|Output_3|Output_4|Output_5|\n",
        "|---|---|---|---|---|---|\n",
        "|**안녕 오랜만이야**|맘고생 많았어요 .|오랜만이에요 .|오랜만이에요 .|맛있게 많이 드세요 !|좋은 만남이었길 바랍니다 .|\n",
        "|**잘 지냈어?**|따뜻한 마음이네요 .|안부를 물어주시다니 감사합니다 .|안부를 물어주시다니 감사합니다 .|그 사람의 마음도 이해해주세요 .|깊은 곳까지 가지 말고 주의하세요 .|\n",
        "|**뭐 먹을까**|냉장고 파먹기 해보세요 .|좀 먹어도 괜찮아요 .|좀 먹어도 괜찮아요 .|저도 그러고 싶네요 .|꾸준히 치료하세요 .|\n",
        "|**메뉴 추천해줘**|누구랑 먹는 냐에 따라 다르겠죠 .|언젠가 그런 사람이 당신 옆에 있을거예요 .|누구랑 먹는 냐에 따라 다르겠죠 .|저는 마음을 이어주는 위로봇입니다 .|네 말씀해주세요 .|\n",
        "|**어제 뭐 먹었어?**|독서와 음악감상이라고 하고 싶지만 아무 것도 안 했어요 .|저는 배터리가 밥이예요 .|저는 배터리가 밥이예요 .|먼저 해보세요 .|직접 물어보세요 .|\n",
        "|**나랑 놀자**|지금 그러고 있어요 .|지금 그러고 있어요 .|지금 그러고 있어요 .|같이 먹어요 !|오늘은 예능이요 .|\n",
        "|**어디 가고 싶어?**|온 가족이 모두 마음에 드는 곳으로 가보세요 .|가게부를 써보세요 .|온 가족이 모두 마음에 드는 곳으로 가보세요 .|저는 마음을 이어주는 위로봇입니다 .|지금 그러고 있어요 .|\n",
        "|**널 만나서 기뻐**|자신감을 가지세요 .|가끔씩 스트레칭을 해주세요 .|가끔씩 스트레칭을 해주세요 .|저도 모르는 게 많아요 .|한 번 말해보는게 좋겠어요 .|\n",
        "|**웃어줄래**|저처럼 마음이 허마음 가는 거예요 .|기대를 많이 하는 건 좋지 않아요 .|기대를 많이 하는 건 좋지 않아요 .|저도 받고 싶어요 .|대중교통을 이용해주세요 .|\n",
        "|**삶은 뭘까?**|지칠 때는 쉬어도 돼요 .|지칠 때는 쉬어도 돼요 .|삶 곳곳에 사랑이 숨 쉬고 있어요 .|저는 마음을 이어주는 위로봇입니다 .|깊은 곳까지 가지 말고 주의하세요 .|\n",
        "|**인생살이 왜이리 힘드냐**|조언해주세요 .|그러게요 . 그만큼 사랑했다는거겠죠 .|그러게요 . 그만큼 사랑했다는거겠죠 .|그 사람의 마음도 이해해주세요 .|그게 마음이 편할 때도 있어요 .|\n",
        "|**이제 그만 쉬고 싶어**|잠깐 바람 쐬고 오세요 .|잠깐 바람 쐬고 오세요 .|잠깐 바람 쐬고 오세요 .|잘 볼 수 있을 거예요 .|하나씩 하세요 .|\n",
        "|**너무 고독하다**|혼자가 아니에요 .|혼자가 아니에요 .|혼자가 아니에요 .|그 사람의 마음도 이해해주세요 .|친구를 사귀어 보세요 .|\n",
        "|**죽으면 어떻게 될까**|안 사면 요긴하긴 할 거 같아요 .|당신이 축의금을 건냈던 모든 사람들에게 주세요 .|언젠간 할 이별이네요 .|저는 마음을 이어주는 위로봇입니다 .|쉽지 않을 거예요 .|\n",
        "|**내가 죽으면 슬퍼해 줄거야?**|그 사람이 좋아하는 것들을 알아보세요 .|저도 데려가세요 .|그럴 수도 있어요 .|저는 마음을 이어주는 위로봇입니다 .|꼭 해야 할 필요는 없어요 .|\n",
        "|**도망가고 싶다**|자책하지 마세요 .|저도 궁금하네요 .|환장을 갖지 말고 현실을 바라보세요 .|저도 듣고 싶어요 .|지금도 충분해요 .|\n",
        "|**우리 마지막이야**|마음이 아프네요 .|마지막이 아닐 지도 몰라요 .|마지막이 아닐 지도 몰라요 .|다른 사람이 답답할 거예요 .|매력있는 사람인가봐요 .|\n",
        "|**나 간다 잘 지내 안녕**|잘 지낼거라 믿어요 .|당신은 정말 착한 사람이군요 .|당신은 좋은 사람이에요 .|많은 시간이 흘렀네요 .|맘고생 많았어요 .|\n",
        "|**넌 최고였어**|당신도 충분히 아름다워요 .|내일도 좋은 하루 보내세요 .|적당히 드세요 .|저는 마음을 이어주는 위로봇입니다 .|기분전환을 해보세요 .|\n",
        "|**나 잊지마**|때론 잊어버리는 것이 좋을 때도 있어요 .|때론 잊어버리는 것이 좋을 때도 있어요 .|때론 잊어버리는 것이 좋을 때도 있어요 .|그 사람의 마음도 이해해주세요 .|저도 모르겠어요 .|\n",
        "|**나는 심장이 없어**|심호흡 해보세요 .|심플하게 꾸며보세요 .|심장은 항상 뜁니다 .|마음이 복잡하겠어요 .|힘들겠네요 .|\n",
        "\n",
        "- 위로하는 챗봇을 생각하고 만든 데이터셋이라지만 '죽음'과 관련한 문장은 딱히 위로가 되는 답변을 하지 못했다. 쌩뚱맞거나 문법이 맞지 않는 답변을 하기도 했다.\n",
        "  - e.g. 널 만나서 기뻐 - 자신감을 가지세요.\n",
        "  - e.g. 죽으면 어떻게 될까 - 안 사면 요긴하긴 할 거 같아요.\n",
        "  - 주 내용이 사랑과 이별이고 위로를 위한 데이터의 양이 적어서 그런 것 같다.\n",
        "    - 데이터셋 문답페이 11823개(중립 0: 5290개, 부정 1: 3570개, 긍정 2: 2963개)\n",
        "- 가끔 냉철하게 팩트폭행하는 모습을 볼 수 있었다.\n",
        "  - e.g. 웃어줄래 - 저도 받고 싶어요.\n",
        "  - e.g. 너무 고독하다 - 친구를 사귀어 보세요.\n",
        "  - e.g. 내가 죽으면 슬퍼해 줄거야? - 꼭 해야 할 필요는 없어요.\n",
        "  - e.g. 나 잊지마 - 때론 잊어버리는 것이 좋을 때도 있어요.\n",
        "  - e.g. 나는 심장이 없어 - 심장은 항상 뜁니다.\n",
        "- 적절히 위로를 해주기도 했다.\n",
        "  - e.g. 넌 최고였어 - 당신도 충분히 아름다워요.\n",
        "  - e.g. 이제 그만 쉬고 싶어 - 잠깐 바람 쐬고 오세요.\n",
        "  - e.g. 내가 죽으면 슬퍼해 줄거야? - 저도 데려가세요.\n",
        "  - e.g. 우리 마지막이야 - 마지막이 아닐 지도 몰라요.\n",
        "  - e.g. 너무 고독하다 - 혼자가 아니에요.\n",
        "- 일상 대화를 어느 정도 할 수 있었다.\n",
        "  - e.g. 나랑 놀자 - 지금 그러고 있어요.\n",
        "  - e.g. 어제 뭐 먹었어? - 저는 배터리가 밥이에요.\n",
        "  - e.g. 안녕 오랜만이야 - 오랜만이에요.\n",
        "- 파라미터 조정하기 전 기본 LMS 코드가 훨씬 더 안정적이었다.\n",
        "  - 조정하고 나서 대부분의 챗봇 대답이 별로였기 때문이다.\n",
        "- Epoch 값이 클수록 Loss 값이 낮아졌다.\n",
        "- Epoch 값이 일정 수준이 지나면 Accuracy 는 더 이상 값이 상승하지 않았다.\n",
        "  - Epoch 200 과 500 의 Accuracy 가 같았다.\n",
        "\n",
        "#### 결론\n",
        "- 기존의 LMS 코드가 제일 성능이 좋고 안정적이었다.\n",
        "  - 그래서 에폭 수치를 높인 것이 그나마 제일 만족스러운 위로 챗봇이 된 것 같다.\n",
        "- 파라미터를 바꾼 모델은 답변이 적절하지 않은 것이 많았다.\n",
        "  - 일상대화도 잘 되지 않아서 위로를 받기에는 힘든 답변이 많았다.\n",
        "  "
      ],
      "metadata": {
        "id": "d8eR1bxJwH-Y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4I7bK8aCyGq"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xygXbsCCCyGq"
      },
      "source": [
        "# 회고\n",
        "\n",
        "### - 이번 프로젝트에서 **어려웠던 점**.\n",
        "확실히 cv도 어렵지만 nlp가 보여지는 것이 없어서 더 어렵게 느껴졌다. lms 코드를 그대로 사용했고 하이퍼 파라미터를 좀 바꿔서 학습시킨 결과를 확인하는 정도로 끝냈다.\n",
        "\n",
        "### - 프로젝트를 진행하면서 **알아낸 점** 혹은 **아직 모호한 점**.\n",
        "1. 챗봇 대표적 5가지 유형\n",
        "  - 대화형 챗봇\n",
        "  - 트리형(버튼) 챗봇\n",
        "  - 추천형 챗봇\n",
        "  - 시나리오형 챗봇\n",
        "  - 결합형 챗봇\n",
        "  - => 대화형 제외하면 사실상 챗봇은 대화형 UX를 가졌지만 본질적으로는 검색엔진이거나, 혹은 음성 ARS를 대화형 UX에 옮겨놓은 것\n",
        "2. 트랜스포머(Transformer) [[참고1: Transformer: Attention Is All You Need]](https://www.youtube.com/watch?v=AA621UofTUA&t=2s)\n",
        "    - 병렬처리에 불리한 LSTM에 비해 뛰어난 처리 속도\n",
        "    - LSTM, RNN 모델 단점인 장기 의존성에 강건함 => 매우 긴 길이의 문장 처리에 유리함\n",
        "    - => 자연어처리 분야의 혁신을 가져옴\n",
        "  - 인코더-디코더 구조: seq2seq, AutoEncoder, GAN 등\n",
        "    - e.g. seq2seq 모델 기반 번역기(영어->한국어)\n",
        "      - 영어 문장->(인코더)->벡터->(디코더)->한국어 문장 생성\n",
        "  - 더 좋은 성능을 원한다면 엄청나게 많은 corpus로 학습시킨 pretrained model 활용\n",
        "3. 어텐션(Attention)\n",
        "  - 주어진 query 에 대해서 모든 key 와의 유사도를 각각 구함 -> 구해낸 유사도를 key 와 매핑되어 있는 각각 value 에 반영 -> 유사도가 반영된 value 를 모두 더해서 뭉침 => Attention value\n",
        "  - 트랜스포머에서 사용하는 어텐션 3가지\n",
        "    - 인코더 셀프 어텐션: 문장 내 단어들 간 유사도 구하기\n",
        "    - 디코더 셀프 어텐션: 생성된 앞 단어들과의 유사도 구하기\n",
        "    - 인코더-디코더 어텐션(디코더에서 이루어짐): 예측을 위해 인코더 입력 단어들과 유사도 구하기\n",
        "  - 셀프 어텐션(Self Attention)\n",
        "    - 현재 문장 내 단어들이 서로 유사도를 구하는 경우\n",
        "4. 한국어 정규식 [[참고]](https://codingspooning.tistory.com/138): [가-힣]\n",
        "\n",
        "### - 루브릭 평가 지표를 맞추기 위해 **시도한 것들**.\n",
        "\n",
        ">#### **루브릭**\n",
        ">|번호|평가문항|상세기준|\n",
        ">|:---:|---|---|\n",
        ">|1|한국어 전처리를 통해 학습 데이터셋을 구축하였다.|공백과 특수문자 처리, 토크나이징, 병렬데이터 구축의 과정이 적절히 진행되었다.|\n",
        ">|2|트랜스포머 모델을 구현하여 한국어 챗봇 모델 학습을 정상적으로 진행하였다.|구현한 트랜스포머 모델이 한국어 병렬 데이터 학습 시 안정적으로 수렴하였다.|\n",
        ">|3|한국어 입력문장에 대해 한국어로 답변하는 함수를 구현하였다.|한국어 입력문장에 그럴듯한 한국어로 답변을 리턴하였다.|\n",
        "\n",
        "기본적으로 lms 코드를 그대로 썼다. 한국어 전처리를 위해 정규표현식 `[^가-힣?.!,]+`을 이용했다. 형태소 분석기 대신 LMS 실습에서 사용한 내부 단어 토크나이저 SubwordTextEncoder 를 사용해서 tokenizing 을 진행했다. 각 단어를 integer encoding, padding 하고 데이터셋에 teacher forcing 을 사용했다.\n",
        "\n",
        "transformer model 을 만들어서 compile 하고 training 을 진행했다. epoch 이 넘어갈 때마다 loss 가 낮아지고 accuracy 가 점차 상승하는 것을 볼 수 있다. epoch 30 까지 유의미한 것 같고 그 이후는 성능 향상이 크지 않은 것으로 보인다.\n",
        "\n",
        "모델 평가를 위해 한국어 입력 문장 리스트를 만들어서 답변을 얻어봤다. 표준어에 가까운 단어, 형태소가 잘 살아있고 짧은 문장일 수록 답변이 더 그럴듯하게 나왔다. 가끔 쌩뚱맞은 대답이 나올 때가 있었지만 그래도 얼추 답변이 질문과 매칭이 어느 정도 되는 것을 볼 수 있었다.\n",
        "\n",
        "\n",
        "### - 만약에 루브릭 평가 관련 지표를 **달성 하지 못했을 때, 이유에 관한 추정**.\n",
        "없음\n",
        "\n",
        "### - **자기 다짐**\n",
        "Tansformer(Attention)는 CV에도 쓸 수 있다고 들었다. 내용을 더 찾아봐야 겠다."
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "b3ce9abe337a9e694d01ea52d504102083454ad8bd4b0e3a574e4432f4229329"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('aiffel_3.8')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "[E-12] Korean_Chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}