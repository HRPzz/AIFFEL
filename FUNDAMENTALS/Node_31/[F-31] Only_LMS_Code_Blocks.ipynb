{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 31. 뉴스기사 크롤링 및 분류\n",
    "\n",
    "**뉴스기사를 주제/섹션별로 모아서 데이터셋을 구축하고, 이를 기반으로 뉴스기사 주제를 분류하는 텍스트 분류기를 구현해 본다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31-1. 들어가며"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ mkdir -p ~/aiffel/news_crawler\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
    "$ cd Mecab-ko-for-Google-Colab\n",
    "$ bash install_mecab-ko_on_colab190912.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ pip install beautifulsoup4\n",
    "$ pip install newspaper3k\n",
    "$ pip install konlpy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31-2. 웹 이해하기 (1) HTML과 태그"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31-3. 웹 이해하기 (2) 선택자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31-4. BeautifulSoup 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#- HTML 문서를 문자열 html로 저장합니다.\n",
    "html = '''\n",
    "<html> \n",
    "    <head> \n",
    "    </head> \n",
    "    <body> \n",
    "        <h1> 장바구니\n",
    "            <p id='clothes' class='name' title='라운드티'> 라운드티\n",
    "                <span class = 'number'> 25 </span> \n",
    "                <span class = 'price'> 29000 </span> \n",
    "                <span class = 'menu'> 의류</span> \n",
    "                <a href = 'http://www.naver.com'> 바로가기 </a> \n",
    "            </p> \n",
    "            <p id='watch' class='name' title='시계'> 시계\n",
    "                <span class = 'number'> 28 </span>\n",
    "                <span class = 'price'> 32000 </span> \n",
    "                <span class = 'menu'> 악세서리 </span> \n",
    "                <a href = 'http://www.facebook.com'> 바로가기 </a> \n",
    "            </p> \n",
    "        </h1> \n",
    "    </body> \n",
    "</html>\n",
    "'''\n",
    "\n",
    "#- BeautifulSoup 인스턴스를 생성합니다.\n",
    "#- 두번째 매개변수는 분석할 분석기(parser)의 종류입니다.\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.select('body'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.select('body'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.select('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.select('h1 .name .menu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.select('html > h1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31-5. newspaper3k 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "\n",
    "#- 파싱할 뉴스 기사 주소입니다.\n",
    "url = 'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=030&aid=0002881076'\n",
    "\n",
    "#- 언어가 한국어이므로 language='ko'로 설정해줍니다.\n",
    "article = Article(url, language='ko')\n",
    "article.download()\n",
    "article.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- 기사 제목을 출력합니다.\n",
    "print('기사 제목 :')\n",
    "print(article.title)\n",
    "print('')\n",
    "\n",
    "#- 기사 내용을 출력합니다.\n",
    "print('기사 내용 :')\n",
    "print(article.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31-6. 네이버 뉴스 기사 크롤링 (1) 뉴스 URL, 페이지 이해하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31-7. 네이버 뉴스 기사 크롤링 (2) BeautifulSoup와 newspaper3k를 통해 크롤러 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤러를 만들기 전 필요한 도구들을 임포트합니다.\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 페이지 수, 카테고리, 날짜를 입력값으로 받습니다.\n",
    "def make_urllist(page_num, code, date): \n",
    "  urllist= []\n",
    "  for i in range(1, page_num + 1):\n",
    "    url = 'https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1='+str(code)+'&date='+str(date)+'&page='+str(i)\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.90 Safari/537.36'}\n",
    "    news = requests.get(url, headers=headers)\n",
    "\n",
    "    # BeautifulSoup의 인스턴스 생성합니다. 파서는 html.parser를 사용합니다.\n",
    "    soup = BeautifulSoup(news.content, 'html.parser')\n",
    "\n",
    "    # CASE 1\n",
    "    news_list = soup.select('.newsflash_body .type06_headline li dl')\n",
    "    # CASE 2\n",
    "    news_list.extend(soup.select('.newsflash_body .type06 li dl'))\n",
    "        \n",
    "    # 각 뉴스로부터 a 태그인 <a href ='주소'> 에서 '주소'만을 가져옵니다.\n",
    "    for line in news_list:\n",
    "        urllist.append(line.a.get('href'))\n",
    "  return urllist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = make_urllist(2, 101, 20200506)\n",
    "print('뉴스 기사의 개수: ',len(url_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {'101' : '경제', '102' : '사회', '103' : '생활/문화', '105' : 'IT/과학'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "\n",
    "#- 데이터프레임을 생성하는 함수입니다.\n",
    "def make_data(urllist, code):\n",
    "  text_list = []\n",
    "  for url in urllist:\n",
    "    article = Article(url, language='ko')\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    text_list.append(article.text)\n",
    "\n",
    "  #- 데이터프레임의 'news' 키 아래 파싱한 텍스트를 밸류로 붙여줍니다.\n",
    "  df = pd.DataFrame({'news': text_list})\n",
    "\n",
    "  #- 데이터프레임의 'code' 키 아래 한글 카테고리명을 붙여줍니다.\n",
    "  df['code'] = idx2word[str(code)]\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = make_data(url_list, 101)\n",
    "#- 상위 10개만 출력해봅니다.\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31-8. 네이버 뉴스 기사 크롤링 (3) 데이터 수집 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_list = [102, 103, 105]\n",
    "\n",
    "code_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import random\n",
    "import time, os\n",
    "\n",
    "def make_total_data(page_num, code_list, date):\n",
    "  start = int(time.time())  \n",
    "  num_cores = 4  \n",
    "  df = None\n",
    "  for code in code_list:\n",
    "    pool = Pool(num_cores)\n",
    "    url_list = make_urllist(page_num, code, date)\n",
    "    df_temp = make_data(url_list, code)\n",
    "    print(str(code)+'번 코드에 대한 데이터를 만들었습니다.')\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    time.sleep(random.randint(0,1))\n",
    "    if df is not None:\n",
    "      df = pd.concat([df, df_temp])\n",
    "    else:\n",
    "      df = df_temp\n",
    "\n",
    "  print(\"***run time(sec) :\", int(time.time()) - start)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_total_data(1, code_list, 20200506)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('뉴스 기사의 개수: ',len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래 주석처리된 코드의 주석을 해제하고 실행을 하면 대량 크롤링이 진행됩니다. \n",
    "# 위에서 수행했던 크롤링의 10배 분량이 수행될 것입니다. 한꺼번에 너무 많은 크롤링 요청이 서버에 전달되지 않도록 주의해 주세요. \n",
    "# 기사 일자를 바꿔보면서 데이터를 모으면 더욱 다양한 데이터를 얻을 수 있게 됩니다. \n",
    "\n",
    "#df = make_total_data(10, code_list, 20200506)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 데이터프레임 파일을 csv 파일로 저장합니다.\n",
    "# 저장경로는 이번 프로젝트를 위해 만든 폴더로 지정해 주세요.\n",
    "csv_path = os.getenv(\"HOME\") + \"/aiffel/news_crawler/news_data.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "if os.path.exists(csv_path):\n",
    "  print('{} File Saved!'.format(csv_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31-9. 네이버 뉴스 기사 크롤링 (4) 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = os.getenv(\"HOME\") + \"/aiffel/news_crawler/news_data.csv\"\n",
    "df = pd.read_table(csv_path, sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규 표현식을 이용해서 한글 외의 문자는 전부 제거합니다.\n",
    "df['news'] = df['news'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "df['news']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복된 샘플들을 제거합니다.\n",
    "df.drop_duplicates(subset=['news'], inplace=True)\n",
    "\n",
    "print('뉴스 기사의 개수: ',len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 샘플 제거\n",
    "df.drop_duplicates(subset=['news'], inplace=True)\n",
    "\n",
    "print('뉴스 기사의 개수: ',len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"NanumGothic\"\n",
    "\n",
    "df['code'].value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.groupby('code').size().reset_index(name = 'count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "\n",
    "kor_text = '밤에 귀가하던 여성에게 범죄를 시도한 대 남성이 구속됐다서울 제주경찰서는 \\\n",
    "            상해 혐의로 씨를 구속해 수사하고 있다고 일 밝혔다씨는 지난달 일 피해 여성을 \\\n",
    "            인근 지하철 역에서부터 따라가 폭행을 시도하려다가 도망간 혐의를 받는다피해 \\\n",
    "            여성이 저항하자 놀란 씨는 도망갔으며 신고를 받고 주변을 수색하던 경찰에 \\\n",
    "            체포됐다피해 여성은 이 과정에서 경미한 부상을 입은 것으로 전해졌다'\n",
    "\n",
    "#- 형태소 분석, 즉 토큰화(tokenization)를 합니다.\n",
    "print(tokenizer.morphs(kor_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['에','는','은','을','했','에게','있','이','의','하','한','다','과','때문','할','수','무단','따른','및','금지','전재','경향신문','기자','는데','가','등','들','파이낸셜','저작','등','뉴스']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 및 토큰화 과정에서 불용어를 제거하는 함수입니다.\n",
    "def preprocessing(data):\n",
    "  text_data = []\n",
    "\n",
    "  for sentence in data:\n",
    "    temp_data = []\n",
    "    #- 토큰화\n",
    "    temp_data = tokenizer.morphs(sentence) \n",
    "    #- 불용어 제거\n",
    "    temp_data = [word for word in temp_data if not word in stopwords] \n",
    "    text_data.append(temp_data)\n",
    "\n",
    "  text_data = list(map(' '.join, text_data))\n",
    "\n",
    "  return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = preprocessing(df['news'])\n",
    "print(text_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31-10. 머신 러닝 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- 훈련 데이터와 테스트 데이터를 분리합니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data, df['code'], random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('훈련용 뉴스 기사의 개수 :', len(X_train))\n",
    "print('테스트용 뉴스 기사의 개수 : ', len(X_test))\n",
    "print('훈련용 레이블의 개수 : ', len(y_train))\n",
    "print('테스트용 레이블의 개수 : ', len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저입니다.\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#- 나이브 베이즈 분류기를 수행합니다.\n",
    "#- X_train은 TF-IDF 벡터, y_train은 레이블입니다.\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorizer(data):\n",
    "  data_counts = count_vect.transform(data)\n",
    "  data_tfidf = tfidf_transformer.transform(data_counts)\n",
    "  return data_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sent = preprocessing([\"민주당 일각에서 법사위의 체계·자구 심사 기능을 없애야 한다는 \\\n",
    "                           주장이 나오는 데 대해 “체계·자구 심사가 법안 지연의 수단으로 \\\n",
    "                          쓰이는 것은 바람직하지 않다”면서도 “국회를 통과하는 법안 중 위헌\\\n",
    "                          법률이 1년에 10건 넘게 나온다. 그런데 체계·자구 심사까지 없애면 매우 위험하다”고 반박했다.\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sent = preprocessing([\"인도 로맨틱 코미디 영화 <까립까립 싱글>(2017)을 봤을 때 나는 두 눈을 의심했다. \\\n",
    "                          저 사람이 남자 주인공이라고? 노안에 가까운 이목구비와 기름때로 뭉친 파마머리와, \\\n",
    "                          대충 툭툭 던지는 말투 등 전혀 로맨틱하지 않은 외모였다. 반감이 일면서 \\\n",
    "                          ‘난 외모지상주의자가 아니다’라고 자부했던 나에 대해 회의가 들었다.\\\n",
    "                           티브이를 꺼버릴까? 다른 걸 볼까? 그런데, 이상하다. 왜 이렇게 매력 있지? 개구리와\\\n",
    "                            같이 툭 불거진 눈망울 안에는 어떤 인도 배우에게서도 느끼지 못한 \\\n",
    "                            부드러움과 선량함, 무엇보다 슬픔이 있었다. 2시간 뒤 영화가 끝나고 나는 완전히 이 배우에게 빠졌다\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sent = preprocessing([\"20분기 연속으로 적자에 시달리는 LG전자가 브랜드 이름부터 성능, 디자인까지 대대적인 변화를 \\\n",
    "                          적용한 LG 벨벳은 등장 전부터 온라인 커뮤니티를 뜨겁게 달궜다. 사용자들은 “디자인이 예쁘다”, \\\n",
    "                          “슬림하다”는 반응을 보이며 LG 벨벳에 대한 기대감을 드러냈다.\"])\n",
    "print(clf.predict(tfidf_vectorizer(new_sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31-11. 프로젝트 - 모델 성능 개선하기"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ce9abe337a9e694d01ea52d504102083454ad8bd4b0e3a574e4432f4229329"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('aiffel_3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
