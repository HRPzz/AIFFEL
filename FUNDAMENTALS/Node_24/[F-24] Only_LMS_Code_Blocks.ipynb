{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24. 딥러닝 레이어의 이해(2) Embedding, Recurrent\n",
    "\n",
    "**딥러닝 레이어의 이해(1)에 이어서, Embedding 레이어와 Recurrent 레이어에 대해 보다 자세하게 알아본다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24-1. 들어가며"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24-2. 분포 가설과 분산 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24-3. 단어를 부탁해! Embedding 레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "vocab = {      # 사용할 단어 사전 정의\n",
    "    \"i\": 0,\n",
    "    \"need\": 1,\n",
    "    \"some\": 2,\n",
    "    \"more\": 3,\n",
    "    \"coffee\": 4,\n",
    "    \"cake\": 5,\n",
    "    \"cat\": 6,\n",
    "    \"dog\": 7\n",
    "}\n",
    "\n",
    "sentence = \"i i i i need some more coffee coffee coffee\"\n",
    "# 위 sentence\n",
    "_input = [vocab[w] for w in sentence.split()]  # [0, 0, 0, 0, 1, 2, 3, 4, 4, 4]\n",
    "\n",
    "vocab_size = len(vocab)   # 8\n",
    "\n",
    "one_hot = tf.one_hot(_input, vocab_size)\n",
    "print(one_hot.numpy())    # 원-핫 인코딩 벡터를 출력해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_size = 2   # 보기 좋게 2차원으로 분산 표현하도록 하죠!\n",
    "linear = tf.keras.layers.Dense(units=distribution_size, use_bias=False)\n",
    "one_hot_linear = linear(one_hot)\n",
    "\n",
    "print(\"Linear Weight\")\n",
    "print(linear.weights[0].numpy())\n",
    "\n",
    "print(\"\\nOne-Hot Linear Result\")\n",
    "print(one_hot_linear.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_words = tf.constant([[3, 57, 35]])\n",
    "# 3번 단어 / 57번 단어 / 35번 단어로 이루어진 한 문장입니다.\n",
    "\n",
    "print(\"Embedding을 진행할 문장:\", some_words.shape)\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=64, output_dim=100)\n",
    "# 총 64개의 단어를 포함한 Embedding 레이어를 선언할 것이고,\n",
    "# 각 단어는 100차원으로 분산 표현 할 것입니다.\n",
    "\n",
    "print(\"Embedding된 문장:\", embedding_layer(some_words).shape)\n",
    "print(\"Embedding Layer의 Weight 형태:\", embedding_layer.weights[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24-4. 순차적인 데이터! Recurrent 레이어 (1) RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"What time is it ?\"\n",
    "dic = {\n",
    "    \"is\": 0,\n",
    "    \"it\": 1,\n",
    "    \"What\": 2,\n",
    "    \"time\": 3,\n",
    "    \"?\": 4\n",
    "}\n",
    "\n",
    "print(\"RNN에 입력할 문장:\", sentence)\n",
    "\n",
    "sentence_tensor = tf.constant([[dic[word] for word in sentence.split()]])\n",
    "\n",
    "print(\"Embedding을 위해 단어 매핑:\", sentence_tensor.numpy())\n",
    "print(\"입력 문장 데이터 형태:\", sentence_tensor.shape)\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=len(dic), output_dim=100)\n",
    "emb_out = embedding_layer(sentence_tensor)\n",
    "\n",
    "print(\"\\nEmbedding 결과:\", emb_out.shape)\n",
    "print(\"Embedding Layer의 Weight 형태:\", embedding_layer.weights[0].shape)\n",
    "\n",
    "rnn_seq_layer = \\\n",
    "tf.keras.layers.SimpleRNN(units=64, return_sequences=True, use_bias=False)\n",
    "rnn_seq_out = rnn_seq_layer(emb_out)\n",
    "\n",
    "print(\"\\nRNN 결과 (모든 Step Output):\", rnn_seq_out.shape)\n",
    "print(\"RNN Layer의 Weight 형태:\", rnn_seq_layer.weights[0].shape)\n",
    "\n",
    "rnn_fin_layer = tf.keras.layers.SimpleRNN(units=64, use_bias=False)\n",
    "rnn_fin_out = rnn_fin_layer(emb_out)\n",
    "\n",
    "print(\"\\nRNN 결과 (최종 Step Output):\", rnn_fin_out.shape)\n",
    "print(\"RNN Layer의 Weight 형태:\", rnn_fin_layer.weights[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_seq_layer = tf.keras.layers.LSTM(units=64, return_sequences=True, use_bias=False)\n",
    "lstm_seq_out = lstm_seq_layer(emb_out)\n",
    "\n",
    "print(\"\\nLSTM 결과 (모든 Step Output):\", lstm_seq_out.shape)\n",
    "print(\"LSTM Layer의 Weight 형태:\", lstm_seq_layer.weights[0].shape)\n",
    "\n",
    "lstm_fin_layer = tf.keras.layers.LSTM(units=64, use_bias=False)\n",
    "lstm_fin_out = lstm_fin_layer(emb_out)\n",
    "\n",
    "print(\"\\nLSTM 결과 (최종 Step Output):\", lstm_fin_out.shape)\n",
    "print(\"LSTM Layer의 Weight 형태:\", lstm_fin_layer.weights[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24-5. 순차적인 데이터! Recurrent 레이어 (2) LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "sentence = \"What time is it ?\"\n",
    "dic = {\n",
    "    \"is\": 0,\n",
    "    \"it\": 1,\n",
    "    \"What\": 2,\n",
    "    \"time\": 3,\n",
    "    \"?\": 4\n",
    "}\n",
    "\n",
    "sentence_tensor = tf.constant([[dic[word] for word in sentence.split()]])\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=len(dic), output_dim=100)\n",
    "emb_out = embedding_layer(sentence_tensor)\n",
    "\n",
    "print(\"입력 문장 데이터 형태:\", emb_out.shape)\n",
    "\n",
    "bi_rnn = \\\n",
    "tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.SimpleRNN(units=64, use_bias=False, return_sequences=True)\n",
    ")\n",
    "bi_out = bi_rnn(emb_out)\n",
    "\n",
    "print(\"Bidirectional RNN 결과 (최종 Step Output):\", bi_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24-6. 마무리"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ce9abe337a9e694d01ea52d504102083454ad8bd4b0e3a574e4432f4229329"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('aiffel_3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
