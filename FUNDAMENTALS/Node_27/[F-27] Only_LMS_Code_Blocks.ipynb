{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 27. Likelihood(MLE와 MAP)\n",
    "\n",
    "**베이지안 머신 러닝 모델의 prior, posterior, likelihood 개념을 학습하고 MLE(최대 가능도 추론)와 MAP(최대 사후 확률 추정)의 최적해를 구하는 과정과 두 방법의 차이를 이해한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27-1. 들어가며: 머신러닝의 목표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27-2. 확률 변수로서의 모델 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "parameter_points = []\n",
    "fig1, axes1 = plt.subplots(2, 5, figsize=(10, 4))\n",
    "\n",
    "for ax in axes1.flatten():\n",
    "    # np.random.uniform: 정해진 구간에서 수를 무작위로 추출하여 반환합니다.\n",
    "    a, b = np.random.uniform(-10, 10, size=2)\n",
    "    a = round(a, 3)\n",
    "    b = round(b, 3)\n",
    "    parameter_points.append((a, b))\n",
    "\n",
    "    x = np.linspace(-10, 10, 50)\n",
    "    y = a*x + b\n",
    "    ax.plot(x, y)\n",
    "\n",
    "    ax.set_title('y='+str(a)+'x'+'{0:+.03f}'.format(b))\n",
    "    ax.set_xlim(-10, 10)\n",
    "    ax.set_ylim(-10, 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "px, py = np.split(np.array(parameter_points), 2, axis=1)\n",
    "fig2 = plt.figure()\n",
    "axes2 = plt.gca()\n",
    "\n",
    "axes2.set_title('samples from parameter space')\n",
    "axes2.set_xlim(-10, 10)\n",
    "axes2.set_ylim(-10, 10)\n",
    "\n",
    "plt.scatter(px, py)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_points = []\n",
    "fig, axes1 = plt.subplots(2, 5, figsize=(10, 4))\n",
    "\n",
    "for ax in axes1.flatten():\n",
    "    # np.random.normal: 정규분포를 따르는 확률 변수의 랜덤한 값을 반환합니다.\n",
    "    a, b = np.random.normal(loc=[1, 0], scale=0.5)\n",
    "    a = round(a, 3)\n",
    "    b = round(b, 3)\n",
    "    parameter_points.append((a, b))\n",
    "\n",
    "    x = np.linspace(-10, 10, 50)\n",
    "    y = a*x + b\n",
    "    ax.plot(x, y)\n",
    "\n",
    "    ax.set_title('y='+str(a)+'x'+'{0:+.03f}'.format(b))\n",
    "    ax.set_xlim(-10, 10)\n",
    "    ax.set_ylim(-10, 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "px, py = np.split(np.array(parameter_points), 2, axis=1)\n",
    "fig2 = plt.figure()\n",
    "axes2 = plt.gca()\n",
    "\n",
    "axes2.set_title('samples from parameter space')\n",
    "axes2.set_xlim(-10, 10)\n",
    "axes2.set_ylim(-10, 10)\n",
    "\n",
    "plt.scatter(px, py)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27-3. posterior와 prior, likelihood 사이의 관계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27-4. likelihood와 머신러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27-5. likelihood 감 잡기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(321)\n",
    "\n",
    "input_data = np.linspace(-2, 2, 5)\n",
    "label = input_data + 1 + np.random.normal(0, 1, size=5)\n",
    "\n",
    "plt.scatter(input_data, label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: y = ax + b\n",
    "# a, b 값을 바꾸면서 실행해보세요\n",
    "#-------------------------------#\n",
    "a = 1\n",
    "b = 1\n",
    "#-------------------------------#\n",
    "\n",
    "# 모델 예측값\n",
    "model_output = a*input_data + b\n",
    "likelihood = []\n",
    "\n",
    "# x: 입력데이터, y: 데이터라벨\n",
    "# 예측값과 라벨의 차이를 제곱해 exp에 사용\n",
    "for x, y, output in zip(input_data, label, model_output):\n",
    "    likelihood.append(1/(math.sqrt(2*math.pi*0.1*0.1))*math.exp(-pow(y-output,2)/(2*0.1*0.1)))\n",
    "\n",
    "model_x = np.linspace(-2, 2, 50)\n",
    "model_y = a*model_x + b\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(input_data, label)\n",
    "ax.plot(model_x, model_y)\n",
    "\n",
    "for i, text in enumerate(likelihood):\n",
    "    ax.annotate('%.3e'%text, (input_data[i], label[i]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27-6. MLE: 최대 가능도 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27-7. MLE 최적해 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "num_samples = 20\n",
    "\n",
    "input_data = np.linspace(-2, 2, num_samples)\n",
    "labels = input_data + 1 + np.random.normal(0, 0.5, size=num_samples)\n",
    "\n",
    "plt.scatter(input_data, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(labels, preds):\n",
    "    result = 1/(np.sqrt(2*math.pi*0.1*0.1))*np.exp(-np.power(labels-preds,2)/(2*0.1*0.1))\n",
    "    \n",
    "    return np.prod(result)\n",
    "\n",
    "def neg_log_likelihood(labels, preds):\n",
    "    const_term = len(labels)*math.log(1/math.sqrt(2*math.pi*0.1*0.1))\n",
    "\n",
    "    return (-1)*(const_term + 1/(2*0.1*0.1)*np.sum(-np.power(labels-preds,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: 20x2 matrix, y: 20x1 matrix\n",
    "# input_data 리스트를 column vector로 바꾼 다음 np.append 함수로 상수항을 추가합니다.\n",
    "X = np.append(input_data.reshape((-1, 1)), np.ones((num_samples, 1)), axis=1)\n",
    "y = labels\n",
    "\n",
    "theta_1, theta_0 = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)\n",
    "\n",
    "print('slope: '+'%.4f'%theta_1+' bias: '+'%.4f'%theta_0)\n",
    "\n",
    "predictions = theta_1 * input_data + theta_0\n",
    "print('likelihood: '+'%.4e'%likelihood(labels, predictions))\n",
    "print('negative log likelihood: '+'%.4e'%neg_log_likelihood(labels, predictions))\n",
    "\n",
    "model_x = np.linspace(-2, 2, 50)\n",
    "model_y = theta_1 * model_x + theta_0\n",
    "\n",
    "plt.scatter(input_data, labels)\n",
    "plt.plot(model_x, model_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27-8. MAP: 최대 사후 확률 추정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27-9. MLE와 MAP의 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "num_samples = 10\n",
    "\n",
    "input_data = np.linspace(-2, 2, num_samples)\n",
    "labels = input_data + 1 + np.random.normal(0, 0.5, size=num_samples)\n",
    "\n",
    "input_data = np.append(input_data, [0.5, 1.5])\n",
    "labels = np.append(labels, [9.0, 10.0])\n",
    "\n",
    "plt.scatter(input_data, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(labels, preds):\n",
    "    result = 1/(np.sqrt(2*math.pi*0.1*0.1))*np.exp(-np.power(labels-preds,2)/(2*0.1*0.1))\n",
    "    \n",
    "    return np.prod(result)\n",
    "\n",
    "def neg_log_likelihood(labels, preds):\n",
    "    const_term = len(labels)*math.log(1/math.sqrt(2*math.pi*0.1*0.1))\n",
    "\n",
    "    return (-1)*(const_term + 1/(2*0.1*0.1)*np.sum(-np.power(labels-preds,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: 21x2 matrix, y: 21x1 matrix\n",
    "# input_data 리스트를 column vector로 바꾼 다음 np.append 함수로 상수항을 추가합니다.\n",
    "X = np.append(input_data.reshape((-1, 1)), np.ones((num_samples+2, 1)), axis=1)\n",
    "y = labels\n",
    "\n",
    "# MLE 파라미터 계산식\n",
    "mle_theta_1, mle_theta_0 = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)\n",
    "# MAP 파라미터 계산식\n",
    "map_theta_1, map_theta_0 = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)+(0.1*0.1)/(0.04*0.04)*np.eye(2)), X.T), y)\n",
    "\n",
    "print('[MLE result] (blue)')\n",
    "print('slope: '+'%.4f'%mle_theta_1+' bias: '+'%.4f'%mle_theta_0)\n",
    "\n",
    "mle_preds = mle_theta_1 * input_data + mle_theta_0\n",
    "print('likelihood: '+'%.4e'%likelihood(labels, mle_preds))\n",
    "print('negative log likelihood: '+'%.4e\\n'%neg_log_likelihood(labels, mle_preds))\n",
    "\n",
    "print('[MAP result] (orange)')\n",
    "print('slope: '+'%.4f'%map_theta_1+' bias: '+'%.4f'%map_theta_0)\n",
    "\n",
    "map_preds = map_theta_1 * input_data + map_theta_0\n",
    "print('likelihood: '+'%.4e'%likelihood(labels, map_preds))\n",
    "print('negative log likelihood: '+'%.4e'%neg_log_likelihood(labels, map_preds))\n",
    "\n",
    "model_x = np.linspace(-2, 2, 50)\n",
    "mle_model_y = mle_theta_1 * model_x + mle_theta_0\n",
    "map_model_y = map_theta_1 * model_x + map_theta_0\n",
    "\n",
    "plt.scatter(input_data, labels)\n",
    "plt.plot(model_x, mle_model_y)\n",
    "plt.plot(model_x, map_model_y)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ce9abe337a9e694d01ea52d504102083454ad8bd4b0e3a574e4432f4229329"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('aiffel_3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
