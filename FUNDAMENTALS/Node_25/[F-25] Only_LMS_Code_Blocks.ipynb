{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25. Regularization\n",
    "\n",
    "**정칙화(Regularization) 개념과, L1, L2 regularization, Dropout, batch normalization에 대해 알아본다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25-1.들어가며"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25-2. Regularization과 Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "target_df = pd.DataFrame(data=iris.target, columns=['species'])\n",
    "\n",
    "# 0, 1, 2로 되어있는 target 데이터를 \n",
    "# 알아보기 쉽게 'setosa', 'versicolor', 'virginica'로 바꿉니다 \n",
    "def converter(species):\n",
    "    if species == 0:\n",
    "        return 'setosa'\n",
    "    elif species == 1:\n",
    "        return 'versicolor'\n",
    "    else:\n",
    "        return 'virginica'\n",
    "\n",
    "target_df['species'] = target_df['species'].apply(converter)\n",
    "\n",
    "iris_df = pd.concat([iris_df, target_df], axis=1)\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [iris_df['petal length (cm)'][a] for a in iris_df.index if iris_df['species'][a]=='virginica']\n",
    "Y = [iris_df['sepal length (cm)'][a] for a in iris_df.index if iris_df['species'][a]=='virginica']\n",
    "\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X,Y)\n",
    "plt.title('petal-sepal scatter before normalization') \n",
    "plt.xlabel('petal length (cm)')\n",
    "plt.ylabel('sepal length (cm)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "X_scale = minmax_scale(X)\n",
    "Y_scale = minmax_scale(Y)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X_scale,Y_scale)\n",
    "plt.title('petal-sepal scatter after normalization') \n",
    "plt.xlabel('petal length (cm)')\n",
    "plt.ylabel('sepal length (cm)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np \n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Iris Dataset을 Linear Regression으로 학습합니다. \n",
    "linear= LinearRegression()\n",
    "linear.fit(X.reshape(-1,1), Y)\n",
    "\n",
    "# Linear Regression의 기울기와 절편을 확인합니다. \n",
    "a, b=linear.coef_, linear.intercept_\n",
    "print(\"기울기 : %0.2f, 절편 : %0.2f\" %(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X,Y)\n",
    "plt.plot(X,linear.predict(X.reshape(-1,1)),'-b')\n",
    "plt.title('petal-sepal scatter with linear regression') \n",
    "plt.xlabel('petal length (cm)')\n",
    "plt.ylabel('sepal length (cm)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L1 regularization은 Lasso로 import 합니다.\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "L1 = Lasso()\n",
    "L1.fit(X.reshape(-1,1), Y)\n",
    "a, b=L1.coef_, L1.intercept_\n",
    "print(\"기울기 : %0.2f, 절편 : %0.2f\" %(a,b))\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X,Y)\n",
    "plt.plot(X,L1.predict(X.reshape(-1,1)),'-b')\n",
    "plt.title('petal-sepal scatter with L1 regularization(Lasso)') \n",
    "plt.xlabel('petal length (cm)')\n",
    "plt.ylabel('sepal length (cm)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L2 regularization은 Ridge로 import 합니다. \n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "L2 = Ridge()\n",
    "L2.fit(X.reshape(-1,1), Y)\n",
    "a, b = L2.coef_, L2.intercept_\n",
    "print(\"기울기 : %0.2f, 절편 : %0.2f\" %(a,b))\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X,Y)\n",
    "plt.plot(X,L2.predict(X.reshape(-1,1)),'-b')\n",
    "plt.title('petal-sepal scatter with L2 regularization(Ridge)') \n",
    "plt.xlabel('petal length (cm)')\n",
    "plt.ylabel('sepal length (cm)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25-3. L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "target_df = pd.DataFrame(data=iris.target, columns=['species'])\n",
    "\n",
    "def converter(species):\n",
    "    if species == 0:\n",
    "        return 'setosa'\n",
    "    elif species == 1:\n",
    "        return 'versicolor'\n",
    "    else:\n",
    "        return 'virginica'\n",
    "\n",
    "target_df['species'] = target_df['species'].apply(converter)\n",
    "\n",
    "iris_df = pd.concat([iris_df, target_df], axis=1)\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [iris_df['petal length (cm)'][a] for a in iris_df.index if iris_df['species'][a]=='virginica']\n",
    "Y = [iris_df['sepal length (cm)'][a] for a in iris_df.index if iris_df['species'][a]=='virginica']\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X,Y)\n",
    "plt.xlabel('petal length (cm)')\n",
    "plt.ylabel('sepal length (cm)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "L1 = Lasso()\n",
    "L1.fit(X.reshape(-1,1), Y)\n",
    "a, b = L1.coef_, L1.intercept_\n",
    "print(\"기울기 : %0.2f, 절편 : %0.2f\" %(a,b))\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X,Y)\n",
    "plt.plot(X,L1.predict(X.reshape(-1,1)),'-b')\n",
    "plt.xlabel('petal length (cm)')\n",
    "plt.ylabel('sepal length (cm)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine = load_wine()\n",
    "wine_df = pd.DataFrame(data=wine.data, columns=wine.feature_names)\n",
    "target_df = pd.DataFrame(data=wine.target, columns=['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# 데이터를 준비하고\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine_df, target_df, test_size=0.3, random_state=101)\n",
    "\n",
    "# 모델을 훈련시킵니다.\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 테스트를 해볼까요?\n",
    "model.predict(X_test)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "# 테스트 결과는 이렇습니다!\n",
    "print(\"result of linear regression\")\n",
    "print('Mean Absolute Error:', mean_absolute_error(y_test, pred))\n",
    "print('Mean Squared Error:', mean_squared_error(y_test, pred))\n",
    "print('Mean Root Squared Error:', np.sqrt(mean_squared_error(y_test, pred)))\n",
    "\n",
    "print(\"\\n\\n coefficient linear regression\")\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# 모델을 준비하고 훈련시킵니다.\n",
    "L1 = Lasso(alpha=0.05)\n",
    "L1.fit(X_train, y_train)\n",
    "\n",
    "# 테스트를 해봅시다.\n",
    "pred = L1.predict(X_test)\n",
    "\n",
    "# 모델 성능은 얼마나 좋을까요?\n",
    "print(\"result of Lasso\")\n",
    "print('Mean Absolute Error:', mean_absolute_error(y_test, pred))\n",
    "print('Mean Squared Error:', mean_squared_error(y_test, pred))\n",
    "print('Mean Root Squared Error:', np.sqrt(mean_squared_error(y_test, pred)))\n",
    "\n",
    "print(\"\\n\\n coefficient of Lasso\")\n",
    "print(L1.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25-4. L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "wine = load_wine()\n",
    "wine_df = pd.DataFrame(data=wine.data, columns=wine.feature_names)\n",
    "target_df = pd.DataFrame(data=wine.target, columns=['Y'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine_df, target_df, test_size= 0.3, random_state=101)\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "L1 = Lasso(alpha=0.05, max_iter=5)\n",
    "L1.fit(X_train, y_train)\n",
    "pred = L1.predict(X_test)\n",
    "\n",
    "print(\"result of Lasso\")\n",
    "print('Mean Absolute Error:', mean_absolute_error(y_test, pred))\n",
    "print('Mean Squared Error:', mean_squared_error(y_test, pred))\n",
    "print('Mean Root Squared Error:', np.sqrt(mean_squared_error(y_test, pred)))\n",
    "\n",
    "print(\"\\n\\n coefficient of Lasso\")\n",
    "print(L1.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "L2 = Ridge(alpha=0.05,max_iter=5)\n",
    "L2.fit(X_train, y_train)\n",
    "pred = L2.predict(X_test)\n",
    "\n",
    "print(\"result of Ridge\")\n",
    "print('Mean Absolute Error:', mean_absolute_error(y_test, pred))\n",
    "print('Mean Squared Error:', mean_squared_error(y_test, pred))\n",
    "print('Mean Root Squared Error:', np.sqrt(mean_squared_error(y_test, pred)))\n",
    "\n",
    "print(\"\\n\\n coefficient of Ridge\")\n",
    "print(L2.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25-5. Extra : Lp norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array([1,10,1,1,1])\n",
    "p=5\n",
    "norm_x=np.linalg.norm(x, ord=p)\n",
    "making_norm = (sum(x**p))**(1/p)\n",
    "print(\"result of numpy package norm function : %0.5f \"%norm_x) \n",
    "print(\"result of making norm : %0.5f \"%making_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_x=np.linalg.norm(x, ord=np.inf)\n",
    "print(\"result of infinite norm : %0.5f \"%norm_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.array([[1,2,3],[1,2,3],[4,6,8]])\n",
    "inf_norm_A=np.linalg.norm(A, ord=np.inf)\n",
    "print(\"result inf norm of A :\", inf_norm_A)\n",
    "one_norm_A=np.linalg.norm(A, ord=1)\n",
    "print(\"result one norm of A :\", one_norm_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25-6. Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    # 여기에 dropout layer를 추가해보았습니다. 나머지 layer는 아래의 실습과 같습니다.\n",
    "    keras.layers.Dropout(0.9),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history= model.fit(train_images, train_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    # 이번에는 dropout layer가 없습니다. \n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_images, train_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train_images, train_labels, test_size=0.01, random_state=101)\n",
    "X_train = X_train / 255.0\n",
    "X_valid = X_valid / 255.0\n",
    "\n",
    "#Dense layer만으로 만들어 낸 classification 모델입니다.\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history= model.fit(X_train, y_train, epochs=200, batch_size=512, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 값을 plot 해보겠습니다.\n",
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "x_len = np.arange(len(y_loss))\n",
    "\n",
    "plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Loss\")\n",
    "plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.title('Loss graph without dropout layer') \n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy 값을 plot 해보겠습니다.\n",
    "y_vacc = history.history['val_accuracy']\n",
    "y_acc = history.history['accuracy']\n",
    "x_len = np.arange(len(y_acc))\n",
    "\n",
    "plt.plot(x_len, y_vacc, marker='.', c='red', label=\"Validation-set accuracy\")\n",
    "plt.plot(x_len, y_acc, marker='.', c='blue', label=\"Train-set accuracy\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.ylim(0.5,1) \n",
    "plt.title('Accuracy graph without dropout layer') \n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    # 여기에 dropout layer를 추가해보았습니다. 나머지 layer는 위의 실습과 같습니다. \n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history= model.fit(X_train, y_train, epochs=200, batch_size=512, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 값을 plot 해보겠습니다. \n",
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "x_len = np.arange(len(y_loss))\n",
    "\n",
    "plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Loss\")\n",
    "plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.ylim(0,1)\n",
    "plt.title('Loss graph with dropout layer') \n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy 값을 plot 해보겠습니다. \n",
    "y_vacc = history.history['val_accuracy']\n",
    "y_acc = history.history['accuracy']\n",
    "x_len = np.arange(len(y_acc))\n",
    "\n",
    "plt.plot(x_len, y_vacc, marker='.', c='red', label=\"Validation-set accuracy\")\n",
    "plt.plot(x_len, y_acc, marker='.', c='blue', label=\"Train-set accuracy\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.ylim(0.5,1) \n",
    "plt.title('Accuracy graph with dropout layer') \n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25-7. Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_images, train_labels, test_size=0.3, random_state=101)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history= model.fit(X_train, y_train, epochs=20, batch_size=2048, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 값을 plot 해보겠습니다. \n",
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "x_len = np.arange(len(y_loss))\n",
    "\n",
    "plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Loss\")\n",
    "plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.ylim(0,1)\n",
    "plt.title('Loss graph without batch normalization') \n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy 값을 plot 해보겠습니다. \n",
    "y_vacc = history.history['val_accuracy']\n",
    "y_acc = history.history['accuracy']\n",
    "x_len = np.arange(len(y_acc))\n",
    "\n",
    "plt.plot(x_len, y_vacc, marker='.', c='red', label=\"Validation-set accuracy\")\n",
    "plt.plot(x_len, y_acc, marker='.', c='blue', label=\"Train-set accuracy\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.ylim(0.5,1)\n",
    "plt.title('Accuracy graph without batch normalization') \n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    #여기에 batchnormalization layer를 추가해보았습니다. 나머지 layer는 위의 실습과 같습니다.\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history= model.fit(X_train, y_train, epochs=20, batch_size=2048, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 값을 plot 해보겠습니다. \n",
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "x_len = np.arange(len(y_loss))\n",
    "\n",
    "plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Loss\")\n",
    "plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.ylim(0,1)\n",
    "plt.title('Loss graph with batch normalization') \n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy 값을 plot 해보겠습니다. \n",
    "y_vacc = history.history['val_accuracy']\n",
    "y_acc = history.history['accuracy']\n",
    "x_len = np.arange(len(y_acc))\n",
    "\n",
    "plt.plot(x_len, y_vacc, marker='.', c='red', label=\"Validation-set accuracy\")\n",
    "plt.plot(x_len, y_acc, marker='.', c='blue', label=\"Train-set accuracy\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.ylim(0.5,1) \n",
    "plt.title('Accurcy graph with batch normalization') \n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ce9abe337a9e694d01ea52d504102083454ad8bd4b0e3a574e4432f4229329"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('aiffel_3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
